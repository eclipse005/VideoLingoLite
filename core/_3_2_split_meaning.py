import concurrent.futures
import math
from typing import List

from core.utils import *
from rich.console import Console
from core.utils.models import _3_1_SPLIT_BY_NLP, _3_2_SPLIT_BY_MEANING, Sentence

console = Console()


def parse_br_positions(llm_output: str) -> List[int]:
    """
    è§£æ LLM è¾“å‡ºä¸­ [br] æ ‡è®°çš„ä½ç½®

    Args:
        llm_output: LLM è¿”å›çš„å¸¦ [br] æ ‡è®°çš„æ–‡æœ¬

    Returns:
        [br] åœ¨ LLM è¾“å‡ºä¸­çš„å­—ç¬¦ä½ç½®åˆ—è¡¨
    """
    import re
    return [m.start() for m in re.finditer(r'\[br\]', llm_output)]


def find_br_positions_in_original(llm_output: str, original_text: str) -> List[int]:
    """
    ä½¿ç”¨ difflib æ‰¾åˆ° [br] åœ¨åŸå§‹å¥å­ä¸­çš„å­—ç¬¦ä½ç½®

    Args:
        llm_output: LLM è¿”å›çš„å¸¦ [br] æ ‡è®°çš„æ–‡æœ¬
        original_text: åŸå§‹å¥å­æ–‡æœ¬

    Returns:
        [br] åœ¨åŸå§‹æ–‡æœ¬ä¸­çš„å­—ç¬¦ä½ç½®åˆ—è¡¨
    """
    import difflib
    from core.utils.sentence_tools import clean_word

    # 1. æ‰¾åˆ° [br] åœ¨ LLM è¾“å‡ºä¸­çš„ä½ç½®
    br_positions = parse_br_positions(llm_output)

    if not br_positions:
        return []

    # 2. æ¸…æ´—æ–‡æœ¬ç”¨äºåŒ¹é…
    llm_clean = clean_word(llm_output.replace('[br]', ''))
    original_clean = clean_word(original_text)

    # 3. ä½¿ç”¨ difflib åŒ¹é…
    s = difflib.SequenceMatcher(None, llm_clean, original_clean, autojunk=False)
    matching_blocks = s.get_matching_blocks()

    # 4. å»ºç«‹ LLM è¾“å‡ºä½ç½®åˆ°åŸå§‹æ–‡æœ¬ä½ç½®çš„æ˜ å°„
    llm_to_original = {}
    for llm_start, orig_start, length in matching_blocks:
        if length == 0:
            continue
        for i in range(length):
            llm_to_original[llm_start + i] = orig_start + i

    # 5. æ‰¾åˆ°æ¯ä¸ª [br] å¯¹åº”çš„åŸå§‹ä½ç½®
    original_br_positions = []
    for br_pos in br_positions:
        # [br] åœ¨æ¸…æ´—åæ–‡æœ¬ä¸­çš„ä½ç½®ï¼ˆéœ€è¦å»é™¤ä¹‹å‰çš„ [br]ï¼‰
        llm_before_br = llm_output[:br_pos]
        llm_clean_before_br = clean_word(llm_before_br.replace('[br]', ''))

        if llm_clean_before_br in llm_to_original:
            original_pos = llm_to_original[llm_clean_before_br]
            original_br_positions.append(original_pos)

    return original_br_positions


def split_sentence_by_br(sentence: Sentence, llm_output: str) -> List[Sentence]:
    """
    æ ¹æ® LLM è¿”å›çš„ [br] æ ‡è®°æ‹†åˆ† Sentence

    Args:
        sentence: åŸå§‹ Sentence å¯¹è±¡
        llm_output: LLM è¿”å›çš„å¸¦ [br] æ ‡è®°çš„æ–‡æœ¬

    Returns:
        æ‹†åˆ†åçš„ Sentence å¯¹è±¡åˆ—è¡¨
    """
    from core.utils.sentence_tools import clean_word

    # 1. æ‰¾åˆ° [br] åœ¨åŸå§‹å¥å­ä¸­çš„ä½ç½®
    br_positions = find_br_positions_in_original(llm_output, sentence.text)

    if not br_positions:
        # æ²¡æœ‰éœ€è¦æ‹†åˆ†çš„åœ°æ–¹
        return [sentence]

    # 2. æ„å»ºæ¸…æ´—åæ–‡æœ¬åˆ° Chunk çš„æ˜ å°„ï¼ˆå…³é”®ä¿®å¤ï¼šä½¿ç”¨æ¸…æ´—åçš„æ–‡æœ¬ï¼‰
    char_to_chunk = []
    for chunk_idx, chunk in enumerate(sentence.chunks):
        cleaned_chunk_text = clean_word(chunk.text)
        char_to_chunk.extend([chunk_idx] * len(cleaned_chunk_text))

    # 3. æ ¹æ® [br] ä½ç½®ç¡®å®š Chunk æ‹†åˆ†ç‚¹
    split_points = [0]  # èµ·å§‹ç‚¹
    for br_pos in br_positions:
        if br_pos < len(char_to_chunk):
            chunk_idx = char_to_chunk[br_pos]
            if chunk_idx not in split_points:
                split_points.append(chunk_idx)
    split_points.append(len(sentence.chunks))  # ç»“æŸç‚¹
    split_points = sorted(set(split_points))

    # 4. æ‹†åˆ† Chunksï¼Œåˆ›å»ºæ–°çš„ Sentence å¯¹è±¡
    new_sentences = []
    for i in range(len(split_points) - 1):
        start_idx = split_points[i]
        end_idx = split_points[i + 1]

        if start_idx >= end_idx:
            continue

        sub_chunks = sentence.chunks[start_idx:end_idx]

        new_sentence = Sentence(
            chunks=sub_chunks,
            text="".join(c.text for c in sub_chunks),
            start=sub_chunks[0].start,
            end=sub_chunks[-1].end,
            index=sentence.index + i,
            is_split=True
        )
        new_sentences.append(new_sentence)

    return new_sentences


def parallel_split_sentences(sentences, max_length, max_workers, retry_attempt=0):
    """Split sentences in parallel using a thread pool."""
    new_sentences = [None] * len(sentences)
    futures = []

    asr_language = load_key("asr.language")

    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
        for index, sentence in enumerate(sentences):
            # ä½¿ç”¨æœ‰æ•ˆé•¿åº¦åˆ¤æ–­
            effective_length = get_effective_length(sentence, asr_language)
            num_parts = math.ceil(effective_length / max_length)
            if check_length_exceeds(sentence, max_length, asr_language):
                future = executor.submit(split_sentence, sentence, num_parts, max_length, index=index)
                futures.append((future, index, num_parts, sentence))
            else:
                new_sentences[index] = [sentence]

        for future, index, num_parts, sentence in futures:
            split_result = future.result()
            if split_result:
                # å¤„ç† [br] æ ‡è®°ä¸ºæ¢è¡Œ
                if '[br]' in split_result:
                    split_lines = [part.strip() for part in split_result.split('[br]')]
                    split_lines = [line for line in split_lines if line]
                else:
                    # å¦‚æœæ²¡æœ‰åˆ‡åˆ†ï¼Œä¿æŒåŸæ ·
                    split_lines = [split_result.strip()]
                new_sentences[index] = split_lines
            else:
                new_sentences[index] = [sentence]

    return [sentence for sublist in new_sentences for sentence in sublist]

@check_file_exists(_3_2_SPLIT_BY_MEANING)
def split_sentences_by_meaning():
    """
    ä¸»å‡½æ•°ï¼šåˆ‡åˆ†é•¿å¥ (Stage 2)

    è¾“å…¥: split_by_nlp.txt (Stage 1 NLP åˆ†å¥ç»“æœ)
    è¾“å‡º: split_by_meaning.txt (LLM åˆ‡åˆ†é•¿å¥åçš„æœ€ç»ˆç»“æœ)
    """
    console.print("[blue]ğŸ” Starting LLM sentence segmentation (Stage 2)[/blue]")

    # è¯»å– Stage 1 çš„è¾“å‡º
    with open(_3_1_SPLIT_BY_NLP, 'r', encoding='utf-8') as f:
        sentences = [line.strip() for line in f.readlines() if line.strip()]

    console.print(f'[cyan]ğŸ“– Loaded {len(sentences)} sentences from Stage 1 ({_3_1_SPLIT_BY_NLP})[/cyan]')

    # ç»Ÿè®¡éœ€è¦åˆ‡åˆ†çš„å¥å­
    asr_language = load_key("asr.language")
    soft_limit = get_language_length_limit(asr_language, 'origin')
    long_sentences = [s for s in sentences if check_length_exceeds(s, soft_limit, asr_language)]

    if long_sentences:
        console.print(f'[yellow]âš ï¸ Found {len(long_sentences)} long sentences that need LLM splitting[/yellow]')
    else:
        console.print(f'[green]âœ… No long sentences found, all sentences are within limit.[/green]')

    # ğŸ”„ å¤šè½®å¤„ç†ç¡®ä¿æ‰€æœ‰é•¿å¥éƒ½è¢«åˆ‡åˆ†
    for retry_attempt in range(3):
        console.print(f'[cyan]ğŸ”„ Round {retry_attempt + 1}/3: Processing sentences...[/cyan]')
        sentences = parallel_split_sentences(
            sentences,
            max_length=soft_limit,
            max_workers=load_key("max_workers"),
            retry_attempt=retry_attempt
        )

    # ğŸ’¾ ä¿å­˜ç»“æœåˆ°æœ€ç»ˆæ–‡ä»¶
    with open(_3_2_SPLIT_BY_MEANING, 'w', encoding='utf-8') as f:
        f.write('\n'.join(sentences))

    console.print(f'[green]âœ… All sentences processed! Final count: {len(sentences)}[/green]')
    console.print(f'[green]ğŸ’¾ Saved to: {_3_2_SPLIT_BY_MEANING}[/green]')

    return sentences

if __name__ == '__main__':
    # print(split_sentence('Which makes no sense to the... average guy who always pushes the character creation slider all the way to the right.', 2, 22))
    split_sentences_by_meaning()