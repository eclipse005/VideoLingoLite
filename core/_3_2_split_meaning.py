import concurrent.futures
import math
from typing import List

from core.utils import *
from core._2_asr import load_chunks
from rich.console import Console
from core.utils.models import _3_1_SPLIT_BY_NLP, _3_2_SPLIT_BY_MEANING, Sentence

console = Console()


def parse_br_positions(llm_output: str) -> List[int]:
    """
    è§£æ LLM è¾“å‡ºä¸­ [br] æ ‡è®°çš„ä½ç½®

    Args:
        llm_output: LLM è¿”å›çš„å¸¦ [br] æ ‡è®°çš„æ–‡æœ¬

    Returns:
        [br] åœ¨ LLM è¾“å‡ºä¸­çš„å­—ç¬¦ä½ç½®åˆ—è¡¨
    """
    import re
    return [m.start() for m in re.finditer(r'\[br\]', llm_output)]


def find_br_positions_in_original(llm_output: str, original_text: str) -> List[int]:
    """
    ä½¿ç”¨ difflib æ‰¾åˆ° [br] åœ¨åŸå§‹å¥å­ä¸­çš„å­—ç¬¦ä½ç½®

    Args:
        llm_output: LLM è¿”å›çš„å¸¦ [br] æ ‡è®°çš„æ–‡æœ¬
        original_text: åŸå§‹å¥å­æ–‡æœ¬

    Returns:
        [br] åœ¨åŸå§‹æ–‡æœ¬ä¸­çš„å­—ç¬¦ä½ç½®åˆ—è¡¨
    """
    import difflib
    from core.utils.sentence_tools import clean_word

    # 1. æ‰¾åˆ° [br] åœ¨ LLM è¾“å‡ºä¸­çš„ä½ç½®
    br_positions = parse_br_positions(llm_output)

    if not br_positions:
        return []

    # 2. æ¸…æ´—æ–‡æœ¬ç”¨äºåŒ¹é…
    llm_clean = clean_word(llm_output.replace('[br]', ''))
    original_clean = clean_word(original_text)

    # 3. ä½¿ç”¨ difflib åŒ¹é…
    s = difflib.SequenceMatcher(None, llm_clean, original_clean, autojunk=False)
    matching_blocks = s.get_matching_blocks()

    # 4. å»ºç«‹ LLM è¾“å‡ºä½ç½®åˆ°åŸå§‹æ–‡æœ¬ä½ç½®çš„æ˜ å°„
    llm_to_original = {}
    for llm_start, orig_start, length in matching_blocks:
        if length == 0:
            continue
        for i in range(length):
            llm_to_original[llm_start + i] = orig_start + i

    # 5. æ‰¾åˆ°æ¯ä¸ª [br] å¯¹åº”çš„åŸå§‹ä½ç½®
    original_br_positions = []
    for br_pos in br_positions:
        # [br] åœ¨æ¸…æ´—åæ–‡æœ¬ä¸­çš„ä½ç½®ï¼ˆéœ€è¦å»é™¤ä¹‹å‰çš„ [br]ï¼‰
        llm_before_br = llm_output[:br_pos]
        llm_clean_before_br = clean_word(llm_before_br.replace('[br]', ''))

        if llm_clean_before_br in llm_to_original:
            original_pos = llm_to_original[llm_clean_before_br]
            original_br_positions.append(original_pos)

    return original_br_positions


def split_sentence_by_br(sentence: Sentence, llm_output: str) -> List[Sentence]:
    """
    æ ¹æ® LLM è¿”å›çš„ [br] æ ‡è®°æ‹†åˆ† Sentence

    Args:
        sentence: åŸå§‹ Sentence å¯¹è±¡
        llm_output: LLM è¿”å›çš„å¸¦ [br] æ ‡è®°çš„æ–‡æœ¬

    Returns:
        æ‹†åˆ†åçš„ Sentence å¯¹è±¡åˆ—è¡¨
    """
    from core.utils.sentence_tools import clean_word

    # 1. æ‰¾åˆ° [br] åœ¨åŸå§‹å¥å­ä¸­çš„ä½ç½®
    br_positions = find_br_positions_in_original(llm_output, sentence.text)

    if not br_positions:
        # æ²¡æœ‰éœ€è¦æ‹†åˆ†çš„åœ°æ–¹
        return [sentence]

    # 2. æ„å»ºæ¸…æ´—åæ–‡æœ¬åˆ° Chunk çš„æ˜ å°„ï¼ˆå…³é”®ä¿®å¤ï¼šä½¿ç”¨æ¸…æ´—åçš„æ–‡æœ¬ï¼‰
    # æ³¨æ„ï¼šè¿™é‡Œä¸éœ€è¦æ·»åŠ ç©ºæ ¼ï¼Œå› ä¸º find_br_positions_in_original è¿”å›çš„ä½ç½®
    # æ˜¯åŸºäºæ¸…æ´—åçš„æ–‡æœ¬ï¼ˆclean_word å»é™¤äº†ç©ºæ ¼ï¼‰ï¼Œæ‰€ä»¥ char_to_chunk ä¹Ÿåº”è¯¥ä¸åŒ…å«ç©ºæ ¼
    char_to_chunk = []
    for chunk_idx, chunk in enumerate(sentence.chunks):
        cleaned_chunk_text = clean_word(chunk.text)
        char_to_chunk.extend([chunk_idx] * len(cleaned_chunk_text))

    # 3. æ ¹æ® [br] ä½ç½®ç¡®å®š Chunk æ‹†åˆ†ç‚¹
    split_points = [0]  # èµ·å§‹ç‚¹
    for br_pos in br_positions:
        if br_pos < len(char_to_chunk):
            chunk_idx = char_to_chunk[br_pos]
            if chunk_idx not in split_points:
                split_points.append(chunk_idx)
    split_points.append(len(sentence.chunks))  # ç»“æŸç‚¹
    split_points = sorted(set(split_points))

    # 4. æ‹†åˆ† Chunksï¼Œåˆ›å»ºæ–°çš„ Sentence å¯¹è±¡
    new_sentences = []
    for i in range(len(split_points) - 1):
        start_idx = split_points[i]
        end_idx = split_points[i + 1]

        if start_idx >= end_idx:
            continue

        sub_chunks = sentence.chunks[start_idx:end_idx]

        # ä½¿ç”¨ joiner æ‹¼æ¥å­å¥æ–‡æœ¬
        asr_language = load_key("asr.language")
        joiner = get_joiner(asr_language)
        sub_text = joiner.join(c.text for c in sub_chunks)

        new_sentence = Sentence(
            chunks=sub_chunks,
            text=sub_text,
            start=sub_chunks[0].start,
            end=sub_chunks[-1].end,
            index=sentence.index + i,
            is_split=True
        )
        new_sentences.append(new_sentence)

    return new_sentences


def parallel_split_sentences(sentences: List[Sentence], max_length: int, max_workers: int, retry_attempt: int = 0) -> List[Sentence]:
    """
    Split sentences in parallel using a thread pool.

    Args:
        sentences: List of Sentence objects to process
        max_length: Maximum effective length per sentence
        max_workers: Number of parallel workers
        retry_attempt: Retry attempt number (for logging)

    Returns:
        Flattened list of Sentence objects (split as needed)
    """
    new_sentences = [None] * len(sentences)
    futures = []

    asr_language = load_key("asr.language")

    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
        for index, sentence in enumerate(sentences):
            # Skip if already split
            if sentence.is_split:
                new_sentences[index] = [sentence]
                continue

            # Use effective length for Sentence object
            effective_length = get_effective_length(sentence.text, asr_language)
            num_parts = math.ceil(effective_length / max_length)

            if check_length_exceeds(sentence.text, max_length, asr_language):
                # Submit LLM task with sentence.text
                future = executor.submit(split_sentence, sentence.text, num_parts, max_length, index=index)
                futures.append((future, index, sentence))
            else:
                new_sentences[index] = [sentence]

        for future, index, sentence in futures:
            split_result = future.result()
            if split_result and '[br]' in split_result:
                # Use split_sentence_by_br to split the Sentence object
                split_sentence_list = split_sentence_by_br(sentence, split_result)
                new_sentences[index] = split_sentence_list
            else:
                # No splitting occurred, keep original
                new_sentences[index] = [sentence]

    # Flatten the list of lists
    return [s for sublist in new_sentences for s in sublist]

def split_sentences_by_meaning(sentences: List[Sentence]) -> List[Sentence]:
    """
    ä¸»å‡½æ•°ï¼šåˆ‡åˆ†é•¿å¥ (Stage 2)

    Args:
        sentences: Sentence å¯¹è±¡åˆ—è¡¨

    Returns:
        List[Sentence]: åˆ‡åˆ†åçš„ Sentence å¯¹è±¡åˆ—è¡¨
    """
    console.print("[blue]ğŸ” Starting LLM sentence segmentation (Stage 2)[/blue]")

    console.print(f'[cyan]ğŸ“– Loaded {len(sentences)} sentences from Stage 1[/cyan]')
    console.print(f'[green]âœ… Received {len(sentences)} Sentence objects from Stage 1[/green]')
    console.print(f"[dim]First sentence has {len(sentences[0].chunks)} chunks | Time: {sentences[0].start:.2f}s - {sentences[0].end:.2f}s[/dim]")

    # ç»Ÿè®¡éœ€è¦åˆ‡åˆ†çš„å¥å­
    asr_language = load_key("asr.language")
    soft_limit = get_language_length_limit(asr_language, 'origin')
    long_sentences = [s for s in sentences if check_length_exceeds(s.text, soft_limit, asr_language)]

    if long_sentences:
        console.print(f'[yellow]âš ï¸ Found {len(long_sentences)} long sentences that need LLM splitting[/yellow]')
    else:
        console.print(f'[green]âœ… No long sentences found, all sentences are within limit.[/green]')

    # ğŸ”„ å¤šè½®å¤„ç†ç¡®ä¿æ‰€æœ‰é•¿å¥éƒ½è¢«åˆ‡åˆ†
    for retry_attempt in range(3):
        console.print(f'[cyan]ğŸ”„ Round {retry_attempt + 1}/3: Processing sentences...[/cyan]')
        sentences = parallel_split_sentences(
            sentences,
            max_length=soft_limit,
            max_workers=load_key("max_workers"),
            retry_attempt=retry_attempt
        )

    # ğŸ’¾ ä¿å­˜ç»“æœåˆ°æœ€ç»ˆæ–‡ä»¶
    with open(_3_2_SPLIT_BY_MEANING, 'w', encoding='utf-8') as f:
        f.write('\n'.join(sent.text for sent in sentences))

    console.print(f'[green]âœ… All sentences processed! Final count: {len(sentences)}[/green]')
    console.print(f'[cyan]ğŸ“Š Returning {len(sentences)} Sentence objects to Stage 3[/cyan]')
    console.print(f'[green]ğŸ’¾ Saved to: {_3_2_SPLIT_BY_MEANING}[/green]')

    return sentences

if __name__ == '__main__':
    # print(split_sentence('Which makes no sense to the... average guy who always pushes the character creation slider all the way to the right.', 2, 22))
    split_sentences_by_meaning()