import concurrent.futures
from difflib import SequenceMatcher
import math
import os
import shutil
import re
from core.prompts import get_split_prompt
from core.utils import *
from rich.console import Console
from rich.table import Table
from core.utils.models import _3_2_SPLIT_BY_MEANING_RAW, _3_2_SPLIT_BY_MEANING
console = Console()


# ================================================================
# Tokenization (æ›¿ä»£ spacyï¼Œæ”¯æŒ CJK)
# ================================================================

def get_word_count(sentence):
    """
    è·å–å¥å­è¯æ•°ï¼Œå…¼å®¹ CJK è¯­è¨€
    - ç©ºæ ¼åˆ†éš”è¯­è¨€ (en, es, fr, etc.): æŒ‰ç©ºæ ¼è®¡æ•°
    - CJK è¯­è¨€ (zh, ja, ko): è‹±æ–‡å•è¯æŒ‰ç©ºæ ¼ï¼Œä¸­æ–‡å­—ç¬¦æŒ‰å­—ç¬¦
    """
    asr_language = load_key("asr.language")
    is_cjk = asr_language.lower() in ['zh', 'chinese', 'ja', 'japanese', 'ko', 'korean']

    if is_cjk:
        # CJK: ä¿æŒè‹±æ–‡+æ•°å­—å¤åˆè¯å®Œæ•´ï¼Œä¸­æ–‡æŒ‰å­—ç¬¦åˆ†å‰²
        # [a-zA-Z]+(?:[0-9]+\.?[0-9]*|[.-][0-9]+)*  ä¿æŒè‹±æ–‡+digits (å¦‚ GPT-4, Python3.9)
        # [^\s]  å…¶ä»–å­—ç¬¦ï¼ˆä¸»è¦æ˜¯ä¸­æ–‡å•å­—ç¬¦ï¼‰
        words = re.findall(r'[a-zA-Z]+(?:[0-9]+\.?[0-9]*|[.-][0-9]+)*|[^\s]',
                          sentence.replace(' ', ''))
        return len(words)
    else:
        # ç©ºæ ¼åˆ†éš”è¯­è¨€: æŒ‰ç©ºæ ¼è®¡æ•°
        return len(sentence.split())


def tokenize_sentence(sentence):
    """ç®€å•åˆ†è¯ï¼Œæ›¿ä»£ spacy"""
    asr_language = load_key("asr.language")
    is_cjk = asr_language.lower() in ['zh', 'chinese', 'ja', 'japanese', 'ko', 'korean']

    if is_cjk:
        # CJK: è‹±æ–‡å•è¯ + ä¸­æ–‡å­—ç¬¦
        words = re.findall(r'[a-zA-Z]+(?:[0-9]+\.?[0-9]*|[.-][0-9]+)*|[^\s]',
                          sentence.replace(' ', ''))
    else:
        # ç©ºæ ¼åˆ†éš”è¯­è¨€: æŒ‰ç©ºæ ¼åˆ†å‰²
        words = sentence.split()

    return words

# ================================================================
# [br] æ˜ å°„: å€Ÿç”¨ _3_llm_sentence_split.py çš„ map_br_to_original_sentence
# ================================================================

def map_br_to_raw_positions(original_sentence, llm_sentence_with_br):
    """
    å°† LLM è¿”å›å¥ä¸­çš„ [br] æ˜ å°„å›åŸå¥çš„å­—ç¬¦ä½ç½®ã€‚

    å€Ÿç”¨è‡ª _3_llm_sentence_split.py:201-333 çš„é€»è¾‘ï¼Œç®€åŒ–ä¸ºçº¯ä½ç½®è¿”å›ã€‚

    ç‰¹æ€§ï¼š
    1. ä¿®æ­£ difflib å¯¹é½åå·®å¯¼è‡´çš„å•è¯å†…åˆ‡åˆ†
    2. ä¿æŠ¤ CJK (ä¸­æ—¥éŸ©) è¯­è¨€çš„å­—ç¬¦é—´åˆ‡åˆ†
    3. è·³è¿‡æ ‡ç‚¹ç¬¦å·
    """
    if '[br]' not in llm_sentence_with_br:
        return []

    # --- è¾…åŠ©å‡½æ•° ---
    def is_cjk(char):
        """åˆ¤æ–­å­—ç¬¦æ˜¯å¦ä¸ºä¸­æ—¥éŸ©å­—ç¬¦"""
        code = ord(char)
        return (
            (0x4E00 <= code <= 0x9FFF) or  # æ±‰å­—
            (0x3040 <= code <= 0x309F) or  # å¹³å‡å
            (0x30A0 <= code <= 0x30FF) or  # ç‰‡å‡å
            (0xAC00 <= code <= 0xD7AF)     # éŸ©æ–‡
        )

    def get_clean_chars(text):
        """åªæå–å­—æ¯ã€æ•°å­—ã€æ±‰å­—ç­‰æœ‰æ•ˆå­—ç¬¦ç”¨äºå¯¹é½"""
        return [c for c in text if re.match(r'\w', c)]

    # --- 1. æ•°æ®å‡†å¤‡ ---
    orig_clean_chars = get_clean_chars(original_sentence)
    orig_clean_str = "".join(orig_clean_chars)

    llm_text_no_br = llm_sentence_with_br.replace('[br]', '')
    llm_clean_chars = get_clean_chars(llm_text_no_br)
    llm_clean_str = "".join(llm_clean_chars)

    # --- 2. ç¡®å®š LLM ä¸­çš„åˆ‡åˆ†ç‚¹ (ç¬¬ N ä¸ªæœ‰æ•ˆå­—ç¬¦å) ---
    br_anchor_indices = []
    parts = llm_sentence_with_br.split('[br]')
    current_valid_char_count = 0

    for part in parts[:-1]:
        part_clean_count = len(get_clean_chars(part))
        current_valid_char_count += part_clean_count
        br_anchor_indices.append(current_valid_char_count)

    # --- 3. ä½¿ç”¨ Diff ç®—æ³•æ˜ å°„ç´¢å¼• ---
    matcher = SequenceMatcher(None, orig_clean_str, llm_clean_str, autojunk=False)
    matching_blocks = matcher.get_matching_blocks()

    mapped_clean_indices = []

    for llm_idx in br_anchor_indices:
        best_match_idx = -1
        # å°è¯•æ‰¾åˆ°åŒ…å«è¯¥ LLM ç´¢å¼•çš„åŒ¹é…å—
        for a_start, b_start, length in matching_blocks:
            b_end = b_start + length
            if b_start <= llm_idx <= b_end:
                offset = llm_idx - b_start
                best_match_idx = a_start + offset
                break

        # å¦‚æœè½åœ¨å·®å¼‚åŒºï¼Œæ‰¾æœ€è¿‘çš„å‰ä¸€ä¸ªåŒ¹é…å—æœ«å°¾
        if best_match_idx == -1:
            closest_preceding_match_end = 0
            for a_start, b_start, length in matching_blocks:
                b_end = b_start + length
                if b_end <= llm_idx:
                    closest_preceding_match_end = a_start + length
                else:
                    break
            best_match_idx = closest_preceding_match_end

        best_match_idx = min(best_match_idx, len(orig_clean_str))
        mapped_clean_indices.append(best_match_idx)

    # --- 4. å°† Clean ç´¢å¼•è½¬å› Raw ç´¢å¼• ---
    raw_insert_positions = []
    for target_clean_idx in mapped_clean_indices:
        raw_idx = 0
        clean_counter = 0
        found = False

        for char in original_sentence:
            if re.match(r'\w', char):
                clean_counter += 1
            raw_idx += 1
            if clean_counter == target_clean_idx:
                raw_insert_positions.append(raw_idx)
                found = True
                break

        if not found:
            raw_insert_positions.append(len(original_sentence))

    # --- 5. è¾¹ç•Œä¿æŠ¤: é¿å…åˆ‡åœ¨è‹±æ–‡å•è¯ä¸­é—´ ---
    final_positions = []
    for pos in raw_insert_positions:
        current_pos = pos

        # æ£€æŸ¥å½“å‰æ’å…¥ç‚¹å·¦å³æ˜¯å¦éƒ½æ˜¯"å•è¯å­—ç¬¦"ï¼Œä¸”éƒ½ä¸æ˜¯ CJK
        while (current_pos > 0 and current_pos < len(original_sentence)):
            prev_char = original_sentence[current_pos - 1]
            curr_char = original_sentence[current_pos] if current_pos < len(original_sentence) else ''

            is_prev_word = re.match(r'\w', prev_char) is not None
            is_curr_word = re.match(r'\w', curr_char) is not None if curr_char else False

            # é‡åˆ°éå•è¯å­—ç¬¦ï¼ˆç©ºæ ¼ã€æ ‡ç‚¹ï¼‰ï¼Œåœæ­¢æ»‘åŠ¨
            if not (is_prev_word and is_curr_word):
                break

            # é‡åˆ° CJKï¼Œåœæ­¢æ»‘åŠ¨
            if is_cjk(prev_char) or is_cjk(curr_char):
                break

            # åˆ‡æ–­è‹±æ–‡å•è¯ â†’ å‘åç§»
            current_pos += 1

        # æ ‡ç‚¹è·³è¿‡: ä¸è¦æ’åœ¨æ ‡ç‚¹å‰é¢
        while current_pos < len(original_sentence) and original_sentence[current_pos] in ",.!?;:ï¼Œã€‚ï¼ï¼Ÿï¼›ï¼š":
            current_pos += 1

        final_positions.append(current_pos)

    return final_positions

def split_sentence(sentence, num_parts, word_limit=20, index=-1, retry_attempt=0):
    """Split a long sentence using LLM and return the result as a string."""
    split_prompt = get_split_prompt(sentence, num_parts, word_limit)

    def valid_split(response_data):
        choice = response_data.get("choice", "1")
        split_key = f'split{choice}'
        if split_key not in response_data:
            return {"status": "error", "message": f"Missing required key: `{split_key}`"}
        if "[br]" not in response_data[split_key]:
            return {"status": "success", "message": "Split not required by LLM."}
        return {"status": "success", "message": "Split completed"}

    response_data = ask_gpt(
        split_prompt,
        resp_type='json',
        valid_def=valid_split,
        log_title='split_long_sentence'
    )

    choice = response_data.get("choice", "1")
    best_split = response_data.get(f"split{choice}", sentence)

    # ä½¿ç”¨æ–°çš„ map_br_to_raw_positions å‡½æ•°
    if '[br]' in best_split:
        split_positions = map_br_to_raw_positions(sentence, best_split)

        # æŒ‰åˆ‡åˆ†ç‚¹é‡å»ºå¥å­
        if split_positions:
            result_parts = []
            last_pos = 0
            for pos in split_positions:
                if pos > last_pos:
                    result_parts.append(sentence[last_pos:pos].strip())
                    last_pos = pos
            if last_pos < len(sentence):
                result_parts.append(sentence[last_pos:].strip())

            best_split = '\n'.join(result_parts)

            if index != -1:
                console.print(f'[green]âœ… Sentence {index} has been successfully split[/green]')

            # æ˜¾ç¤ºåˆ‡åˆ†ç»“æœ
            table = Table(title="")
            table.add_column("Type", style="cyan")
            table.add_column("Sentence")
            table.add_row("Original", sentence, style="yellow")
            table.add_row("Split", best_split.replace('\n', ' ||'), style="yellow")
            console.print(table)

    return best_split

def parallel_split_sentences(sentences, max_length, max_workers, retry_attempt=0):
    """Split sentences in parallel using a thread pool."""
    new_sentences = [None] * len(sentences)
    futures = []

    asr_language = load_key("asr.language")

    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
        for index, sentence in enumerate(sentences):
            # ä½¿ç”¨æœ‰æ•ˆé•¿åº¦åˆ¤æ–­
            effective_length = get_effective_length(sentence, asr_language)
            num_parts = math.ceil(effective_length / max_length)
            if check_length_exceeds(sentence, max_length, asr_language):
                future = executor.submit(split_sentence, sentence, num_parts, max_length, index=index, retry_attempt=retry_attempt)
                futures.append((future, index, num_parts, sentence))
            else:
                new_sentences[index] = [sentence]

        for future, index, num_parts, sentence in futures:
            split_result = future.result()
            if split_result:
                split_lines = split_result.strip().split('\n')
                new_sentences[index] = [line.strip() for line in split_lines]
            else:
                new_sentences[index] = [sentence]

    return [sentence for sublist in new_sentences for sentence in sublist]

@check_file_exists(_3_2_SPLIT_BY_MEANING)
def split_sentences_by_meaning():
    """
    ä¸»å‡½æ•°ï¼šåˆ‡åˆ†é•¿å¥

    è¾“å…¥: split_by_meaning_raw.txt (ç”± LLM ç»„å¥æˆ– Parakeet segments ç”Ÿæˆ)
    è¾“å‡º: split_by_meaning.txt (åˆ‡åˆ†é•¿å¥åçš„æœ€ç»ˆç»“æœ)
    """
    # è¯»å–è¾“å…¥å¥å­ (raw)
    with open(_3_2_SPLIT_BY_MEANING_RAW, 'r', encoding='utf-8') as f:
        sentences = [line.strip() for line in f.readlines() if line.strip()]

    console.print(f'[cyan]ğŸ“– Loaded {len(sentences)} sentences from {_3_2_SPLIT_BY_MEANING_RAW}[/cyan]')

    # ç»Ÿè®¡éœ€è¦åˆ‡åˆ†çš„å¥å­
    asr_language = load_key("asr.language")
    soft_limit = get_language_length_limit(asr_language, 'origin')
    hard_limit = get_hard_limit(soft_limit, asr_language)
    long_sentences = [s for s in sentences if check_length_exceeds(s, soft_limit, asr_language)]

    if long_sentences:
        console.print(f'[yellow]âš ï¸ Found {len(long_sentences)} long sentences (> {hard_limit})[/yellow]')
    else:
        console.print(f'[green]âœ… No long sentences found, all sentences are within limit.[/green]')
        # ç›´æ¥å¤åˆ¶åˆ°æœ€ç»ˆæ–‡ä»¶
        shutil.copy(_3_2_SPLIT_BY_MEANING_RAW, _3_2_SPLIT_BY_MEANING)
        console.print(f'[green]ğŸ’¾ Copied to: {_3_2_SPLIT_BY_MEANING}[/green]')
        return sentences

    # ğŸ”„ å¤šè½®å¤„ç†ç¡®ä¿æ‰€æœ‰é•¿å¥éƒ½è¢«åˆ‡åˆ†
    for retry_attempt in range(3):
        console.print(f'[cyan]ğŸ”„ Round {retry_attempt + 1}/3: Processing sentences...[/cyan]')
        sentences = parallel_split_sentences(
            sentences,
            max_length=soft_limit,
            max_workers=load_key("max_workers"),
            retry_attempt=retry_attempt
        )

    # ğŸ’¾ ä¿å­˜ç»“æœåˆ°æœ€ç»ˆæ–‡ä»¶
    with open(_3_2_SPLIT_BY_MEANING, 'w', encoding='utf-8') as f:
        f.write('\n'.join(sentences))

    console.print(f'[green]âœ… All sentences processed! Final count: {len(sentences)}[/green]')
    console.print(f'[green]ğŸ’¾ Saved to: {_3_2_SPLIT_BY_MEANING}[/green]')

    return sentences

if __name__ == '__main__':
    # print(split_sentence('Which makes no sense to the... average guy who always pushes the character creation slider all the way to the right.', 2, 22))
    split_sentences_by_meaning()