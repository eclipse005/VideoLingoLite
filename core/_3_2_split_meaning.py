import concurrent.futures
import math
from typing import List

from core.utils import *
from core._2_asr import load_chunks
from rich.console import Console
from rich.progress import Progress, SpinnerColumn, TextColumn, BarColumn
from core.utils.models import _3_1_SPLIT_BY_NLP, _3_2_SPLIT_BY_MEANING, _CACHE_SENTENCES_SPLIT, Sentence
import time

console = Console()


def parse_br_positions(llm_output: str) -> List[int]:
    """
    è§£æ LLM è¾“å‡ºä¸­ [br] æ ‡è®°çš„ä½ç½®

    Args:
        llm_output: LLM è¿”å›çš„å¸¦ [br] æ ‡è®°çš„æ–‡æœ¬

    Returns:
        [br] åœ¨ LLM è¾“å‡ºä¸­çš„å­—ç¬¦ä½ç½®åˆ—è¡¨
    """
    import re
    return [m.start() for m in re.finditer(r'\[br\]', llm_output)]


def find_br_positions_in_original(llm_output: str, original_text: str) -> List[int]:
    """
    ä½¿ç”¨ difflib æ‰¾åˆ° [br] åœ¨åŸå§‹å¥å­ä¸­çš„å­—ç¬¦ä½ç½®

    Args:
        llm_output: LLM è¿”å›çš„å¸¦ [br] æ ‡è®°çš„æ–‡æœ¬
        original_text: åŸå§‹å¥å­æ–‡æœ¬

    Returns:
        [br] åœ¨åŸå§‹æ–‡æœ¬ä¸­çš„å­—ç¬¦ä½ç½®åˆ—è¡¨
    """
    import difflib
    from core.utils.sentence_tools import clean_word

    # 1. æ‰¾åˆ° [br] åœ¨ LLM è¾“å‡ºä¸­çš„ä½ç½®
    br_positions = parse_br_positions(llm_output)

    if not br_positions:
        return []

    # 2. æ¸…æ´—æ–‡æœ¬ç”¨äºåŒ¹é…
    llm_clean = clean_word(llm_output.replace('[br]', ''))
    original_clean = clean_word(original_text)

    # 3. ä½¿ç”¨ difflib åŒ¹é…
    s = difflib.SequenceMatcher(None, llm_clean, original_clean, autojunk=False)
    matching_blocks = s.get_matching_blocks()

    # 4. å»ºç«‹ LLM è¾“å‡ºä½ç½®åˆ°åŸå§‹æ–‡æœ¬ä½ç½®çš„æ˜ å°„
    llm_to_original = {}
    for llm_start, orig_start, length in matching_blocks:
        if length == 0:
            continue
        for i in range(length):
            llm_to_original[llm_start + i] = orig_start + i

    # 5. æ‰¾åˆ°æ¯ä¸ª [br] å¯¹åº”çš„åŸå§‹ä½ç½®
    original_br_positions = []
    for br_pos in br_positions:
        # [br] åœ¨æ¸…æ´—åæ–‡æœ¬ä¸­çš„ä½ç½®ï¼ˆéœ€è¦å»é™¤ä¹‹å‰çš„ [br]ï¼‰
        llm_before_br = llm_output[:br_pos]
        llm_clean_before_br = clean_word(llm_before_br.replace('[br]', ''))

        # ä½¿ç”¨æ¸…æ´—åæ–‡æœ¬çš„é•¿åº¦ï¼ˆå­—ç¬¦ä½ç½®ï¼‰æ¥æŸ¥æ‰¾æ˜ å°„
        clean_pos = len(llm_clean_before_br)
        if clean_pos in llm_to_original:
            original_pos = llm_to_original[clean_pos]
            original_br_positions.append(original_pos)

    return original_br_positions


def split_sentence_by_br(sentence: Sentence, llm_output: str) -> List[Sentence]:
    """
    æ ¹æ® LLM è¿”å›çš„ [br] æ ‡è®°æ‹†åˆ† Sentence

    Args:
        sentence: åŸå§‹ Sentence å¯¹è±¡
        llm_output: LLM è¿”å›çš„å¸¦ [br] æ ‡è®°çš„æ–‡æœ¬

    Returns:
        æ‹†åˆ†åçš„ Sentence å¯¹è±¡åˆ—è¡¨
    """
    from core.utils.sentence_tools import clean_word

    # 1. æ‰¾åˆ° [br] åœ¨åŸå§‹å¥å­ä¸­çš„ä½ç½®
    br_positions = find_br_positions_in_original(llm_output, sentence.text)

    if not br_positions:
        # æ²¡æœ‰éœ€è¦æ‹†åˆ†çš„åœ°æ–¹
        return [sentence]

    # 2. æ„å»ºæ¸…æ´—åæ–‡æœ¬åˆ° Chunk çš„æ˜ å°„ï¼ˆå…³é”®ä¿®å¤ï¼šä½¿ç”¨æ¸…æ´—åçš„æ–‡æœ¬ï¼‰
    # æ³¨æ„ï¼šè¿™é‡Œä¸éœ€è¦æ·»åŠ ç©ºæ ¼ï¼Œå› ä¸º find_br_positions_in_original è¿”å›çš„ä½ç½®
    # æ˜¯åŸºäºæ¸…æ´—åçš„æ–‡æœ¬ï¼ˆclean_word å»é™¤äº†ç©ºæ ¼ï¼‰ï¼Œæ‰€ä»¥ char_to_chunk ä¹Ÿåº”è¯¥ä¸åŒ…å«ç©ºæ ¼
    char_to_chunk = []
    for chunk_idx, chunk in enumerate(sentence.chunks):
        cleaned_chunk_text = clean_word(chunk.text)
        char_to_chunk.extend([chunk_idx] * len(cleaned_chunk_text))

    # 3. æ ¹æ® [br] ä½ç½®ç¡®å®š Chunk æ‹†åˆ†ç‚¹
    split_points = [0]  # èµ·å§‹ç‚¹
    for br_pos in br_positions:
        if br_pos < len(char_to_chunk):
            chunk_idx = char_to_chunk[br_pos]
            if chunk_idx not in split_points:
                split_points.append(chunk_idx)
    split_points.append(len(sentence.chunks))  # ç»“æŸç‚¹
    split_points = sorted(set(split_points))

    # 4. æ‹†åˆ† Chunksï¼Œåˆ›å»ºæ–°çš„ Sentence å¯¹è±¡
    new_sentences = []
    for i in range(len(split_points) - 1):
        start_idx = split_points[i]
        end_idx = split_points[i + 1]

        if start_idx >= end_idx:
            continue

        sub_chunks = sentence.chunks[start_idx:end_idx]

        # ä½¿ç”¨ joiner æ‹¼æ¥å­å¥æ–‡æœ¬
        asr_language = load_key("asr.language")
        joiner = get_joiner(asr_language)
        sub_text = joiner.join(c.text for c in sub_chunks)

        new_sentence = Sentence(
            chunks=sub_chunks,
            text=sub_text,
            start=sub_chunks[0].start,
            end=sub_chunks[-1].end,
            index=sentence.index + i,
            is_split=True
        )
        new_sentences.append(new_sentence)

    return new_sentences


def parallel_split_sentences(sentences: List[Sentence], max_length: int, max_workers: int, retry_attempt: int = 0) -> List[Sentence]:
    """
    Split sentences in parallel using a thread pool.

    Args:
        sentences: List of Sentence objects to process
        max_length: Maximum effective length per sentence
        max_workers: Number of parallel workers
        retry_attempt: Retry attempt number (for logging)

    Returns:
        Flattened list of Sentence objects (split as needed)
    """
    new_sentences = [None] * len(sentences)
    futures = []

    asr_language = load_key("asr.language")

    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
        for index, sentence in enumerate(sentences):
            # Skip if already split
            if sentence.is_split:
                new_sentences[index] = [sentence]
                continue

            # Use effective length for Sentence object
            effective_length = get_effective_length(sentence.text, asr_language)
            num_parts = math.ceil(effective_length / max_length)

            if check_length_exceeds(sentence.text, max_length, asr_language):
                # Submit LLM task with sentence.text
                future = executor.submit(split_sentence, sentence.text, num_parts, max_length, index=index)
                futures.append((future, index, sentence))
            else:
                new_sentences[index] = [sentence]

        total = len(futures)
        # åˆ›å»º future åˆ° (index, sentence) çš„æ˜ å°„
        future_to_data = {future: (index, sentence) for future, index, sentence in futures}

        # åªæœ‰åœ¨æœ‰ä»»åŠ¡æ—¶æ‰æ˜¾ç¤ºè¿›åº¦æ¡
        if total > 0:
            with Progress(
                SpinnerColumn(),
                TextColumn("[progress.description]{task.description}"),
                BarColumn(),
                TextColumn("[progress.percentage]{task.percentage:>3.0f}%"),
                TextColumn("({task.completed}/{task.total})"),
                transient=False,
                console=console
            ) as progress:
                task = progress.add_task(f"[cyan]ç¬¬ {retry_attempt + 1} è½®: åˆ‡åˆ†å¥å­...", total=total)

                for future in concurrent.futures.as_completed(future_to_data.keys()):
                    index, sentence = future_to_data[future]
                    split_result = future.result()

                    if split_result and '[br]' in split_result:
                        # Use split_sentence_by_br to split the Sentence object
                        split_sentence_list = split_sentence_by_br(sentence, split_result)
                        new_sentences[index] = split_sentence_list
                    else:
                        # No splitting occurred, keep original
                        new_sentences[index] = [sentence]

                    progress.update(task, advance=1)
        else:
            # æ²¡æœ‰éœ€è¦åˆ‡åˆ†çš„å¥å­ï¼Œç›´æ¥è·³è¿‡
            pass

    # Flatten the list of lists
    return [s for sublist in new_sentences for s in sublist]

@cache_objects(_CACHE_SENTENCES_SPLIT, _3_2_SPLIT_BY_MEANING)
def split_sentences_by_meaning(sentences: List[Sentence]) -> List[Sentence]:
    """
    ä¸»å‡½æ•°ï¼šåˆ‡åˆ†é•¿å¥ (Stage 2)

    Args:
        sentences: Sentence å¯¹è±¡åˆ—è¡¨

    Returns:
        List[Sentence]: åˆ‡åˆ†åçš„ Sentence å¯¹è±¡åˆ—è¡¨
    """
    console.print("[blue]ğŸ” å¼€å§‹ LLM é•¿å¥åˆ‡åˆ† (Stage 2)[/blue]")

    start_time = time.time()

    # ç»Ÿè®¡éœ€è¦åˆ‡åˆ†çš„å¥å­
    asr_language = load_key("asr.language")
    soft_limit = get_language_length_limit(asr_language, 'origin')
    long_sentences = [s for s in sentences if check_length_exceeds(s.text, soft_limit, asr_language)]

    if long_sentences:
        console.print(f'[yellow]âš ï¸ å‘ç° {len(long_sentences)} ä¸ªéœ€è¦æ‹†åˆ†çš„é•¿å¥[/yellow]')
    else:
        console.print(f'[green]âœ… æ‰€æœ‰å¥å­é•¿åº¦ç¬¦åˆè¦æ±‚ï¼Œæ— éœ€æ‹†åˆ†[/green]')

    # ğŸ”„ å¤šè½®å¤„ç†ç¡®ä¿æ‰€æœ‰é•¿å¥éƒ½è¢«åˆ‡åˆ†
    for retry_attempt in range(3):
        # æ£€æŸ¥æ˜¯å¦è¿˜æœ‰éœ€è¦åˆ‡åˆ†çš„å¥å­
        remaining_long = [s for s in sentences if check_length_exceeds(s.text, soft_limit, asr_language)]
        if not remaining_long:
            if retry_attempt > 0:
                console.print(f'[green]âœ… æ‰€æœ‰å¥å­å·²ç¬¦åˆé•¿åº¦é™åˆ¶[/green]')
            break

        # åªæœ‰åœ¨æœ‰éœ€è¦åˆ‡åˆ†çš„å¥å­æ—¶æ‰è°ƒç”¨å¤„ç†å‡½æ•°
        sentences = parallel_split_sentences(
            sentences,
            max_length=soft_limit,
            max_workers=load_key("max_workers"),
            retry_attempt=retry_attempt
        )

    elapsed = time.time() - start_time
    console.print(f"[dim]â±ï¸ LLM é•¿å¥åˆ‡åˆ†è€—æ—¶: {format_duration(elapsed)}[/dim]")

    console.print(f'[green]âœ… å¤„ç†å®Œæˆï¼æœ€ç»ˆå¥å­æ•°: {len(sentences)}[/green]')

    return sentences

def load_sentences() -> List[Sentence]:
    """
    ä»ç¼“å­˜åŠ è½½ Sentence å¯¹è±¡

    Returns:
        List[Sentence]: Sentence å¯¹è±¡åˆ—è¡¨
    """
    import pickle
    import os
    if os.path.exists(_CACHE_SENTENCES_SPLIT):
        with open(_CACHE_SENTENCES_SPLIT, 'rb') as f:
            return pickle.load(f)
    else:
        raise FileNotFoundError(f"ç¼“å­˜æ–‡ä»¶ä¸å­˜åœ¨: {_CACHE_SENTENCES_SPLIT}")


if __name__ == '__main__':
    split_sentences_by_meaning()