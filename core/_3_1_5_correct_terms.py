"""
ASR æœ¯è¯­çŸ«æ­£æ¨¡å— (Stage 1.5)

åœ¨ NLP åˆ†å¥åè¿›è¡Œï¼Œä½¿ç”¨ LLM Agent æ™ºèƒ½è¯†åˆ«å¹¶çŸ«æ­£ ASR é”™è¯¯

è¾“å…¥: List[Sentence] (æ¥è‡ª _3_1_split_nlp.py)
è¾“å‡º: List[Sentence] (çŸ«æ­£åï¼Œä¼ é€’ç»™ _3_2_split_meaning.py)
"""

import json
import re
from typing import List, Tuple, Dict, Any, Optional

from core.utils import rprint, load_key, timer
from core.utils.ask_gpt import ask_gpt_with_tools
from core.utils.models import Sentence
from core.utils.sentence_tools import clean_word


# ==================== å·¥å…·å®šä¹‰ ====================

SENTENCES_TOOLS_DEFINITION = [
    {
        "type": "function",
        "function": {
            "name": "read_sentences",
            "description": "è¯»å–å¥å­å†…å®¹ï¼Œè¿”å›å…¨éƒ¨æˆ–æŒ‡å®šç´¢å¼•èŒƒå›´",
            "parameters": {
                "type": "object",
                "properties": {
                    "start_idx": {"type": "integer", "description": "èµ·å§‹ç´¢å¼•ï¼ˆå¯é€‰ï¼Œä»0å¼€å§‹ï¼‰"},
                    "end_idx": {"type": "integer", "description": "ç»“æŸç´¢å¼•ï¼ˆå¯é€‰ï¼‰"}
                }
            }
        }
    },
    {
        "type": "function",
        "function": {
            "name": "get_context",
            "description": "è·å–æŸå¥å­çš„ä¸Šä¸‹æ–‡ï¼ˆå‰åå„Nå¥ï¼‰",
            "parameters": {
                "type": "object",
                "properties": {
                    "sentence_idx": {"type": "integer"},
                    "context_count": {"type": "integer", "default": 2}
                },
                "required": ["sentence_idx"]
            }
        }
    },
    {
        "type": "function",
        "function": {
            "name": "batch_replace",
            "description": "æ‰¹é‡æ‰§è¡Œå¤šå¤„æ›¿æ¢ã€‚åœ¨æ‰€æœ‰å¥å­ä¸­æŸ¥æ‰¾å¹¶æ›¿æ¢å¤šç§é”™è¯¯å½¢å¼",
            "parameters": {
                "type": "object",
                "properties": {
                    "replacements": {
                        "type": "array",
                        "items": {
                            "type": "object",
                            "properties": {
                                "old_text": {"type": "string", "description": "è¦æŸ¥æ‰¾çš„é”™è¯¯æ–‡æœ¬"},
                                "new_text": {"type": "string", "description": "æ›¿æ¢åçš„æ­£ç¡®æœ¯è¯­"}
                            },
                            "required": ["old_text", "new_text"]
                        }
                    }
                },
                "required": ["replacements"]
            }
        }
    },
    {
        "type": "function",
        "function": {
            "name": "finish",
            "description": "å®ŒæˆçŸ«æ­£ä»»åŠ¡",
            "parameters": {
                "type": "object",
                "properties": {
                    "summary": {"type": "string", "description": "ä¿®æ”¹æ€»ç»“"}
                },
                "required": ["summary"]
            }
        }
    }
]


# ==================== System Prompt ====================

def build_system_prompt(terms_with_meanings: List[dict]) -> str:
    """æ„å»º System Promptï¼ˆå®Œå…¨å¤ç”¨ agent_correct.pyï¼‰"""
    terms_info = "\n".join([
        f"  - {t['name']}" + (f": {t['meaning']}" if t['meaning'] else "")
        for t in terms_with_meanings
    ])

    return f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ ASRï¼ˆè‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼‰æœ¯è¯­çŸ«æ­£ä¸“å®¶ã€‚

## ä»»åŠ¡
çŸ«æ­£æ–‡æœ¬ä¸­è¢«è¯­éŸ³è¯†åˆ«é”™è¯¯çš„ä¸“ä¸šæœ¯è¯­ã€‚

## æœ¯è¯­åˆ—è¡¨åŠå«ä¹‰
{terms_info}

## å¯ç”¨å·¥å…·
1. read_sentences - è¯»å–å¥å­å†…å®¹
2. get_context - æŸ¥çœ‹æŸå¥çš„ä¸Šä¸‹æ–‡ï¼ˆå‰åå‡ å¥ï¼‰
3. batch_replace - æ‰¹é‡æ›¿æ¢ï¼ˆåœ¨æ‰€æœ‰å¥å­ä¸­æ‰§è¡Œå¤šä¸ªæ›¿æ¢è§„åˆ™ï¼‰
4. finish - å®ŒæˆçŸ«æ­£

## é«˜æ•ˆå·¥ä½œæµç¨‹ï¼ˆé‡è¦ï¼ï¼‰

1. **å…ˆå…¨å±€æ‰«æ**ï¼šç”¨ read_sentences æŸ¥çœ‹å¥å­å†…å®¹
2. **è®°å½•æ‰€æœ‰ç–‘ä¼¼é”™è¯¯**ï¼šåœ¨å¿ƒä¸­/è‰ç¨¿ä¸­è®°å½•æ‰€æœ‰å‘ç°çš„é”™è¯¯ä½ç½®å’Œå½¢å¼
3. **æ‰¹é‡å¤„ç†**ï¼šé˜…è¯»å®Œæˆåï¼Œä½¿ç”¨ batch_replace ä¸€æ¬¡æ€§å®Œæˆæ‰€æœ‰æ›¿æ¢
4. **æœ€åè°ƒç”¨ finish**

### æ‰¹é‡æ“ä½œç¤ºä¾‹

**æ¨èæ–¹å¼ï¼šä½¿ç”¨ batch_replace**
å¦‚æœå‘ç°å¤šç§é”™è¯¯æ¨¡å¼ï¼Œç”¨ä¸€æ¬¡ batch_replace å®Œæˆæ‰€æœ‰æ›¿æ¢ï¼š
```
batch_replace(replacements=[
  {{"old_text": "L L M", "new_text": "LLM"}},
  {{"old_text": "j son", "new_text": "JSON"}},
  {{"old_text": "A P I", "new_text": "API"}}
])
```

### ä½¿ç”¨ batch_replace çš„å…³é”®åŸåˆ™ï¼ˆé‡è¦ï¼ï¼‰

**ä¸€æ¬¡æäº¤æ‰€æœ‰è§„åˆ™ï¼ŒåŒ…æ‹¬å¤æ•°å½¢å¼**ï¼š
- å¦‚æœæœ¯è¯­åœ¨æ–‡æœ¬ä¸­æœ‰å¤æ•°å‡ºç°ï¼Œå¿…é¡»ä¸ºå•æ•°å’Œå¤æ•°å„å†™ä¸€æ¡è§„åˆ™
- ä¸è¦å…ˆæ›¿æ¢å†å›å¤´ä¿®å¤å¤æ•°ï¼Œä¸€æ¬¡åˆ°ä½

ç¤ºä¾‹ï¼šå‡è®¾æœ¯è¯­æ˜¯ "ABC"ï¼Œæ–‡æœ¬ä¸­å‘ç°é”™è¯¯å½¢å¼ "A B C" å’Œ "A B Cs"
```
batch_replace(replacements=[
  {{"old_text": "A B C", "new_text": "ABC"}},
  {{"old_text": "A B Cs", "new_text": "ABCs"}}
])
```

é”™è¯¯åšæ³•ï¼š
- åªæäº¤ {{"old_text": "A B Cs", "new_text": "ABC"}}ï¼ˆä¸¢å¤±å¤æ•°ï¼‰
- å…ˆæäº¤å•æ•°è§„åˆ™ï¼Œåç»­å†è¡¥å……å¤æ•°è§„åˆ™ï¼ˆæµªè´¹è½®æ¬¡ï¼‰

## é‡è¦åŸåˆ™

### å¤æ•°å½¢å¼
- æ ¹æ®ä¸Šä¸‹æ–‡åˆ¤æ–­æ˜¯å¦éœ€è¦å¤æ•°å½¢å¼
- å¦‚æœè¯­å¢ƒæ˜¯å¤æ•°ï¼ˆthese/those/all/multiple/several ç­‰æ ‡è®°ï¼‰ï¼Œæ›¿æ¢æ—¶ä½¿ç”¨å¤æ•°å½¢å¼

ç¤ºä¾‹ï¼š
- "these L L M models" â†’ æ›¿æ¢ä¸º "these LLM models"ï¼ˆå¤æ•°ï¼‰
- "a L L M model" â†’ æ›¿æ¢ä¸º "a LLM model"ï¼ˆå•æ•°ï¼‰

### è°¨æ…ä¿®æ­£
- åªä¿®æ”¹æ˜ç¡®æ˜¯ ASR è¯¯è¯†åˆ«çš„æƒ…å†µ
- å¦‚æœæŸä¸ªè¯åœ¨å½“å‰ä¸Šä¸‹æ–‡ä¸­æ˜¯åˆç†çš„ï¼Œä¸è¦ä¿®æ”¹

ç¤ºä¾‹ï¼š
- "I can see the point" â†’ "see" æ˜¯æ­£ç¡®çš„ï¼Œä¸è¦æ”¹æˆ "C" æˆ– "sea"
- "We use API calls" â†’ "API" æ˜¯æ­£ç¡®çš„ï¼Œä¸éœ€è¦ä¿®æ”¹

## å¸¸è§ ASR é”™è¯¯æ¨¡å¼å‚è€ƒ
- ç©ºæ ¼æ’å…¥ï¼šå­—æ¯é—´è¢«æ’å…¥ç©ºæ ¼ï¼ˆL L M â†’ LLMï¼‰
- å¤§å°å†™é”™è¯¯ï¼šé¦–å­—æ¯æœªå¤§å†™æˆ–å…¨å°å†™ï¼ˆjson â†’ JSONï¼‰
- åŒéŸ³è¿‘éŸ³ï¼šå‘éŸ³ç›¸ä¼¼çš„é”™è¯¯æ›¿æ¢

è¯·å¼€å§‹å·¥ä½œï¼Œè®°ä½ï¼šæ‰¹é‡å¤„ç†ï¼Œä¸è¦é€ä¸ªå¤„ç†ã€‚
"""


def _parse_terms(terms_config: List[str]) -> List[dict]:
    """è§£ææœ¯è¯­åˆ—è¡¨ï¼Œæ”¯æŒ 'æœ¯è¯­ : å«ä¹‰' æ ¼å¼"""
    parsed = []
    for term in terms_config:
        if ' : ' in term:
            parts = term.split(' : ', 1)
        elif ': ' in term:
            parts = term.split(': ', 1)
        elif 'ï¼š' in term:
            parts = term.split('ï¼š', 1)
        else:
            parsed.append({'name': term.strip(), 'meaning': None})
            continue

        parsed.append({
            'name': parts[0].strip(),
            'meaning': parts[1].strip() if len(parts) > 1 else None
        })
    return parsed


class SentenceToolExecutor:
    """Sentence å¯¹è±¡çš„å·¥å…·æ‰§è¡Œå™¨"""

    def __init__(self, sentences: List[Sentence]):
        self.sentences = sentences
        self.changes: List[Dict] = []

    def read_sentences(self, start_idx: int = None, end_idx: int = None) -> str:
        """è¯»å–å¥å­å†…å®¹ï¼Œè¿”å›å…¨éƒ¨æˆ–æŒ‡å®šç´¢å¼•èŒƒå›´"""
        total = len(self.sentences)

        if start_idx is None:
            start_idx = 0
        if end_idx is None:
            end_idx = total

        # é™åˆ¶æœ€å¤šæ˜¾ç¤º 100 å¥
        MAX_DISPLAY = 100
        truncated = False
        actual_end = end_idx

        if actual_end - start_idx > MAX_DISPLAY:
            actual_end = start_idx + MAX_DISPLAY
            truncated = True

        result = []
        for i in range(start_idx, min(actual_end, total)):
            result.append(f"ç¬¬{i}å¥: {self.sentences[i].text}")

        output = "\n".join(result)

        if truncated:
            remaining = total - actual_end
            output += f"\n\n[å…± {total} å¥ï¼Œå·²æ˜¾ç¤ºåˆ°ç¬¬ {actual_end} å¥ï¼Œå‰©ä½™ {remaining} å¥ã€‚"
            output += f"è¯·è°ƒç”¨ read_sentences(start_idx={actual_end}) ç»§ç»­æŸ¥çœ‹]"

        return output

    def get_context(self, sentence_idx: int, context_count: int = 2) -> str:
        """è·å–æŸå¥å­çš„ä¸Šä¸‹æ–‡"""
        start = max(0, sentence_idx - context_count)
        end = min(len(self.sentences), sentence_idx + context_count + 1)

        result = []
        for i in range(start, end):
            marker = ">>> " if i == sentence_idx else "    "
            result.append(f"{marker}ç¬¬{i}å¥: {self.sentences[i].text}")

        return "\n".join(result)

    def batch_replace(self, replacements: list) -> str:
        """æ‰¹é‡æ›¿æ¢æœ¯è¯­ï¼ˆåªä¿®æ”¹æ–‡æœ¬ï¼Œä¸åŠ¨ chunks å’Œæ—¶é—´æˆ³ï¼‰"""
        results = []
        total_changes = 0

        for replacement in replacements:
            old_text = replacement.get("old_text", "")
            new_text = replacement.get("new_text", "")

            if not old_text:
                results.append({"error": "old_text ä¸èƒ½ä¸ºç©º"})
                continue

            # åœ¨æ‰€æœ‰å¥å­ä¸­æŸ¥æ‰¾å¹¶æ›¿æ¢
            for sent_idx, sentence in enumerate(self.sentences):
                changes_count = self._replace_in_sentence(
                    sentence, sent_idx, old_text, new_text
                )
                total_changes += changes_count

            results.append({
                "old_text": old_text,
                "new_text": new_text,
                "count": total_changes
            })

        return json.dumps({
            "success": True,
            "total_changes": total_changes,
            "details": results
        }, ensure_ascii=False)

    def _replace_in_sentence(
        self, sentence: Sentence, sent_idx: int,
        old_text: str, new_text: str
    ) -> int:
        """åœ¨å•ä¸ªå¥å­ä¸­æŸ¥æ‰¾å¹¶æ›¿æ¢ï¼ˆåªä¿®æ”¹æ–‡æœ¬ï¼‰"""
        # æ¸…æ´—æ–‡æœ¬ç”¨äºåŒ¹é…
        sent_clean = clean_word(sentence.text)
        old_clean = clean_word(old_text)

        # æŸ¥æ‰¾æ‰€æœ‰åŒ¹é…ä½ç½®
        matches = list(re.finditer(re.escape(old_clean), sent_clean))
        if not matches:
            return 0

        # åœ¨åŸå§‹æ–‡æœ¬ä¸­è¿›è¡Œæ›¿æ¢ï¼ˆä»åå¾€å‰é¿å…ä½ç½®åç§»ï¼‰
        changes_count = 0
        for match in reversed(matches):
            # åœ¨åŸå§‹æ–‡æœ¬ä¸­æ‰¾åˆ°å¯¹åº”ä½ç½®
            original_start, original_end = self._find_original_position(
                sentence.text, sent_clean, match.start(), match.end()
            )

            if original_start is not None:
                # æ‰§è¡Œæ›¿æ¢
                sentence.text = (
                    sentence.text[:original_start] +
                    new_text +
                    sentence.text[original_end:]
                )
                changes_count += 1

                # è®°å½•ä¿®æ”¹
                self.changes.append({
                    "sentence_idx": sent_idx,
                    "old_text": old_text,
                    "new_text": new_text
                })

        return changes_count

    def _find_original_position(
        self, original_text: str, cleaned_text: str,
        clean_start: int, clean_end: int
    ) -> Tuple[Optional[int], Optional[int]]:
        """
        åœ¨åŸå§‹æ–‡æœ¬ä¸­æ‰¾åˆ°æ¸…æ´—åæ–‡æœ¬å¯¹åº”çš„ä½ç½®

        ä½¿ç”¨æ»‘åŠ¨çª—å£åŒ¹é…ï¼Œæ‰¾åˆ°æœ€å¯èƒ½çš„ä½ç½®
        """
        # æ¸…æ´—å‰åçš„æ–‡æœ¬é•¿åº¦å¯èƒ½ä¸åŒ
        # ä½¿ç”¨æ»‘åŠ¨çª—å£åœ¨åŸå§‹æ–‡æœ¬ä¸­æŸ¥æ‰¾åŒ¹é…
        window_size = clean_end - clean_start

        # æå–æ¸…æ´—åçª—å£å†…å®¹
        cleaned_window = cleaned_text[clean_start:clean_end]

        # åœ¨åŸå§‹æ–‡æœ¬ä¸­æœç´¢
        best_match = None
        best_score = 0

        for i in range(len(original_text) - window_size + 1):
            window = original_text[i:i + window_size]
            # æ¸…æ´—çª—å£å†…å®¹è¿›è¡Œå¯¹æ¯”
            from core.utils.sentence_tools import clean_word
            if clean_word(window) == cleaned_window:
                # ç²¾ç¡®åŒ¹é…
                return i, i + window_size

        # å¦‚æœæ²¡æœ‰ç²¾ç¡®åŒ¹é…ï¼Œè¿”å› None
        return None, None

    def finish(self, summary: str) -> str:
        """å®ŒæˆçŸ«æ­£ä»»åŠ¡"""
        return json.dumps({
            "changes_count": len(self.changes),
            "summary": summary,
            "is_finish": True
        }, ensure_ascii=False)


# ==================== ä¸»å…¥å£å‡½æ•° ====================

def _sync_chunks_to_text(sentences: List[Sentence]) -> None:
    """
    åŒæ­¥ chunks åˆ°çŸ«æ­£åçš„æ–‡æœ¬

    å°†çŸ«æ­£åçš„ sentence.text ä½œä¸ºä¸€ä¸ªæ•´ä½“ chunkï¼Œ
    æ—¶é—´æˆ³ä½¿ç”¨åŸå¥å­çš„ start/endã€‚

    Args:
        sentences: Sentence å¯¹è±¡åˆ—è¡¨ï¼ˆä¼šå°±åœ°ä¿®æ”¹ï¼‰
    """
    for sentence in sentences:
        # å¦‚æœ sentence.text è¢«ä¿®æ”¹äº†ï¼Œé‡å»º chunks
        # ç®€åŒ–ç­–ç•¥ï¼šå°†æ•´ä¸ª text ä½œä¸ºä¸€ä¸ª chunk
        new_chunk = Chunk(
            text=sentence.text,
            start=sentence.start,
            end=sentence.end,
            speaker_id=sentence.chunks[0].speaker_id if sentence.chunks else None,
            index=0
        )
        sentence.chunks = [new_chunk]

        # ç¡®ä¿ Sentence.start/end ä¸ chunks ä¸€è‡´
        if sentence.chunks:
            sentence.start = sentence.chunks[0].start
            sentence.end = sentence.chunks[-1].end


@timer("ASR æœ¯è¯­çŸ«æ­£")
def correct_terms_in_sentences(sentences: List[Sentence]) -> List[Sentence]:
    """
    ä¸»å‡½æ•°ï¼šå¯¹å¥å­åˆ—è¡¨è¿›è¡Œæœ¯è¯­çŸ«æ­£

    Args:
        sentences: NLP åˆ†å¥åçš„ Sentence å¯¹è±¡åˆ—è¡¨

    Returns:
        List[Sentence]: çŸ«æ­£åçš„ Sentence å¯¹è±¡åˆ—è¡¨
    """
    # 1. æ£€æŸ¥å¼€å…³
    enabled = load_key("asr_term_correction.enabled", default=False)
    if not enabled:
        rprint("[yellow]â­ï¸ æœ¯è¯­çŸ«æ­£å·²ç¦ç”¨ï¼Œè·³è¿‡[/yellow]")
        return sentences

    # 2. åŠ è½½æœ¯è¯­é…ç½®
    terms_config = load_key("asr_term_correction.terms", default=[])
    if not terms_config:
        rprint("[yellow]â­ï¸ æœªé…ç½®æœ¯è¯­ï¼Œè·³è¿‡çŸ«æ­£[/yellow]")
        return sentences

    # 3. è§£ææœ¯è¯­åˆ—è¡¨
    terms_with_meanings = _parse_terms(terms_config)
    rprint(f"[blue]ğŸ” å¼€å§‹æœ¯è¯­çŸ«æ­£ï¼Œå…± {len(terms_with_meanings)} ä¸ªæœ¯è¯­[/blue]")
    for t in terms_with_meanings:
        rprint(f"  - {t['name']}" + (f": {t['meaning']}" if t['meaning'] else ""))

    # 4. åˆ›å»ºå·¥å…·æ‰§è¡Œå™¨
    tool_executor = SentenceToolExecutor(sentences)

    # 5. æ„å»ºç³»ç»Ÿæç¤ºè¯
    system_prompt = build_system_prompt(terms_with_meanings)

    # 6. è°ƒç”¨ LLM Agent
    user_task = f"è¯·çŸ«æ­£è¿™ {len(sentences)} ä¸ªå¥å­ä¸­çš„æœ¯è¯­é”™è¯¯"

    result = ask_gpt_with_tools(
        system_prompt=system_prompt,
        prompt=user_task,
        tools=SENTENCES_TOOLS_DEFINITION,
        tool_executor=tool_executor,
        max_rounds=20,
        log_title="asr_term_correction"
    )

    # 7. è¾“å‡ºç»Ÿè®¡
    if result and result.get("is_finish"):
        changes_count = result.get("changes_count", 0)
        if changes_count > 0:
            rprint(f"[green]âœ… çŸ«æ­£å®Œæˆ: {changes_count} å¤„ä¿®æ”¹[/green]")

            # æ˜¾ç¤ºä¿®æ”¹æ˜ç»†
            from collections import Counter
            stats = Counter(f"{c['old_text']} â†’ {c['new_text']}" for c in tool_executor.changes)
            for change, count in stats.most_common():
                rprint(f"  {change}: {count} å¤„")
        else:
            rprint("[green]âœ… æœªå‘ç°éœ€è¦çŸ«æ­£çš„é”™è¯¯[/green]")

        rprint(f"[dim]LLM æ€»ç»“: {result.get('summary', 'N/A')}[/dim]")
    else:
        rprint("[yellow]âš ï¸ LLM æœªæ­£å¸¸å®Œæˆï¼Œè¿”å›åŸå§‹å¥å­[/yellow]")

    # 8. åŒæ­¥ chunks åˆ°çŸ«æ­£åçš„æ–‡æœ¬
    _sync_chunks_to_text(sentences)

    return sentences
