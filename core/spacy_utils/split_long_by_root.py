import os
import string
import warnings
from typing import List, Tuple
from core.utils import rprint, load_key, get_joiner
from core.spacy_utils.load_nlp_model import init_nlp
from core.utils.models import _3_1_SPLIT_BY_NLP, Sentence
from core.utils.sentence_tools import map_char_positions_to_chunks

warnings.filterwarnings("ignore", category=FutureWarning)

# ------------
# Helper Functions
# ------------

def split_long_sentence(doc):
    """
    ä½¿ç”¨åŠ¨æ€è§„åˆ’åˆ‡åˆ†é•¿å¥

    Returns:
        List[int]: åˆ‡åˆ†ç‚¹çš„ token ç´¢å¼•åˆ—è¡¨ï¼ˆå‡åºï¼‰
    """
    tokens = [token.text for token in doc]
    n = len(tokens)

    dp = [float('inf')] * (n + 1)
    dp[0] = 0
    prev = [0] * (n + 1)

    for i in range(1, n + 1):
        for j in range(max(0, i - 100), i):
            if i - j >= 30:
                token = doc[i-1]
                if j == 0 or (token.is_sent_end or token.pos_ in ['VERB', 'AUX'] or token.dep_ == 'ROOT'):
                    if dp[j] + 1 < dp[i]:
                        dp[i] = dp[j] + 1
                        prev[i] = j

    # å›æº¯è·å–åˆ‡åˆ†ç‚¹
    split_points = []
    i = n
    while i > 0:
        j = prev[i]
        if j > 0:
            split_points.append(j)
        i = j

    return split_points[::-1]  # è¿”å›å‡åº

def split_extremely_long_sentence(doc):
    tokens = [token.text for token in doc]
    n = len(tokens)

    num_parts = (n + 59) // 60
    part_length = n // num_parts

    sentences = []
    language = load_key("asr.language")
    joiner = get_joiner(language)
    for i in range(num_parts):
        start = i * part_length
        end = start + part_length if i < num_parts - 1 else n
        sentence = joiner.join(tokens[start:end])
        sentences.append(sentence)

    return sentences


# ------------
# Original file-based function (deprecated)
# ------------

def split_long_by_root_main(nlp):
    """æ—§çš„æ–‡ä»¶æµç‰ˆæœ¬ï¼ˆå·²å¼ƒç”¨ï¼‰"""
    from core.spacy_utils.load_nlp_model import SPLIT_BY_CONNECTOR_FILE

    with open(SPLIT_BY_CONNECTOR_FILE, "r", encoding="utf-8") as input_file:
        sentences = input_file.readlines()

    all_split_sentences = []
    for sentence in sentences:
        doc = nlp(sentence.strip())
        if len(doc) > 60:
            split_sentences = split_long_sentence(doc)
            if any(len(nlp(sent)) > 60 for sent in split_sentences):
                split_sentences = [subsent for sent in split_sentences for subsent in split_extremely_long_sentence(nlp(sent))]
            all_split_sentences.extend(split_sentences)
            rprint(f"[yellow]âœ‚ï¸  Splitting long sentences by root: {sentence[:30]}...[/yellow]")
        else:
            all_split_sentences.append(sentence.strip())

    punctuation = string.punctuation + "'" + '"'

    with open(_3_1_SPLIT_BY_NLP, "w", encoding="utf-8") as output_file:
        for i, sentence in enumerate(all_split_sentences):
            stripped_sentence = sentence.strip()
            if not stripped_sentence or all(char in punctuation for char in stripped_sentence):
                rprint(f"[yellow]âš ï¸  Warning: Empty or punctuation-only line detected at index {i}[/yellow]")
                continue
            output_file.write(sentence + "\n")

    rprint(f"[green]ğŸ’¾ Long sentences split by root saved to â†’  {_3_1_SPLIT_BY_NLP}[/green]")


# ------------
# New Object-based Function
# ------------

def split_by_root(sentences: List[Sentence], nlp) -> List[Sentence]:
    """
    æŒ‰è¯æ ¹åˆ‡åˆ†é•¿å¥ï¼ˆå¯¹è±¡åŒ–ç‰ˆæœ¬ï¼‰

    ä½¿ç”¨ä¸ _3_1_split_nlp.py ç›¸åŒçš„å­—ç¬¦ä½ç½®æ˜ å°„é€»è¾‘ã€‚

    Args:
        sentences: Sentence å¯¹è±¡åˆ—è¡¨
        nlp: spaCy NLP æ¨¡å‹

    Returns:
        List[Sentence]: åˆ†å¥åçš„ Sentence å¯¹è±¡åˆ—è¡¨
    """
    asr_language = load_key("asr.language")
    joiner = get_joiner(asr_language)
    result = []

    for sentence in sentences:
        if len(sentence.chunks) <= 1:
            result.append(sentence)
            continue

        doc = nlp(sentence.text)

        # åªå¤„ç†è¶…è¿‡ 60 ä¸ª token çš„å¥å­
        if len(doc) <= 60:
            result.append(sentence)
            continue

        # ä½¿ç”¨ DP ç®—æ³•æ‰¾åˆ°åˆ‡åˆ†ç‚¹ï¼ˆtoken ç´¢å¼•ï¼‰
        token_split_indices = split_long_sentence(doc)

        if not token_split_indices:
            result.append(sentence)
            continue

        # å°† token ç´¢å¼•è½¬æ¢ä¸ºå­—ç¬¦ä½ç½®
        char_positions = []
        for token_idx in token_split_indices:
            if token_idx < len(doc):
                char_positions.append(doc[token_idx].idx)

        # ä½¿ç”¨å…¬å…±å‡½æ•°æ˜ å°„åˆ° Chunk ç´¢å¼•
        chunk_split_indices = map_char_positions_to_chunks(sentence, char_positions)

        # æŒ‰åˆ‡åˆ†ç‚¹åˆ†å‰² chunks
        split_points = [0] + chunk_split_indices + [len(sentence.chunks)]
        split_points = sorted(set(split_points))

        for i in range(len(split_points) - 1):
            start_idx = split_points[i]
            end_idx = split_points[i + 1]

            if start_idx >= end_idx:
                continue

            sub_chunks = sentence.chunks[start_idx:end_idx]
            if sub_chunks:
                new_sentence = Sentence(
                    chunks=sub_chunks,
                    text=joiner.join(c.text for c in sub_chunks),
                    start=sub_chunks[0].start,
                    end=sub_chunks[-1].end,
                    is_split=True
                )
                result.append(new_sentence)

        rprint(f"[yellow]âœ‚ï¸  Splitting long sentence by root: {sentence.text[:30]}...[/yellow]")

    return result


if __name__ == "__main__":
    nlp = init_nlp()
    split_long_by_root_main(nlp)
