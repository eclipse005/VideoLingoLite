import os
import warnings
from typing import List
from core.utils import rprint, load_key, get_joiner
from core.spacy_utils.load_nlp_model import init_nlp, SPLIT_BY_CONNECTOR_FILE
from core.utils.models import Sentence
from core.utils.sentence_tools import map_char_positions_to_chunks, should_split_by_origin_length

warnings.filterwarnings("ignore", category=FutureWarning)

def analyze_connectors(doc, token):
    """
    Analyze whether a token is a connector that should trigger a sentence split.
    """
    lang = load_key("asr.language")
    if lang == "en":
        connectors = ["that", "which", "where", "when", "because", "but", "and", "or"]
        mark_dep = "mark"
        det_pron_deps = ["det", "pron"]
        verb_pos = "VERB"
        noun_pos = ["NOUN", "PROPN"]
    elif lang == "zh":
        connectors = ["å› ä¸º", "æ‰€ä»¥", "ä½†æ˜¯", "è€Œä¸”", "è™½ç„¶", "å¦‚æœ", "å³ä½¿", "å°½ç®¡"]
        mark_dep = "mark"
        det_pron_deps = ["det", "pron"]
        verb_pos = "VERB"
        noun_pos = ["NOUN", "PROPN"]
    elif lang == "ja":
        connectors = ["ã‘ã‚Œã©ã‚‚", "ã—ã‹ã—", "ã ã‹ã‚‰", "ãã‚Œã§", "ã®ã§", "ã®ã«", "ãŸã‚"]
        mark_dep = "mark"
        det_pron_deps = ["case"]
        verb_pos = "VERB"
        noun_pos = ["NOUN", "PROPN"]
    elif lang == "ko":
        connectors = ["í•˜ì§€ë§Œ", "ê·¸ëŸ¬ë‚˜", "ê·¸ë¦¬ê³ ", "ê·¸ë˜ì„œ", "ì™œëƒí•˜ë©´", "ê·¸ëŸ°ë°", "ë˜ëŠ”", "ì¦‰", "ê²Œë‹¤ê°€"]
        mark_dep = "mark"
        det_pron_deps = ["det"]
        verb_pos = "VERB"
        noun_pos = ["NOUN", "PROPN"]
    elif lang == "fr":
        connectors = ["que", "qui", "oÃ¹", "quand", "parce que", "mais", "et", "ou"]
        mark_dep = "mark"
        det_pron_deps = ["det", "pron"]
        verb_pos = "VERB"
        noun_pos = ["NOUN", "PROPN"]
    elif lang == "ru":
        connectors = ["Ñ‡Ñ‚Ğ¾", "ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹", "Ğ³Ğ´Ğµ", "ĞºĞ¾Ğ³Ğ´Ğ°", "Ğ¿Ğ¾Ñ‚Ğ¾Ğ¼Ñƒ Ñ‡Ñ‚Ğ¾", "Ğ½Ğ¾", "Ğ¸", "Ğ¸Ğ»Ğ¸"]
        mark_dep = "mark"
        det_pron_deps = ["det"]
        verb_pos = "VERB"
        noun_pos = ["NOUN", "PROPN"]
    elif lang == "es":
        connectors = ["que", "cual", "donde", "cuando", "porque", "pero", "y", "o"]
        mark_dep = "mark"
        det_pron_deps = ["det", "pron"]
        verb_pos = "VERB"
        noun_pos = ["NOUN", "PROPN"]
    elif lang == "de":
        connectors = ["dass", "welche", "wo", "wann", "weil", "aber", "und", "oder"]
        mark_dep = "mark"
        det_pron_deps = ["det", "pron"]
        verb_pos = "VERB"
        noun_pos = ["NOUN", "PROPN"]
    elif lang == "it":
        connectors = ["che", "quale", "dove", "quando", "perchÃ©", "ma", "e", "o"]
        mark_dep = "mark"
        det_pron_deps = ["det", "pron"]
        verb_pos = "VERB"
        noun_pos = ["NOUN", "PROPN"]
    else:
        return False, False

    if token.text.lower() not in connectors:
        return False, False

    if lang == "en" and token.text.lower() == "that":
        if token.dep_ == mark_dep and token.head.pos_ == verb_pos:
            return True, False
        else:
            return False, False
    elif token.dep_ in det_pron_deps and token.head.pos_ in noun_pos:
        return False, False
    else:
        return True, False

def split_by_connectors(text, context_words=5, nlp=None):
    doc = nlp(text)
    sentences = [doc.text]

    lang = load_key("asr.language")

    while True:
        split_occurred = False
        new_sentences = []

        for sent in sentences:
            doc = nlp(sent)
            start = 0

            for i, token in enumerate(doc):
                split_before, _ = analyze_connectors(doc, token)

                if lang == "en" and i + 1 < len(doc) and doc[i + 1].text in ["'s", "'re", "'ve", "'ll", "'d"]:
                    continue

                left_words = doc[max(0, token.i - context_words):token.i]
                right_words = doc[token.i+1:min(len(doc), token.i + context_words + 1)]

                left_words = [word.text for word in left_words if not word.is_punct]
                right_words = [word.text for word in right_words if not word.is_punct]

                if len(left_words) >= context_words and len(right_words) >= context_words and split_before:
                    rprint(f"[yellow]âœ‚ï¸  Split before '{token.text}': {' '.join(left_words)}| {token.text} {' '.join(right_words)}[/yellow]")
                    new_sentences.append(doc[start:token.i].text.strip())
                    start = token.i
                    split_occurred = True
                    break

            if start < len(doc):
                new_sentences.append(doc[start:].text.strip())

        if not split_occurred:
            break

        sentences = new_sentences

    return sentences


# ------------
# Original file-based function (deprecated)
# ------------

def split_sentences_main(nlp):
    """æ—§çš„æ–‡ä»¶æµç‰ˆæœ¬ï¼ˆå·²å¼ƒç”¨ï¼‰"""
    from core.spacy_utils.load_nlp_model import SPLIT_BY_COMMA_FILE

    with open(SPLIT_BY_COMMA_FILE, "r", encoding="utf-8") as input_file:
        sentences = input_file.readlines()

    all_split_sentences = []
    for sentence in sentences:
        split_sentences = split_by_connectors(sentence.strip(), nlp=nlp)
        all_split_sentences.extend(split_sentences)

    with open(SPLIT_BY_CONNECTOR_FILE, "w+", encoding="utf-8") as output_file:
        for sentence in all_split_sentences:
            output_file.write(sentence + "\n")
        output_file.seek(output_file.tell() - 1, os.SEEK_SET)
        output_file.truncate()

    rprint(f"[green]ğŸ’¾ Sentences split by connectors saved to â†’  `{SPLIT_BY_CONNECTOR_FILE}`[/green]")


# ------------
# New Object-based Function
# ------------

def split_by_connector(sentences: List[Sentence], nlp) -> List[Sentence]:
    """
    æŒ‰è¿æ¥è¯åˆ†å¥ï¼ˆå¯¹è±¡åŒ–ç‰ˆæœ¬ï¼‰

    ä½¿ç”¨ä¸ _3_1_split_nlp.py ç›¸åŒçš„å­—ç¬¦ä½ç½®æ˜ å°„é€»è¾‘ã€‚

    Args:
        sentences: Sentence å¯¹è±¡åˆ—è¡¨
        nlp: spaCy NLP æ¨¡å‹

    Returns:
        List[Sentence]: åˆ†å¥åçš„ Sentence å¯¹è±¡åˆ—è¡¨
    """
    lang = load_key("asr.language")
    context_words = 5
    result = []

    for sentence in sentences:
        # å¯¹æ¯ä¸ªå¥å­è¿›è¡Œå¯èƒ½çš„å¤šæ¬¡åˆ‡åˆ†
        current_sentences = [sentence]

        while True:
            split_occurred = False
            new_sentences = []

            for sent in current_sentences:
                if len(sent.chunks) <= 1:
                    new_sentences.append(sent)
                    continue

                if not should_split_by_origin_length(sent.text):
                    new_sentences.append(sent)
                    continue

                doc = nlp(sent.text)
                split_char_pos = None

                for i, token in enumerate(doc):
                    # æ£€æŸ¥æ˜¯å¦ä¸ºè¿æ¥è¯
                    is_connector, _ = analyze_connectors(doc, token)

                    # è·³è¿‡è‹±è¯­ç¼©å†™åçš„è¿æ¥è¯
                    if lang == "en" and i + 1 < len(doc) and doc[i + 1].text in ["'s", "'re", "'ve", "'ll", "'d"]:
                        continue

                    left_words = doc[max(0, token.i - context_words):token.i]
                    right_words = doc[token.i+1:min(len(doc), token.i + context_words + 1)]

                    left_words_list = [word.text for word in left_words if not word.is_punct]
                    right_words_list = [word.text for word in right_words if not word.is_punct]

                    if (len(left_words_list) >= context_words and
                        len(right_words_list) >= context_words and
                        is_connector):

                        # ä½¿ç”¨ token.idxï¼ˆåŸå§‹æ–‡æœ¬ä¸­çš„å­—ç¬¦ä½ç½®ï¼‰
                        split_char_pos = token.idx
                        rprint(f"[yellow]âœ‚ï¸  Split before '{token.text}': {' '.join(left_words_list[-4:])}| {token.text} {' '.join(right_words_list[:4])}[/yellow]")
                        split_occurred = True
                        break

                if split_char_pos is not None:
                    # ä½¿ç”¨å…¬å…±å‡½æ•°æ˜ å°„åˆ° Chunk ç´¢å¼•
                    chunk_split_indices = map_char_positions_to_chunks(sent, [split_char_pos])

                    if chunk_split_indices and 0 < chunk_split_indices[0] < len(sent.chunks):
                        split_idx = chunk_split_indices[0]
                        left_chunks = sent.chunks[:split_idx]
                        right_chunks = sent.chunks[split_idx:]

                        if left_chunks and right_chunks:
                            joiner = get_joiner(lang)
                            left_sentence = Sentence(
                                chunks=left_chunks,
                                text=joiner.join(c.text for c in left_chunks),
                                start=left_chunks[0].start,
                                end=left_chunks[-1].end,
                                is_split=True
                            )
                            right_sentence = Sentence(
                                chunks=right_chunks,
                                text=joiner.join(c.text for c in right_chunks),
                                start=right_chunks[0].start,
                                end=right_chunks[-1].end,
                                is_split=True
                            )
                            new_sentences.extend([left_sentence, right_sentence])
                        else:
                            new_sentences.append(sent)
                    else:
                        new_sentences.append(sent)
                else:
                    new_sentences.append(sent)

            if not split_occurred:
                break

            current_sentences = new_sentences

        result.extend(current_sentences)

    return result


if __name__ == "__main__":
    nlp = init_nlp()
    split_sentences_main(nlp)
