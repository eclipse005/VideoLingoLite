import os
import pandas as pd
from core.utils.config_utils import load_key, get_joiner
from rich import print as rprint

def split_by_pause():
    # è¾“å…¥è¾“å‡ºè·¯å¾„
    input_nlp_path = r"output\log\split_by_nlp.txt"
    chunks_path = r"output\log\cleaned_chunks.xlsx"

    # Get pause threshold from config (0 or null means disabled)
    pause_threshold = load_key("pause_split_threshold")
    if pause_threshold is None or pause_threshold == 0:
        rprint("[yellow]â­ï¸  Pause-based splitting is disabled (pause_split_threshold=0 or null)[/yellow]")
        return

    # Get language and joiner from config
    language = load_key("asr.language")
    joiner = get_joiner(language)
    rprint(f"[blue]ğŸ” Using {language} language joiner: '{joiner}'[/blue]")
    rprint(f"[blue]ğŸ” Pause threshold: {pause_threshold}s[/blue]")

    print(f"ğŸš€ å¼€å§‹æ‰§è¡Œç‰©ç†åœé¡¿åˆ‡åˆ†...")
    
    if not os.path.exists(input_nlp_path) or not os.path.exists(chunks_path):
        print(f"âŒ é”™è¯¯ï¼šæ‰¾ä¸åˆ°æ–‡ä»¶ã€‚è¯·ç¡®ä¿ä»¥ä¸‹è·¯å¾„å­˜åœ¨ï¼š\n- {input_nlp_path}\n- {chunks_path}")
        return

    # 1. è¯»å– NLP ç»“æœ
    with open(input_nlp_path, 'r', encoding='utf-8') as f:
        # é‡ç‚¹ï¼šstrip() å»æ‰æ¢è¡Œï¼Œç„¶å strip('"') å»æ‰ä½ æåˆ°çš„æ¯ä¸€è¡Œå‰åçš„å¼•å·
        raw_lines = [line.strip().strip('"') for line in f.readlines() if line.strip()]
    
    # 2. è¯»å–æ—¶é—´æˆ³ Excel
    chunks_df = pd.read_excel(chunks_path)
    
    print(f"ğŸ“Š æˆåŠŸåŠ è½½ {len(raw_lines)} è¡Œæ–‡æœ¬å’Œ {len(chunks_df)} ä¸ªè¯å—æ—¶é—´æˆ³ã€‚")

    final_sentences = []
    chunk_idx = 0
    total_chunks = len(chunks_df)

    # 3. åŒ¹é…ä¸æ£€æµ‹é€»è¾‘
    for sentence in raw_lines:
        # ä¸ºäº†æ¯”å¯¹ï¼Œå»æ‰å¥å­ä¸­æ‰€æœ‰ç©ºæ ¼å¹¶è½¬å°å†™
        target_text = "".join(sentence.split()).lower()
        
        current_sentence_chunks = []
        collected_text = ""
        
        # åœ¨ chunks åˆ—è¡¨ä¸­æ»‘åŠ¨ï¼Œæ‰¾åˆ°å±äºè¿™ä¸€è¡Œæ–‡æœ¬çš„æ‰€æœ‰è¯å—
        while chunk_idx < total_chunks and len(collected_text) < len(target_text):
            # åŒæ ·å»æ‰ chunk æ–‡æœ¬ä¸­çš„å¼•å·è¿›è¡Œæ¯”å¯¹
            chunk_raw = str(chunks_df.iloc[chunk_idx]['text']).strip('"')
            clean_chunk = "".join(chunk_raw.split()).lower()
            
            collected_text += clean_chunk
            current_sentence_chunks.append(chunks_df.iloc[chunk_idx])
            chunk_idx += 1
            
        # å¦‚æœåªå¯¹åº” 1 ä¸ªè¯å—ï¼Œæ— æ³•è®¡ç®—é—´éš”ï¼Œç›´æ¥ä¿ç•™
        if len(current_sentence_chunks) < 2:
            final_sentences.append(sentence)
            continue

        # 4. æ ¸å¿ƒï¼šæ£€æŸ¥ç‰©ç†é—´éš™ (Pause)
        temp_group = []
        last_chunk = current_sentence_chunks[0]
        temp_group.append(str(last_chunk['text']).strip('"'))

        for i in range(1, len(current_sentence_chunks)):
            current_chunk = current_sentence_chunks[i]
            
            # è®¡ç®—é—´éš™ï¼šå½“å‰è¯ Start - ä¸Šä¸ªè¯ End
            gap = current_chunk['start'] - last_chunk['end']
            
            if gap > pause_threshold:
                # é—´éš™è¿‡å¤§ï¼Œå¼ºåˆ¶æ–­å¼€æˆæ–°è¡Œ
                final_sentences.append(joiner.join(temp_group))
                print(f"âœ‚ï¸  æ£€æµ‹åˆ°åœé¡¿ {gap:.2f}sï¼Œå·²åœ¨è¯å— '{last_chunk['text']}' åæ–­å¥")
                temp_group = [str(current_chunk['text']).strip('"')]
            else:
                temp_group.append(str(current_chunk['text']).strip('"'))
            
            last_chunk = current_chunk
            
        if temp_group:
            final_sentences.append(joiner.join(temp_group))

    # 5. å†™å›åŸæ–‡ä»¶
    with open(input_nlp_path, 'w', encoding='utf-8') as f:
        f.write("\n".join(final_sentences))

    print(f"\nâœ¨ å¤„ç†å®Œæˆï¼")
    print(f"ğŸ“ åŸå§‹è¡Œæ•°: {len(raw_lines)} -> åˆ‡åˆ†åè¡Œæ•°: {len(final_sentences)}")
    print(f"ğŸ’¾ å·²æ›´æ–°æ–‡ä»¶ï¼š{input_nlp_path}")

if __name__ == '__main__':
    split_by_pause()