import os
import pandas as pd
import difflib
import unicodedata
from core.utils.config_utils import load_key, get_joiner
from core.spacy_utils.load_nlp_model import SPLIT_BY_NLP_FILE, SPLIT_BY_PAUSE_FILE
from rich import print as rprint
from core.utils.models import _2_CLEANED_CHUNKS


def is_latin_text(text):
    """æ£€æµ‹æ–‡æœ¬æ˜¯å¦æ˜¯æ‹‰ä¸å­—æ¯æˆ–æ•°å­—ï¼ˆåŒ…æ‹¬æ‹‰ä¸å­—ç¬¦é›†ï¼‰"""
    if not text:
        return False

    for char in text:
        code = ord(char)
        # Basic Latin (0x0020-0x007F): ç©ºæ ¼, a-z, A-Z, 0-9, å¸¸è§æ ‡ç‚¹
        # Latin-1 Supplement (0x0080-0x00FF): Ã¨, Ã©, Ãª, Ã«, Ã , Ã¢, Ã¤, ç­‰
        # Latin Extended-A (0x0100-0x017F): Ä€, Ä, Ä‚, Äƒ, Ä„, Ä…, ç­‰
        # Latin Extended-B (0x0180-0x024F): Æ€, Éƒ, É‚, ç­‰
        if not (0x0020 <= code <= 0x024F):
            return False
    return True


def smart_join(chunks):
    """æ™ºèƒ½è¿æ¥ chunksï¼šå¦‚æœç›¸é‚»ä¸¤ä¸ªéƒ½æ˜¯æ‹‰ä¸å­—æ¯ï¼Œç”¨ç©ºæ ¼è¿æ¥"""
    if not chunks:
        return ""

    result = str(chunks[0]).strip('"')
    for i in range(1, len(chunks)):
        prev = result
        curr = str(chunks[i]).strip('"')

        # æ£€æµ‹å‰ä¸€ä¸ªæ–‡æœ¬çš„ç»“å°¾å’Œå½“å‰æ–‡æœ¬çš„å¼€å¤´æ˜¯å¦éƒ½æ˜¯æ‹‰ä¸å­—æ¯
        if prev and curr:
            prev_last = prev[-1] if prev else ""
            curr_first = curr[0] if curr else ""

            if is_latin_text(prev_last) and is_latin_text(curr_first):
                result += " " + curr
            else:
                result += curr
        else:
            result += curr

    return result


def split_by_pause():
    # è¾“å…¥è¾“å‡ºè·¯å¾„ - ä½¿ç”¨ç»Ÿä¸€çš„å¸¸é‡
    input_nlp_path = SPLIT_BY_NLP_FILE
    chunks_path = _2_CLEANED_CHUNKS
    # ç›´æ¥è¦†ç›– split_by_nlp.txtï¼Œè¿™æ ·åç»­çš„ split_by_meaning.py èƒ½è¯»å–åˆ°ä¿®å¤åçš„å†…å®¹
    output_pause_path = SPLIT_BY_NLP_FILE

    # Get pause threshold from config (0 or null means disabled)
    pause_threshold = load_key("pause_split_threshold")
    if pause_threshold is None or pause_threshold == 0:
        rprint("[yellow]â­ï¸  Pause-based splitting is disabled (pause_split_threshold=0 or null)[/yellow]")
        return

    # Get language and joiner from config
    language = load_key("asr.language")
    joiner = get_joiner(language)
    rprint(f"[blue]ğŸ” Using {language} language joiner: '{joiner}'[/blue]")
    rprint(f"[blue]ğŸ” Pause threshold: {pause_threshold}s[/blue]")

    print(f"ğŸš€ å¼€å§‹æ‰§è¡Œç‰©ç†åœé¡¿åˆ‡åˆ†...")

    if not os.path.exists(input_nlp_path) or not os.path.exists(chunks_path):
        print(f"âŒ é”™è¯¯ï¼šæ‰¾ä¸åˆ°æ–‡ä»¶ã€‚è¯·ç¡®ä¿ä»¥ä¸‹è·¯å¾„å­˜åœ¨ï¼š\n- {input_nlp_path}\n- {chunks_path}")
        return

    # ========== åŠ è½½æ•°æ®æ–‡ä»¶ ==========
    chunks_df = pd.read_excel(chunks_path)
    with open(input_nlp_path, 'r', encoding='utf-8') as f:
        raw_lines = [line.strip() for line in f if line.strip()]

    # ========== è¾¹ç•Œä¿®å¤ï¼šé€å­—ç¬¦å¯¹æ¯”æ³• ==========
    # æ—¢ç„¶ nlp æ‹¼æ¥ = chunks æ‹¼æ¥ï¼Œå°±ä¸¤è¾¹åŒæ—¶éå†ï¼Œå‘ç°ä¸ä¸€è‡´å°±ä¿®å¤
    print("ğŸ”§ å¼€å§‹ä¿®å¤ nlp å¥å­è¾¹ç•Œ...")

    # é¢„å¤„ç†ï¼šå»ºç«‹ (å­—ç¬¦, chunkç´¢å¼•) çš„æ˜ å°„
    chunk_char_map = []  # [(char, chunk_idx), ...]
    for idx, row in chunks_df.iterrows():
        chunk_text = str(row['text']).strip('"')
        if joiner == " ":
            cleaned = "".join(chunk_text.split()).lower()
        else:
            cleaned = chunk_text.lower()
        for char in cleaned:
            chunk_char_map.append((char, idx))

    # æå–çº¯å­—ç¬¦åˆ—è¡¨
    chunk_chars = [char for char, _ in chunk_char_map]

    # é€å­—ç¬¦å¯¹æ¯”ï¼Œå‘ç°ä¸ä¸€è‡´å°±ä¿®å¤
    fixed_lines = list(raw_lines)  # å¤åˆ¶ä¸€ä»½
    i = 0  # nlp å¥å­ç´¢å¼•
    j = 0  # chunk å­—ç¬¦ç´¢å¼•
    changes = 0

    while i < len(fixed_lines) and j < len(chunk_chars):
        line = fixed_lines[i]
        if joiner == " ":
            line_chars = list("".join(line.split()).lower())
        else:
            line_chars = list(line.lower())

        # é€å­—ç¬¦å¯¹æ¯”
        k = 0
        while k < len(line_chars) and j < len(chunk_chars):
            if line_chars[k] == chunk_chars[j]:
                k += 1
                j += 1
            else:
                # ä¸åŒ¹é…ï¼è¿™ä¸åº”è¯¥å‘ç”Ÿï¼ˆå› ä¸º nlp æ‹¼æ¥ = chunks æ‹¼æ¥ï¼‰
                # è·³è¿‡ä¸åŒ¹é…çš„å­—ç¬¦
                print(f"  è­¦å‘Š: line[{i}] position {k} '{line_chars[k]}' != chunk[{j}] '{chunk_chars[j]}'")
                j += 1

        # æ£€æŸ¥ï¼šline æ˜¯å¦åœ¨ chunk ä¸­é—´ç»“æŸï¼Ÿ
        # å¦‚æœ j < len(chunk_chars)ï¼Œæ£€æŸ¥å½“å‰ chunk æ˜¯å¦å·²å®Œæˆ
        if k >= len(line_chars) and j < len(chunk_chars):
            current_chunk_idx = chunk_char_map[j][1]

            # æ‰¾åˆ°å½“å‰ chunk çš„èµ·å§‹ä½ç½®
            chunk_start_idx = j
            while chunk_start_idx > 0 and chunk_char_map[chunk_start_idx - 1][1] == current_chunk_idx:
                chunk_start_idx -= 1

            # æ‰¾åˆ°å½“å‰ chunk çš„ç»“æŸä½ç½®
            chunk_end_idx = j
            while chunk_end_idx < len(chunk_char_map) and chunk_char_map[chunk_end_idx][1] == current_chunk_idx:
                chunk_end_idx += 1

            # å¦‚æœå½“å‰ chunk æœªå®Œæˆï¼ˆj ä¸åœ¨ chunk èµ·å§‹ä½ç½®ï¼‰ï¼Œè¯´æ˜ line åœ¨ chunk ä¸­é—´ç»“æŸ
            # j > chunk_start_idx è¡¨ç¤º j åœ¨ chunk ä¸­é—´ï¼ŒspaCy åœ¨ chunk å†…éƒ¨è¿›è¡Œäº†åˆ‡åˆ†
            if j > chunk_start_idx:
                # å½“å‰ chunk çš„ä¸€éƒ¨åˆ†åœ¨å½“å‰ lineï¼Œå¦ä¸€éƒ¨åˆ†åœ¨ä¸‹ä¸€ä¸ª line
                # éœ€è¦æŠŠ chunk å‰©ä½™éƒ¨åˆ†ç§»åˆ°å½“å‰ line
                if i + 1 < len(fixed_lines):
                    next_line = fixed_lines[i + 1]
                    if joiner == " ":
                        next_line_chars = list("".join(next_line.split()).lower())
                    else:
                        next_line_chars = list(next_line.lower())

                    # chunk ä¸­å‰©ä½™çš„å­—ç¬¦æ•°é‡
                    chunk_remaining = chunk_end_idx - j
                    # å½“å‰ line å·²ç»ç»“æŸï¼Œéœ€è¦ä»ä¸‹ä¸€ä¸ª line å– chunk_remaining ä¸ªå­—ç¬¦
                    if len(next_line_chars) >= chunk_remaining:
                        # ä»ä¸‹ä¸€ä¸ª line å–å­—ç¬¦
                        chars_to_move = ''.join(next_line_chars[:chunk_remaining])
                        # åŠ åˆ°å½“å‰ line
                        fixed_lines[i] = line + chars_to_move
                        # ä»ä¸‹ä¸€ä¸ª line å»æ‰è¿™äº›å­—ç¬¦
                        fixed_lines[i + 1] = next_line[chunk_remaining:] if len(next_line) > chunk_remaining else ''
                        print(f"  è¾¹ç•Œä¿®å¤: line[{i}] åŠ  '{chars_to_move}'ï¼ˆæ¥è‡ª chunk {current_chunk_idx} çš„å‰©ä½™éƒ¨åˆ†ï¼‰")
                        changes += 1
                        # æ›´æ–° j ä½ç½®
                        j = chunk_end_idx
                    else:
                        print(f"  è­¦å‘Š: line[{i+1}] å­—ç¬¦ä¸è¶³ï¼Œæ— æ³•ä¿®å¤ chunk {current_chunk_idx} çš„åˆ†å‰²")
                        j = chunk_end_idx
                else:
                    print(f"  è­¦å‘Š: line[{i}] æ˜¯æœ€åä¸€è¡Œï¼Œæ— æ³•ä¿®å¤ chunk {current_chunk_idx} çš„åˆ†å‰²")
                    j = chunk_end_idx

        # ç§»åŠ¨åˆ°ä¸‹ä¸€è¡Œ
        i += 1

    if changes > 0:
        print(f"âœ… å…±ä¿®å¤ {changes} å¤„è¾¹ç•Œé”™è¯¯")
    else:
        print("â„¹ï¸  æœªå‘ç°éœ€è¦ä¿®å¤çš„è¾¹ç•Œ")

    raw_lines = fixed_lines
    # =======================================================================

    print(f"ğŸ“Š æˆåŠŸåŠ è½½ {len(raw_lines)} è¡Œæ–‡æœ¬å’Œ {len(chunks_df)} ä¸ªè¯å—æ—¶é—´æˆ³ã€‚")

    # 3. é¢„å¤„ç†ï¼šå»ºç«‹ (æ¸…æ´—åçš„å­—ç¬¦ -> chunkç´¢å¼•) çš„æ˜ å°„
    # è¿™æ ·å³ä½¿ spaCy ä¿®æ”¹äº†æ–‡æœ¬ï¼Œæˆ‘ä»¬ä¾ç„¶èƒ½é€šè¿‡æ¨¡ç³ŠåŒ¹é…æ‰¾åˆ°å¯¹åº”çš„ chunks
    chunk_char_map = []  # [(char, chunk_idx), ...]
    for idx, row in chunks_df.iterrows():
        chunk_text = str(row['text']).strip('"')
        # æ ¹æ® joiner å†³å®šæ˜¯å¦ä¿ç•™ç©ºæ ¼
        if joiner == " ":
            cleaned = "".join(chunk_text.split()).lower()
        else:
            cleaned = chunk_text.lower()
        # ä¸ºæ¯ä¸ªå­—ç¬¦å»ºç«‹æ˜ å°„
        for char in cleaned:
            chunk_char_map.append({'char': char, 'chunk_idx': idx})

    # æå–çº¯å­—ç¬¦åˆ—è¡¨ç”¨äº difflib åŒ¹é…
    all_chars = [item['char'] for item in chunk_char_map]

    # å®šä¹‰å¥æœ«æ ‡ç‚¹ï¼ˆç”¨äºè¾¹ç•Œä¿®å¤ï¼‰
    sentence_end_punctuations = ['ã€‚', 'ï¼', 'ï¼Ÿ', '.', '!', '?']

    # è®°å½•å½“å‰åœ¨å­—ç¬¦åˆ—è¡¨ä¸­çš„ä½ç½®
    current_char_pos = 0
    final_sentences = []

    # 4. å¯¹æ¯ä¸ªå¥å­è¿›è¡ŒåŒ¹é…å’Œåˆ‡åˆ†
    for sentence in raw_lines:
        # æ ¹æ® joiner å†³å®šæ˜¯å¦å»æ‰ç©ºæ ¼
        if joiner == " ":
            sentence_chars = list("".join(sentence.split()).lower())
        else:
            sentence_chars = list(sentence.lower())

        # å¦‚æœå¥å­ä¸ºç©ºï¼Œè·³è¿‡
        if not sentence_chars:
            continue

        # åœ¨å‰©ä½™å­—ç¬¦åˆ—è¡¨é‡Œè¿›è¡Œæ¨¡ç³ŠåŒ¹é…
        remaining_chars = all_chars[current_char_pos:]

        s = difflib.SequenceMatcher(None, remaining_chars, sentence_chars, autojunk=False)
        matching_blocks = s.get_matching_blocks()

        # å¯»æ‰¾åŒ¹é…å—ï¼šå¿…é¡»åŒ¹é…å¥å­çš„å¼€å¤´ (b_start == 0)
        match_start_rel_idx = -1
        match_length = 0

        for a_start, b_start, length in matching_blocks:
            if b_start == 0:
                match_start_rel_idx = a_start
                match_length = length
                break

        if match_start_rel_idx == -1:
            print(f"\nâš ï¸ Warning: No close match found for sentence: {sentence}")
            # å…œåº•ç­–ç•¥ï¼šä¿ç•™åŸå¥ï¼Œä¸åˆ‡åˆ†
            final_sentences.append(sentence)
            continue

        # è®¡ç®—åŒ¹é…å¼€å§‹åœ¨å­—ç¬¦åˆ—è¡¨ä¸­çš„ç»å¯¹ä½ç½®
        absolute_start_char_idx = current_char_pos + match_start_rel_idx

        # è®¡ç®—åŒ¹é…ç»“æŸä½ç½®ï¼ˆå¤„ç†éè¿ç»­åŒ¹é…ï¼‰
        absolute_end_char_idx = absolute_start_char_idx + match_length - 1

        if match_length < len(sentence_chars):
            found_chars_count = match_length
            for a_start, b_start, length in matching_blocks:
                if a_start <= match_start_rel_idx:
                    continue
                # å¦‚æœè¿™ä¸€å—æ¥ç€ä¸Šä¸€å—çš„å¥å­å†…å®¹
                if b_start == found_chars_count:
                    additional_len = min(length, len(sentence_chars) - found_chars_count)
                    absolute_end_char_idx = current_char_pos + a_start + additional_len - 1
                    found_chars_count += additional_len
                    if found_chars_count >= len(sentence_chars):
                        break

        # è¶Šç•Œä¿æŠ¤
        max_char_idx = len(chunk_char_map) - 1
        absolute_end_char_idx = min(absolute_end_char_idx, max_char_idx)
        absolute_start_char_idx = min(absolute_start_char_idx, max_char_idx)
        if absolute_end_char_idx < absolute_start_char_idx:
            absolute_end_char_idx = absolute_start_char_idx

        # 5. æ ¹æ®å­—ç¬¦ä½ç½®è·å–å¯¹åº”çš„ chunk èŒƒå›´
        start_chunk_idx = chunk_char_map[absolute_start_char_idx]['chunk_idx']
        end_chunk_idx = chunk_char_map[absolute_end_char_idx]['chunk_idx']

        # è·å–è¿™äº› chunks çš„æ•°æ®
        sentence_chunks = chunks_df.iloc[start_chunk_idx:end_chunk_idx + 1]

        # å¦‚æœå¥å­ä¸ºç©ºæˆ–åªæœ‰ 1 ä¸ª chunkï¼Œæ— æ³•è®¡ç®—é—´éš”ï¼Œç›´æ¥ä¿ç•™
        if len(sentence_chunks) < 2:
            final_sentences.append(sentence)
            # æ›´æ–°ä½ç½®åˆ°åŒ¹é…ç»“æŸåçš„ä¸‹ä¸€ä¸ªå­—ç¬¦
            current_char_pos = absolute_end_char_idx + 1
            if current_char_pos >= len(all_chars):
                current_char_pos = len(all_chars) - 1
            continue

        # 6. æ£€æŸ¥ç‰©ç†é—´éš™ (Pause)ï¼Œå¿…è¦æ—¶è¿›ä¸€æ­¥åˆ‡åˆ†
        temp_group = []
        last_chunk = sentence_chunks.iloc[0]
        temp_group.append(str(last_chunk['text']).strip('"'))

        for i in range(1, len(sentence_chunks)):
            current_chunk = sentence_chunks.iloc[i]

            # è®¡ç®—é—´éš™ï¼šå½“å‰è¯ Start - ä¸Šä¸ªè¯ End
            gap = current_chunk['start'] - last_chunk['end']

            if gap > pause_threshold:
                # é—´éš™è¿‡å¤§ï¼Œå¼ºåˆ¶æ–­å¼€æˆæ–°è¡Œ
                final_sentences.append(smart_join(temp_group))
                print(f"âœ‚ï¸  æ£€æµ‹åˆ°åœé¡¿ {gap:.2f}sï¼Œå·²åœ¨è¯å— '{last_chunk['text']}' åæ–­å¥")
                temp_group = [str(current_chunk['text']).strip('"')]
            else:
                temp_group.append(str(current_chunk['text']).strip('"'))

            last_chunk = current_chunk

        if temp_group:
            final_sentences.append(smart_join(temp_group))

        # 7. æ›´æ–°ä½ç½®åˆ°åŒ¹é…ç»“æŸåçš„ä¸‹ä¸€ä¸ªå­—ç¬¦
        current_char_pos = absolute_end_char_idx + 1
        if current_char_pos >= len(all_chars):
            current_char_pos = len(all_chars) - 1

    # 8. å†™å…¥æ–‡ä»¶
    with open(output_pause_path, 'w', encoding='utf-8') as f:
        f.write("\n".join(final_sentences))

    print(f"\nâœ¨ å¤„ç†å®Œæˆï¼")
    print(f"ğŸ“ åŸå§‹è¡Œæ•°: {len(raw_lines)} -> åˆ‡åˆ†åè¡Œæ•°: {len(final_sentences)}")
    print(f"ğŸ’¾ å·²ä¿å­˜åˆ°æ–‡ä»¶ï¼š{output_pause_path}")


if __name__ == '__main__':
    split_by_pause()
