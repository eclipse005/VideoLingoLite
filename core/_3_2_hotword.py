"""
ASR çƒ­è¯çŸ«æ­£æ¨¡å— (Stage 2)

åœ¨ NLP åˆ†å¥åè¿›è¡Œï¼Œä½¿ç”¨ LLM Agent æ™ºèƒ½è¯†åˆ«å¹¶çŸ«æ­£ ASR é”™è¯¯

è¾“å…¥: List[Sentence] (æ¥è‡ª _3_1_split_nlp.py)
è¾“å‡º: List[Sentence] (çŸ«æ­£åï¼Œä¼ é€’ç»™ _3_3_split_meaning.py)
"""

import json
import os
import re
import unicodedata
from typing import List, Tuple, Dict, Any, Optional

from core.utils import rprint, load_key, timer
from core.utils.ask_gpt import ask_gpt_with_tools
from core.utils.models import Sentence, Chunk, Correction
from core.utils.sentence_tools import get_joiner
from core.utils.cache_utils import cache


# ==================== å·¥å…·å®šä¹‰ ====================

SENTENCES_TOOLS_DEFINITION = [
    {
        "type": "function",
        "function": {
            "name": "read_sentences",
            "description": "è¯»å–å¥å­å†…å®¹ï¼Œè¿”å›å…¨éƒ¨æˆ–æŒ‡å®šç´¢å¼•èŒƒå›´",
            "parameters": {
                "type": "object",
                "properties": {
                    "start_idx": {"type": "integer", "description": "èµ·å§‹ç´¢å¼•ï¼ˆå¯é€‰ï¼Œä»0å¼€å§‹ï¼‰"},
                    "end_idx": {"type": "integer", "description": "ç»“æŸç´¢å¼•ï¼ˆå¯é€‰ï¼‰"}
                }
            }
        }
    },
    {
        "type": "function",
        "function": {
            "name": "get_context",
            "description": "è·å–æŸå¥å­çš„ä¸Šä¸‹æ–‡ï¼ˆå‰åå„Nå¥ï¼‰",
            "parameters": {
                "type": "object",
                "properties": {
                    "sentence_idx": {"type": "integer"},
                    "context_count": {"type": "integer", "default": 2}
                },
                "required": ["sentence_idx"]
            }
        }
    },
    {
        "type": "function",
        "function": {
            "name": "batch_replace",
            "description": "æ‰¹é‡æ‰§è¡Œå¤šå¤„æ›¿æ¢ã€‚åœ¨æ‰€æœ‰å¥å­ä¸­æŸ¥æ‰¾å¹¶æ›¿æ¢å¤šç§é”™è¯¯å½¢å¼",
            "parameters": {
                "type": "object",
                "properties": {
                    "replacements": {
                        "type": "array",
                        "items": {
                            "type": "object",
                            "properties": {
                                "old_text": {"type": "string", "description": "è¦æŸ¥æ‰¾çš„é”™è¯¯æ–‡æœ¬"},
                                "new_text": {"type": "string", "description": "æ›¿æ¢åçš„æ­£ç¡®æœ¯è¯­"}
                            },
                            "required": ["old_text", "new_text"]
                        }
                    }
                },
                "required": ["replacements"]
            }
        }
    },
    {
        "type": "function",
        "function": {
            "name": "finish",
            "description": "å®ŒæˆçŸ«æ­£ä»»åŠ¡",
            "parameters": {
                "type": "object",
                "properties": {
                    "summary": {"type": "string", "description": "ä¿®æ”¹æ€»ç»“"}
                },
                "required": ["summary"]
            }
        }
    }
]


# ==================== System Prompt ====================

def build_system_prompt(terms_with_meanings: List[dict]) -> str:
    """æ„å»º System Promptï¼ˆå®Œå…¨å¤ç”¨ agent_correct.pyï¼‰"""
    terms_info = "\n".join([
        f"  - {t['name']}" + (f": {t['meaning']}" if t['meaning'] else "")
        for t in terms_with_meanings
    ])

    return f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ ASRï¼ˆè‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼‰æœ¯è¯­çŸ«æ­£ä¸“å®¶ã€‚

## ä»»åŠ¡
çŸ«æ­£æ–‡æœ¬ä¸­è¢«è¯­éŸ³è¯†åˆ«é”™è¯¯çš„ä¸“ä¸šæœ¯è¯­ã€‚

## æœ¯è¯­åˆ—è¡¨åŠå«ä¹‰
{terms_info}

## âš ï¸ é‡è¦é™åˆ¶ï¼ˆå¿…é¡»éµå®ˆï¼‰
**åªèƒ½çŸ«æ­£ä¸Šè¿°æœ¯è¯­åˆ—è¡¨ä¸­çš„æœ¯è¯­**ï¼Œä¸è¦çŸ«æ­£å…¶ä»–å†…å®¹ã€‚
- å¦‚æœæ–‡æœ¬ä¸­æ²¡æœ‰ä¸Šè¿°æœ¯è¯­çš„é”™è¯¯å½¢å¼ï¼Œç›´æ¥è°ƒç”¨ finish
- ä¸è¦è‡ªåˆ›æœ¯è¯­æˆ–çŸ«æ­£ä¸åœ¨åˆ—è¡¨ä¸­çš„å†…å®¹
- åªå¤„ç†æŒ‡å®šçš„ {len(terms_with_meanings)} ä¸ªæœ¯è¯­
- **å¤æ•°å½¢å¼ä¹Ÿå…è®¸**ï¼šæ ¹æ®çŸ«æ­£å‰çš„æ–‡æœ¬åˆ¤æ–­ï¼ˆå¦‚ abc s â†’ ABCsï¼‰

## å¯ç”¨å·¥å…·
1. read_sentences - è¯»å–å¥å­å†…å®¹
2. get_context - æŸ¥çœ‹æŸå¥çš„ä¸Šä¸‹æ–‡ï¼ˆå‰åå‡ å¥ï¼‰
3. batch_replace - æ‰¹é‡æ›¿æ¢ï¼ˆåœ¨æ‰€æœ‰å¥å­ä¸­æ‰§è¡Œå¤šä¸ªæ›¿æ¢è§„åˆ™ï¼‰
4. finish - å®ŒæˆçŸ«æ­£

## é«˜æ•ˆå·¥ä½œæµç¨‹ï¼ˆé‡è¦ï¼ï¼‰

1. **å…ˆå…¨å±€æ‰«æ**ï¼šç”¨ read_sentences æŸ¥çœ‹å¥å­å†…å®¹
2. **è®°å½•æ‰€æœ‰ç–‘ä¼¼é”™è¯¯**ï¼šåœ¨å¿ƒä¸­/è‰ç¨¿ä¸­è®°å½•æ‰€æœ‰å‘ç°çš„é”™è¯¯ä½ç½®å’Œå½¢å¼
3. **æ‰¹é‡å¤„ç†**ï¼šé˜…è¯»å®Œæˆåï¼Œä½¿ç”¨ batch_replace ä¸€æ¬¡æ€§å®Œæˆæ‰€æœ‰æ›¿æ¢
4. **æœ€åè°ƒç”¨ finish**

### æ‰¹é‡æ“ä½œç¤ºä¾‹

**æ¨èæ–¹å¼ï¼šä½¿ç”¨ batch_replace**
å¦‚æœå‘ç°å¤šç§é”™è¯¯æ¨¡å¼ï¼Œç”¨ä¸€æ¬¡ batch_replace å®Œæˆæ‰€æœ‰æ›¿æ¢ï¼š
```
batch_replace(replacements=[
  {{"old_text": "L L M", "new_text": "LLM"}},
  {{"old_text": "j son", "new_text": "JSON"}},
  {{"old_text": "A P I", "new_text": "API"}}
])
```

### ä½¿ç”¨ batch_replace çš„å…³é”®åŸåˆ™ï¼ˆé‡è¦ï¼ï¼‰

**ä¸€æ¬¡æäº¤æ‰€æœ‰è§„åˆ™ï¼ŒåŒ…æ‹¬å¤æ•°å½¢å¼**ï¼š
- å¦‚æœæœ¯è¯­åœ¨æ–‡æœ¬ä¸­æœ‰å¤æ•°å‡ºç°ï¼Œå¿…é¡»ä¸ºå•æ•°å’Œå¤æ•°å„å†™ä¸€æ¡è§„åˆ™
- ä¸è¦å…ˆæ›¿æ¢å†å›å¤´ä¿®å¤å¤æ•°ï¼Œä¸€æ¬¡åˆ°ä½

ç¤ºä¾‹ï¼šå‡è®¾æœ¯è¯­æ˜¯ "ABC"ï¼Œæ–‡æœ¬ä¸­å‘ç°é”™è¯¯å½¢å¼ "A B C" å’Œ "A B Cs"
```
batch_replace(replacements=[
  {{"old_text": "A B C", "new_text": "ABC"}},
  {{"old_text": "A B Cs", "new_text": "ABCs"}}
])
```

é”™è¯¯åšæ³•ï¼š
- åªæäº¤ {{"old_text": "A B Cs", "new_text": "ABC"}}ï¼ˆä¸¢å¤±å¤æ•°ï¼‰
- å…ˆæäº¤å•æ•°è§„åˆ™ï¼Œåç»­å†è¡¥å……å¤æ•°è§„åˆ™ï¼ˆæµªè´¹è½®æ¬¡ï¼‰

## é‡è¦åŸåˆ™

### å¤æ•°å½¢å¼
- æ ¹æ®ä¸Šä¸‹æ–‡åˆ¤æ–­æ˜¯å¦éœ€è¦å¤æ•°å½¢å¼
- å¦‚æœè¯­å¢ƒæ˜¯å¤æ•°ï¼ˆthese/those/all/multiple/several ç­‰æ ‡è®°ï¼‰ï¼Œæ›¿æ¢æ—¶ä½¿ç”¨å¤æ•°å½¢å¼

ç¤ºä¾‹ï¼š
- "these L L M models" â†’ æ›¿æ¢ä¸º "these LLM models"ï¼ˆå¤æ•°ï¼‰
- "a L L M model" â†’ æ›¿æ¢ä¸º "a LLM model"ï¼ˆå•æ•°ï¼‰

### è°¨æ…ä¿®æ­£
- åªä¿®æ”¹æ˜ç¡®æ˜¯ ASR è¯¯è¯†åˆ«çš„æƒ…å†µ
- å¦‚æœæŸä¸ªè¯åœ¨å½“å‰ä¸Šä¸‹æ–‡ä¸­æ˜¯åˆç†çš„ï¼Œä¸è¦ä¿®æ”¹

ç¤ºä¾‹ï¼š
- "I can see the point" â†’ "see" æ˜¯æ­£ç¡®çš„ï¼Œä¸è¦æ”¹æˆ "C" æˆ– "sea"
- "We use API calls" â†’ "API" æ˜¯æ­£ç¡®çš„ï¼Œä¸éœ€è¦ä¿®æ”¹

## å¸¸è§ ASR é”™è¯¯æ¨¡å¼å‚è€ƒ
- ç©ºæ ¼æ’å…¥ï¼šå­—æ¯é—´è¢«æ’å…¥ç©ºæ ¼ï¼ˆL L M â†’ LLMï¼‰
- å¤§å°å†™é”™è¯¯ï¼šé¦–å­—æ¯æœªå¤§å†™æˆ–å…¨å°å†™ï¼ˆjson â†’ JSONï¼‰
- åŒéŸ³è¿‘éŸ³ï¼šå‘éŸ³ç›¸ä¼¼çš„é”™è¯¯æ›¿æ¢

è¯·å¼€å§‹å·¥ä½œï¼Œè®°ä½ï¼šæ‰¹é‡å¤„ç†ï¼Œä¸è¦é€ä¸ªå¤„ç†ã€‚
"""


def _parse_terms(terms_config: List[str]) -> List[dict]:
    """è§£ææœ¯è¯­åˆ—è¡¨ï¼Œæ”¯æŒ 'æœ¯è¯­ : å«ä¹‰' æ ¼å¼"""
    parsed = []
    for term in terms_config:
        if ' : ' in term:
            parts = term.split(' : ', 1)
        elif ': ' in term:
            parts = term.split(': ', 1)
        elif 'ï¼š' in term:
            parts = term.split('ï¼š', 1)
        else:
            parsed.append({'name': term.strip(), 'meaning': None})
            continue

        parsed.append({
            'name': parts[0].strip(),
            'meaning': parts[1].strip() if len(parts) > 1 else None
        })
    return parsed


class SentenceToolExecutor:
    """Sentence å¯¹è±¡çš„å·¥å…·æ‰§è¡Œå™¨"""

    def __init__(self, sentences: List[Sentence]):
        self.sentences = sentences
        self.changes: List[Dict] = []

    def read_sentences(self, start_idx: int = None, end_idx: int = None) -> str:
        """è¯»å–å¥å­å†…å®¹ï¼Œè¿”å›å…¨éƒ¨æˆ–æŒ‡å®šç´¢å¼•èŒƒå›´"""
        total = len(self.sentences)

        if start_idx is None:
            start_idx = 0
        if end_idx is None:
            end_idx = total

        # é™åˆ¶æœ€å¤šæ˜¾ç¤º 100 å¥
        MAX_DISPLAY = 100
        truncated = False
        actual_end = end_idx

        if actual_end - start_idx > MAX_DISPLAY:
            actual_end = start_idx + MAX_DISPLAY
            truncated = True

        result = []
        for i in range(start_idx, min(actual_end, total)):
            result.append(f"ç¬¬{i}å¥: {self.sentences[i].text}")

        output = "\n".join(result)

        if truncated:
            remaining = total - actual_end
            output += f"\n\n[å…± {total} å¥ï¼Œå·²æ˜¾ç¤ºåˆ°ç¬¬ {actual_end} å¥ï¼Œå‰©ä½™ {remaining} å¥ã€‚"
            output += f"è¯·è°ƒç”¨ read_sentences(start_idx={actual_end}) ç»§ç»­æŸ¥çœ‹]"

        return output

    def get_context(self, sentence_idx: int, context_count: int = 2) -> str:
        """è·å–æŸå¥å­çš„ä¸Šä¸‹æ–‡"""
        start = max(0, sentence_idx - context_count)
        end = min(len(self.sentences), sentence_idx + context_count + 1)

        result = []
        for i in range(start, end):
            marker = ">>> " if i == sentence_idx else "    "
            result.append(f"{marker}ç¬¬{i}å¥: {self.sentences[i].text}")

        return "\n".join(result)

    def batch_replace(self, replacements: list) -> str:
        """æ‰¹é‡æ›¿æ¢æœ¯è¯­ï¼ˆåªä¿®æ”¹æ–‡æœ¬ï¼Œä¸åŠ¨ chunks å’Œæ—¶é—´æˆ³ï¼‰"""
        results = []
        total_changes = 0

        for replacement in replacements:
            old_text = replacement.get("old_text", "")
            new_text = replacement.get("new_text", "")

            if not old_text:
                results.append({"error": "old_text ä¸èƒ½ä¸ºç©º"})
                continue

            # åœ¨æ‰€æœ‰å¥å­ä¸­æŸ¥æ‰¾å¹¶æ›¿æ¢
            for sent_idx, sentence in enumerate(self.sentences):
                changes_count = self._replace_in_sentence(
                    sentence, sent_idx, old_text, new_text
                )
                total_changes += changes_count

            results.append({
                "old_text": old_text,
                "new_text": new_text,
                "count": total_changes
            })

        return json.dumps({
            "success": True,
            "total_changes": total_changes,
            "details": results
        }, ensure_ascii=False)

    def _replace_in_sentence(
        self, sentence: Sentence, sent_idx: int,
        old_text: str, new_text: str
    ) -> int:
        """åœ¨å•ä¸ªå¥å­ä¸­æŸ¥æ‰¾å¹¶æ›¿æ¢ï¼ˆè®°å½• Correction å¹¶ä¿®æ”¹æ–‡æœ¬ï¼‰"""
        changes_count = 0

        for match in re.finditer(re.escape(old_text), sentence.text):
            start, end = match.span()

            if self._is_word_boundary(sentence.text, start, end - start):
                # è®°å½•çŸ«æ­£ï¼ˆåœ¨ä¿®æ”¹æ–‡æœ¬ä¹‹å‰è®°å½•ä½ç½®ï¼‰
                sentence.corrections.append(Correction(
                    old_text=old_text,
                    new_text=new_text,
                    start_idx=start,
                    end_idx=end
                ))

                # æ‰§è¡Œæ›¿æ¢
                sentence.text = (
                    sentence.text[:start] +
                    new_text +
                    sentence.text[end:]
                )
                changes_count += 1

                # ä¿ç•™æ—§çš„ changes è®°å½•ï¼ˆç”¨äºç»Ÿè®¡è¾“å‡ºï¼‰
                self.changes.append({
                    "sentence_idx": sent_idx,
                    "old_text": old_text,
                    "new_text": new_text
                })

        return changes_count

    def _is_word_boundary(self, text: str, pos: int, length: int) -> bool:
        """
        æ£€æŸ¥æŒ‡å®šä½ç½®æ˜¯å¦ä¸ºå•è¯è¾¹ç•Œï¼ˆå¤šè¯­è¨€é€šç”¨ï¼‰

        ç­–ç•¥ï¼šæ£€æŸ¥åŒ¹é…ä½ç½®å‰åæ˜¯å¦ä¸ºå­—æ¯ï¼ˆL* ç±»åˆ«ï¼‰
             å¦‚æœå‰åéƒ½ä¸æ˜¯å­—æ¯ï¼Œåˆ™ä¸ºå•è¯è¾¹ç•Œ

        æ”¯æŒï¼šä¸­ã€æ—¥ã€éŸ©ã€è‹±ã€å¾·ã€ä¿„ã€æ³•ã€æ„ã€è¥¿ç­‰æ‰€æœ‰è¯­è¨€

        Args:
            text: å®Œæ•´æ–‡æœ¬
            pos: åŒ¹é…èµ·å§‹ä½ç½®
            length: åŒ¹é…é•¿åº¦

        Returns:
            bool: True è¡¨ç¤ºæ˜¯å•è¯è¾¹ç•Œï¼ŒFalse è¡¨ç¤ºä¸æ˜¯
        """
        # æ£€æŸ¥å‰ä¸€ä¸ªå­—ç¬¦
        if pos > 0:
            prev_char = text[pos - 1]
            if unicodedata.category(prev_char).startswith('L'):  # å‰é¢æ˜¯å­—æ¯
                return False

        # æ£€æŸ¥åä¸€ä¸ªå­—ç¬¦
        end_pos = pos + length
        if end_pos < len(text):
            next_char = text[end_pos]
            if unicodedata.category(next_char).startswith('L'):  # åé¢æ˜¯å­—æ¯
                return False

        return True  # å‰åéƒ½ä¸æ˜¯å­—æ¯ï¼Œæ˜¯å•è¯è¾¹ç•Œ

    def finish(self, summary: str) -> str:
        """å®ŒæˆçŸ«æ­£ä»»åŠ¡"""
        return json.dumps({
            "changes_count": len(self.changes),
            "summary": summary,
            "is_finish": True
        }, ensure_ascii=False)


# ==================== ä¸»å…¥å£å‡½æ•° ====================

def _rebuild_chunks_from_corrections(sentences: List[Sentence]) -> None:
    """åŸºäº sentence.corrections é‡å»º chunksï¼ˆé”å®šæ—¶é—´è¾¹ç•Œï¼‰"""
    asr_language = load_key("asr.language")
    joiner = get_joiner(asr_language)

    for sentence in sentences:
        if not sentence.corrections:
            continue

        orig_chunks = sentence.chunks
        if not orig_chunks:
            continue

        # æ„å»º chunk ä½ç½®æ˜ å°„ï¼šå­—ç¬¦ä½ç½® -> Chunk
        chunk_map = []
        curr_idx = 0
        for chunk in orig_chunks:
            start_pos = sentence.original_text.find(chunk.text, curr_idx)
            if start_pos == -1:
                start_pos = curr_idx
            end_pos = start_pos + len(chunk.text)
            chunk_map.append({
                "chunk": chunk,
                "start_idx": start_pos,
                "end_idx": end_pos
            })
            curr_idx = end_pos

        # æŒ‰ä½ç½®æ’åº correctionsï¼ˆä»åå¾€å‰å¤„ç†ï¼Œé¿å…ä½ç½®åç§»ï¼‰
        sorted_corrections = sorted(
            sentence.corrections,
            key=lambda c: c.start_idx,
            reverse=True
        )

        final_chunks = []
        skip_until = 0

        for item in chunk_map:
            chunk = item["chunk"]
            chunk_start = item["start_idx"]
            chunk_end = item["end_idx"]

            if chunk_start < skip_until:
                continue

            # æ£€æŸ¥æ˜¯å¦æœ‰ correction æ¶‰åŠè¿™ä¸ª chunk
            correction = None
            for corr in sorted_corrections:
                if not (chunk_end <= corr.start_idx or chunk_start >= corr.end_idx):
                    correction = corr
                    break

            if correction:
                # æ‰¾åˆ°æ‰€æœ‰è¢«è¿™ä¸ª correction å½±å“çš„ chunks
                affected_items = [
                    item for item in chunk_map
                    if not (item["end_idx"] <= correction.start_idx or
                           item["start_idx"] >= correction.end_idx)
                ]

                if affected_items:
                    # é”å®šæ—¶é—´è¾¹ç•Œ
                    fixed_start = affected_items[0]["chunk"].start
                    fixed_end = affected_items[-1]["chunk"].end
                    spk_id = affected_items[0]["chunk"].speaker_id
                else:
                    fixed_start = final_chunks[-1].end if final_chunks else sentence.start
                    fixed_end = fixed_start
                    spk_id = orig_chunks[0].speaker_id

                # ç”¨ new_text åˆ‡åˆ†å¹¶åˆ†é…æ—¶é—´
                new_chunks = _split_text_into_chunks(
                    correction.new_text, fixed_start, fixed_end,
                    asr_language, joiner, spk_id
                )
                final_chunks.extend(new_chunks)

                skip_until = correction.end_idx
            else:
                final_chunks.append(chunk)

        sentence.chunks = final_chunks
        if final_chunks:
            sentence.start = final_chunks[0].start
            sentence.end = final_chunks[-1].end


def _split_text_into_chunks(
    text: str, start_time: float, end_time: float,
    asr_language: str, joiner: str,
    speaker_id: Optional[str]
) -> List[Chunk]:
    """å°†æ–‡æœ¬åˆ‡åˆ†ä¸º chunks å¹¶åœ¨æ—¶é—´èŒƒå›´å†…å¹³å‡åˆ†é…"""
    # æ ¹æ®è¯­è¨€é€‰æ‹©åˆ‡åˆ†æ–¹å¼
    if asr_language in ['zh', 'ja', 'ko']:
        parts = list(text)
    else:
        parts = text.split()
        if joiner:
            text_with_joiner = joiner.join(parts)
            parts = text_with_joiner.split(joiner) if joiner else parts

    if not parts:
        return []

    total_duration = end_time - start_time
    chunk_duration = total_duration / len(parts)

    chunks = []
    current_time = start_time

    for i, part in enumerate(parts):
        chunk_start = current_time
        chunk_end = current_time + chunk_duration

        # æœ€åä¸€ä¸ª chunk ä½¿ç”¨ end_timeï¼ˆé¿å…æµ®ç‚¹è¯¯å·®ï¼‰
        if i == len(parts) - 1:
            chunk_end = end_time

        chunks.append(Chunk(
            text=part,
            start=chunk_start,
            end=chunk_end,
            speaker_id=speaker_id,
            index=i
        ))

        current_time = chunk_end

    return chunks


@timer("ASR æœ¯è¯­çŸ«æ­£")
@cache()
def correct_terms_in_sentences(sentences: List[Sentence]) -> List[Sentence]:
    """
    ä¸»å‡½æ•°ï¼šå¯¹å¥å­åˆ—è¡¨è¿›è¡Œæœ¯è¯­çŸ«æ­£

    Args:
        sentences: NLP åˆ†å¥åçš„ Sentence å¯¹è±¡åˆ—è¡¨

    Returns:
        List[Sentence]: çŸ«æ­£åçš„ Sentence å¯¹è±¡åˆ—è¡¨
    """
    # 1. æ£€æŸ¥å¼€å…³
    enabled = load_key("asr_term_correction.enabled")
    if not enabled:
        rprint("[yellow]â­ï¸ æœ¯è¯­çŸ«æ­£å·²ç¦ç”¨ï¼Œè·³è¿‡[/yellow]")
        return sentences

    # 2. åŠ è½½æœ¯è¯­é…ç½®
    terms_config = load_key("asr_term_correction.terms")
    if not terms_config:
        rprint("[yellow]â­ï¸ æœªé…ç½®æœ¯è¯­ï¼Œè·³è¿‡çŸ«æ­£[/yellow]")
        return sentences

    # 3. è§£ææœ¯è¯­åˆ—è¡¨
    terms_with_meanings = _parse_terms(terms_config)
    rprint(f"[blue]ğŸ” å¼€å§‹æœ¯è¯­çŸ«æ­£ï¼Œå…± {len(terms_with_meanings)} ä¸ªæœ¯è¯­[/blue]")
    for t in terms_with_meanings:
        rprint(f"  - {t['name']}" + (f": {t['meaning']}" if t['meaning'] else ""))

    # 4. ä¿å­˜åŸå§‹æ–‡æœ¬ï¼ˆç”¨äº chunk é‡å»ºï¼‰
    for sent in sentences:
        sent.original_text = sent.text

    # 5. åˆ›å»ºå·¥å…·æ‰§è¡Œå™¨
    tool_executor = SentenceToolExecutor(sentences)

    # 6. æ„å»ºç³»ç»Ÿæç¤ºè¯
    system_prompt = build_system_prompt(terms_with_meanings)

    # 7. è°ƒç”¨ LLM Agent
    user_task = f"è¯·çŸ«æ­£è¿™ {len(sentences)} ä¸ªå¥å­ä¸­çš„æœ¯è¯­é”™è¯¯"

    result = ask_gpt_with_tools(
        system_prompt=system_prompt,
        prompt=user_task,
        tools=SENTENCES_TOOLS_DEFINITION,
        tool_executor=tool_executor,
        max_rounds=20,
        log_title="hotword_correction"
    )

    # 8. è¾“å‡ºç»Ÿè®¡
    if result and result.get("is_finish"):
        changes_count = result.get("changes_count", 0)
        if changes_count > 0:
            rprint(f"[green]âœ… çŸ«æ­£å®Œæˆ: {changes_count} å¤„ä¿®æ”¹[/green]")

            # æ˜¾ç¤ºä¿®æ”¹æ˜ç»†
            from collections import Counter
            stats = Counter(f"{c['old_text']} â†’ {c['new_text']}" for c in tool_executor.changes)
            for change, count in stats.most_common():
                rprint(f"  {change}: {count} å¤„")

            # ä¿å­˜çŸ«æ­£è®°å½•åˆ°æ–‡ä»¶ï¼ˆä¸æ§åˆ¶å°è¾“å‡ºä¸€è‡´ï¼‰
            _save_correction_log(stats, changes_count)
        else:
            rprint("[green]âœ… æœªå‘ç°éœ€è¦çŸ«æ­£çš„é”™è¯¯[/green]")
    else:
        rprint("[yellow]âš ï¸ LLM æœªæ­£å¸¸å®Œæˆï¼Œè¿”å›åŸå§‹å¥å­[/yellow]")

    # 9. åŒæ­¥ chunksï¼ˆåŸºäº corrections é‡å»ºï¼‰
    _rebuild_chunks_from_corrections(sentences)

    return sentences


def _save_correction_log(stats, changes_count):
    """ä¿å­˜çŸ«æ­£æ—¥å¿—åˆ° output/log/hotword_correct.txt"""
    log_path = "output/log/hotword_correct.txt"

    with open(log_path, 'w', encoding='utf-8') as f:
        if changes_count > 0:
            f.write(f"âœ… çŸ«æ­£å®Œæˆ: {changes_count} å¤„ä¿®æ”¹\n")
            f.write("\n")
            for change, count in stats.most_common():
                f.write(f"  {change}: {count} å¤„\n")
        else:
            f.write("âœ… æœªå‘ç°éœ€è¦çŸ«æ­£çš„é”™è¯¯\n")

    rprint(f"[dim]ğŸ“ çŸ«æ­£æ—¥å¿—å·²ä¿å­˜: {log_path}[/dim]")
