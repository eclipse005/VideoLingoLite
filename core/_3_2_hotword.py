"""
ASR çƒ­è¯çŸ«æ­£æ¨¡å— (Stage 2)

åœ¨ NLP åˆ†å¥åè¿›è¡Œï¼Œä½¿ç”¨ LLM Agent æ™ºèƒ½è¯†åˆ«å¹¶çŸ«æ­£ ASR é”™è¯¯

è¾“å…¥: List[Sentence] (æ¥è‡ª _3_1_split_nlp.py)
è¾“å‡º: List[Sentence] (çŸ«æ­£åï¼Œä¼ é€’ç»™ _3_3_split_meaning.py)
"""

import json
import os
import re
import unicodedata
from typing import List, Tuple, Dict, Any, Optional

from core.utils import rprint, load_key, timer
from core.utils.ask_gpt import ask_gpt_with_tools
from core.utils.models import Sentence, Chunk, Correction
from core.utils.sentence_tools import get_joiner
from core.utils.cache_utils import cache


# ==================== å·¥å…·å®šä¹‰ ====================

SENTENCES_TOOLS_DEFINITION = [
    {
        "type": "function",
        "function": {
            "name": "read_sentences",
            "description": "è¯»å–å¥å­å†…å®¹ï¼Œè¿”å›å…¨éƒ¨æˆ–æŒ‡å®šç´¢å¼•èŒƒå›´",
            "parameters": {
                "type": "object",
                "properties": {
                    "start_idx": {"type": "integer", "description": "èµ·å§‹ç´¢å¼•ï¼ˆå¯é€‰ï¼Œä»0å¼€å§‹ï¼‰"},
                    "end_idx": {"type": "integer", "description": "ç»“æŸç´¢å¼•ï¼ˆå¯é€‰ï¼‰"}
                }
            }
        }
    },
    {
        "type": "function",
        "function": {
            "name": "batch_replace",
            "description": "æ‰¹é‡æ‰§è¡Œå¤šå¤„æ›¿æ¢ã€‚åœ¨æ‰€æœ‰å¥å­ä¸­æŸ¥æ‰¾å¹¶æ›¿æ¢å¤šç§é”™è¯¯å½¢å¼",
            "parameters": {
                "type": "object",
                "properties": {
                    "replacements": {
                        "type": "array",
                        "items": {
                            "type": "object",
                            "properties": {
                                "old_text": {"type": "string", "description": "è¦æŸ¥æ‰¾çš„é”™è¯¯æ–‡æœ¬"},
                                "new_text": {"type": "string", "description": "æ›¿æ¢åçš„æ­£ç¡®æœ¯è¯­"}
                            },
                            "required": ["old_text", "new_text"]
                        }
                    }
                },
                "required": ["replacements"]
            }
        }
    },
    {
        "type": "function",
        "function": {
            "name": "finish",
            "description": "å®ŒæˆçŸ«æ­£ä»»åŠ¡",
            "parameters": {
                "type": "object",
                "properties": {
                    "summary": {"type": "string", "description": "ä¿®æ”¹æ€»ç»“"}
                },
                "required": ["summary"]
            }
        }
    },
    {
        "type": "function",
        "function": {
            "name": "analyze_frame",
            "description": "åˆ†æè§†é¢‘åœ¨æŒ‡å®šæ—¶é—´æˆ³çš„ç”»é¢å†…å®¹ï¼Œè¯†åˆ«å±å¹•ä¸Šæ˜¾ç¤ºçš„æ–‡å­—ï¼ˆå¦‚å›¾è¡¨æ ‡é¢˜ã€ç•Œé¢æ ‡ç­¾ã€å­—å¹•ç­‰ï¼‰ã€‚å½“ä½ éš¾ä»¥åˆ¤æ–­æ­£ç¡®çš„æœ¯è¯­æ—¶ï¼Œå¯ä»¥è°ƒç”¨æ­¤å·¥å…·æŸ¥çœ‹ç”»é¢è¾…åŠ©å†³ç­–ã€‚",
            "parameters": {
                "type": "object",
                "properties": {
                    "timestamp": {
                        "type": "number",
                        "description": "è§†é¢‘æ—¶é—´æˆ³ï¼ˆç§’ï¼‰"
                    }
                },
                "required": ["timestamp"]
            }
        }
    }
]


# ==================== System Prompt ====================

def build_system_prompt(terms_with_meanings: List[dict], asr_language: str) -> str:
    """æ„å»ºé€šç”¨çš„ System Prompt"""
    terms_info = "\n".join([
        f"  - {t['name']}" + (f"ï¼ˆ{t['meaning']}ï¼‰" if t['meaning'] else "")
        for t in terms_with_meanings
    ])

    return f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ ASRï¼ˆè‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼‰æœ¯è¯­çŸ«æ­£ä¸“å®¶ã€‚

## ä»»åŠ¡
éŸ³é¢‘è½¬å½•ä¸­å¸¸å› å£éŸ³ã€æ‚éŸ³å°†ä¸“ä¸šæœ¯è¯­è¯¯è¯†åˆ«ä¸ºå‘éŸ³ç›¸è¿‘çš„æ™®é€šè¯æ±‡ã€‚ä½ éœ€è¦é€šè¿‡ä¸Šä¸‹æ–‡åˆ†æå’Œè§†è§‰è¾…åŠ©ï¼Œå°†è¿™äº›é”™è¯¯è¿˜åŸä¸ºæœ¯è¯­åˆ—è¡¨ä¸­çš„æ­£ç¡®è¡¨è¾¾ã€‚

## éŸ³é¢‘è¯­è¨€
{asr_language}

## æœ¯è¯­åº“
{terms_info}

## æ ¸å¿ƒåˆ¤æ–­æ–¹æ³•

å¯¹æ¯ä¸ªå¯ç–‘è¯ï¼Œé—®è‡ªå·±ï¼š**å¿«é€ŸæŠŠè¿™ä¸ªè¯è¯»å‡ºæ¥ï¼Œå¬èµ·æ¥åƒä¸åƒæœ¯è¯­åº“ä¸­çš„æŸä¸ªæœ¯è¯­ï¼Ÿ**

å¦‚æœå‘éŸ³ç›¸ä¼¼ï¼Œä¸”ä¸Šä¸‹æ–‡ä¸­è¯¥æœ¯è¯­å‡ºç°æ˜¯åˆç†çš„ï¼Œå°±æ›¿æ¢ã€‚

## âš ï¸ é‡è¦åŸåˆ™

1. **åªçŸ«æ­£æœ¯è¯­åº“ä¸­çš„æœ¯è¯­**ï¼Œä¸è¦çŸ«æ­£å…¶ä»–å†…å®¹
2. **å¿…é¡»å‘éŸ³ç›¸ä¼¼ä¸”ä¸Šä¸‹æ–‡åˆç†æ‰æ›¿æ¢**
3. **ä¸ç¡®å®šæ—¶ä¸æ›¿æ¢**
4. **å¦‚æœæ²¡æœ‰å‘ç°é”™è¯¯ï¼Œç›´æ¥è°ƒç”¨ finish**

## è§†è§‰è¾…åŠ© (analyze_frame) è°ƒç”¨å‡†åˆ™

å½“åˆ¤æ–­å›°éš¾æ—¶ï¼Œå¯è°ƒç”¨ analyze_frame(timestamp) æŸ¥çœ‹ç”»é¢è¾…åŠ©å†³ç­–ã€‚

- timestampï¼šè¯¥æœ¯è¯­æ‰€åœ¨å¥å­çš„æ—¶é—´æˆ³
- ç”»é¢å†…å®¹å¯èƒ½åŒ…å«ç›¸å…³ä¿¡æ¯æ—¶æ‰ä½¿ç”¨
- å¦‚æœä»…å‡­éŸ³é¢‘ä¸Šä¸‹æ–‡å°±èƒ½ç¡®å®šï¼Œæ— éœ€è°ƒç”¨è§†è§‰è¾…åŠ©

## å·¥ä½œæµç¨‹

1. ç”¨ read_sentences è¯»å–æ‰€æœ‰å¥å­ï¼ˆæ³¨æ„æ¯å¥éƒ½æœ‰æ—¶é—´æˆ³ [start-end]ï¼‰
2. é€šè¯»å…¨æ–‡ï¼Œç†è§£æ•´ä½“åœ¨è®¨è®ºä»€ä¹ˆé¢†åŸŸ
3. é€ä¸ªæœ¯è¯­æ€è€ƒï¼šè¿™ä¸ªæœ¯è¯­çš„å‘éŸ³ï¼Œåœ¨æ–‡ä¸­æœ‰æ²¡æœ‰è¢«é”™è¯¯è¯†åˆ«çš„å½¢å¼ï¼Ÿ
4. å¦‚æœåˆ¤æ–­å›°éš¾ï¼Œå¯è°ƒç”¨ analyze_frame(timestamp) æŸ¥çœ‹ç”»é¢è¾…åŠ©å†³ç­–
5. æ”¶é›†æ‰€æœ‰å‘ç°çš„é”™è¯¯å¯¹ï¼Œç”¨ä¸€æ¬¡ batch_replace å®Œæˆæ›¿æ¢
6. è°ƒç”¨ finish

### æ‰¹é‡æ“ä½œç¤ºä¾‹

å‡è®¾æœ¯è¯­åº“ä¸­æœ‰æŸæŠ€æœ¯æœ¯è¯­å’ŒæŸäº§å“åç§°ï¼Œåœ¨è½¬å½•æ–‡æœ¬ä¸­å‘ç°ï¼š
- "é”™è¯¯å½¢å¼1" è¯»èµ·æ¥åƒ "æœ¯è¯­1"ï¼Œä¸”ä¸Šä¸‹æ–‡åœ¨è®¨è®ºç›¸å…³æŠ€æœ¯
- "é”™è¯¯å½¢å¼2" è¯»èµ·æ¥åƒ "æœ¯è¯­2"ï¼Œä¸”ä¸Šä¸‹æ–‡åœ¨è®¨è®ºç›¸å…³äº§å“

```
batch_replace(replacements=[
  {{"old_text": "é”™è¯¯å½¢å¼1", "new_text": "æœ¯è¯­1"}},
  {{"old_text": "é”™è¯¯å½¢å¼2", "new_text": "æœ¯è¯­2"}}
])
```

### å¤æ•°å½¢å¼
å¦‚æœæœ¯è¯­æœ‰å¤æ•°å½¢å¼å‡ºç°ï¼Œå•æ•°å’Œå¤æ•°å„å†™ä¸€æ¡è§„åˆ™ï¼Œä¸€æ¬¡åˆ°ä½ã€‚
"""


def _parse_terms(terms_config: List[str]) -> List[dict]:
    """è§£ææœ¯è¯­åˆ—è¡¨ï¼Œæ”¯æŒ 'æœ¯è¯­ : å«ä¹‰' æ ¼å¼"""
    parsed = []
    for term in terms_config:
        if ' : ' in term:
            parts = term.split(' : ', 1)
        elif ': ' in term:
            parts = term.split(': ', 1)
        elif 'ï¼š' in term:
            parts = term.split('ï¼š', 1)
        else:
            parsed.append({'name': term.strip(), 'meaning': None})
            continue

        parsed.append({
            'name': parts[0].strip(),
            'meaning': parts[1].strip() if len(parts) > 1 else None
        })
    return parsed


class SentenceToolExecutor:
    """Sentence å¯¹è±¡çš„å·¥å…·æ‰§è¡Œå™¨"""

    def __init__(self, sentences: List[Sentence], video_path: Optional[str] = None):
        self.sentences = sentences
        self.changes: List[Dict] = []
        self.video_path = video_path
        self.vision_calls: List[Dict] = []  # è®°å½•è§†è§‰è¾…åŠ©è°ƒç”¨
        self.last_vision_call: Optional[Dict] = None  # æœ€åä¸€æ¬¡è§†è§‰è¾…åŠ©è°ƒç”¨ï¼ˆç”¨äºæ ‡è®°åç»­æ›¿æ¢ï¼‰

    def read_sentences(self, start_idx: int = None, end_idx: int = None) -> str:
        """è¯»å–å¥å­å†…å®¹ï¼Œè¿”å›å…¨éƒ¨æˆ–æŒ‡å®šç´¢å¼•èŒƒå›´"""
        total = len(self.sentences)

        if start_idx is None:
            start_idx = 0
        if end_idx is None:
            end_idx = total

        # é™åˆ¶æœ€å¤šæ˜¾ç¤º 500 å¥
        MAX_DISPLAY = 500
        truncated = False
        actual_end = end_idx

        if actual_end - start_idx > MAX_DISPLAY:
            actual_end = start_idx + MAX_DISPLAY
            truncated = True

        result = []
        for i in range(start_idx, min(actual_end, total)):
            s = self.sentences[i]
            # æ˜¾ç¤ºæ—¶é—´æˆ³ï¼Œæ–¹ä¾¿ LLM è°ƒç”¨ analyze_frame
            result.append(f"ç¬¬{i}å¥ [{s.start:.1f}s-{s.end:.1f}s]: {s.text}")

        output = "\n".join(result)
        if truncated:
            remaining = total - actual_end
            output += f"\n\n[å…± {total} å¥ï¼Œå·²æ˜¾ç¤ºåˆ°ç¬¬ {actual_end} å¥ï¼Œå‰©ä½™ {remaining} å¥ã€‚"
            output += f"è¯·è°ƒç”¨ read_sentences(start_idx={actual_end}) ç»§ç»­æŸ¥çœ‹]"

        return output


    def batch_replace(self, replacements: list) -> str:
        """æ‰¹é‡æ›¿æ¢æœ¯è¯­ï¼ˆåªä¿®æ”¹æ–‡æœ¬ï¼Œä¸åŠ¨ chunks å’Œæ—¶é—´æˆ³ï¼‰"""
        results = []
        total_changes = 0

        for replacement in replacements:
            old_text = replacement.get("old_text", "")
            new_text = replacement.get("new_text", "")

            if not old_text:
                results.append({"error": "old_text ä¸èƒ½ä¸ºç©º"})
                continue

            # åœ¨æ‰€æœ‰å¥å­ä¸­æŸ¥æ‰¾å¹¶æ›¿æ¢
            rule_changes = 0
            for sent_idx, sentence in enumerate(self.sentences):
                changes_count = self._replace_in_sentence(
                    sentence, sent_idx, old_text, new_text
                )
                rule_changes += changes_count
            total_changes += rule_changes

            results.append({
                "old_text": old_text,
                "new_text": new_text,
                "count": rule_changes
            })

        # æ¸…é™¤è§†è§‰è¾…åŠ©æ ‡è®°ï¼ˆåªæ ‡è®°ç´§è·Ÿåœ¨è§†è§‰è¾…åŠ©åçš„æ›¿æ¢ï¼‰
        self.last_vision_call = None

        return json.dumps({
            "success": True,
            "total_changes": total_changes,
            "details": results
        }, ensure_ascii=False)

    def _replace_in_sentence(
        self, sentence: Sentence, sent_idx: int,
        old_text: str, new_text: str
    ) -> int:
        """åœ¨å•ä¸ªå¥å­ä¸­æŸ¥æ‰¾å¹¶æ›¿æ¢ï¼ˆè®°å½• Correction å¹¶ä¿®æ”¹æ–‡æœ¬ï¼‰"""
        # å…ˆæ”¶é›†æ‰€æœ‰åŒ¹é…ï¼Œé¿å…åœ¨å¾ªç¯ä¸­ä¿®æ”¹æ–‡æœ¬å¯¼è‡´ä½ç½®æ¼‚ç§»
        matches = [
            m for m in re.finditer(re.escape(old_text), sentence.text)
            if self._is_word_boundary(sentence.text, m.start(), m.end() - m.start())
        ]

        # ä»åå¾€å‰æ›¿æ¢ï¼Œä¸å½±å“å‰é¢çš„ä½ç½®
        for match in reversed(matches):
            start, end = match.span()

            # è®°å½•çŸ«æ­£ï¼ˆåœ¨ä¿®æ”¹æ–‡æœ¬ä¹‹å‰è®°å½•ä½ç½®ï¼‰
            sentence.corrections.append(Correction(
                old_text=old_text,
                new_text=new_text,
                start_idx=start,
                end_idx=end
            ))

            # æ‰§è¡Œæ›¿æ¢
            sentence.text = (
                sentence.text[:start] +
                new_text +
                sentence.text[end:]
            )

            # ä¿ç•™æ—§çš„ changes è®°å½•ï¼ˆç”¨äºç»Ÿè®¡è¾“å‡ºï¼‰
            change_record = {
                "sentence_idx": sent_idx,
                "old_text": old_text,
                "new_text": new_text
            }
            # å¦‚æœç´§è·Ÿåœ¨è§†è§‰è¾…åŠ©è°ƒç”¨ä¹‹åï¼Œæ ‡è®°ä¸ºä½¿ç”¨äº†è§†è§‰è¾…åŠ©
            if self.last_vision_call:
                change_record["vision_assisted"] = True
                change_record["vision_timestamp"] = self.last_vision_call["timestamp"]
                change_record["vision_result"] = self.last_vision_call["result"]
            self.changes.append(change_record)

        return len(matches)

    def _is_word_boundary(self, text: str, pos: int, length: int) -> bool:
        """
        æ£€æŸ¥æŒ‡å®šä½ç½®æ˜¯å¦ä¸ºå•è¯è¾¹ç•Œï¼ˆå¤šè¯­è¨€é€šç”¨ï¼‰

        ç­–ç•¥ï¼šæ£€æŸ¥åŒ¹é…ä½ç½®å‰åæ˜¯å¦ä¸ºå­—æ¯ï¼ˆL* ç±»åˆ«ï¼‰
             å¦‚æœå‰åéƒ½ä¸æ˜¯å­—æ¯ï¼Œåˆ™ä¸ºå•è¯è¾¹ç•Œ

        æ”¯æŒï¼šä¸­ã€æ—¥ã€éŸ©ã€è‹±ã€å¾·ã€ä¿„ã€æ³•ã€æ„ã€è¥¿ç­‰æ‰€æœ‰è¯­è¨€

        Args:
            text: å®Œæ•´æ–‡æœ¬
            pos: åŒ¹é…èµ·å§‹ä½ç½®
            length: åŒ¹é…é•¿åº¦

        Returns:
            bool: True è¡¨ç¤ºæ˜¯å•è¯è¾¹ç•Œï¼ŒFalse è¡¨ç¤ºä¸æ˜¯
        """
        # æ£€æŸ¥å‰ä¸€ä¸ªå­—ç¬¦
        if pos > 0:
            prev_char = text[pos - 1]
            if unicodedata.category(prev_char).startswith('L'):  # å‰é¢æ˜¯å­—æ¯
                return False

        # æ£€æŸ¥åä¸€ä¸ªå­—ç¬¦
        end_pos = pos + length
        if end_pos < len(text):
            next_char = text[end_pos]
            if unicodedata.category(next_char).startswith('L'):  # åé¢æ˜¯å­—æ¯
                return False

        return True  # å‰åéƒ½ä¸æ˜¯å­—æ¯ï¼Œæ˜¯å•è¯è¾¹ç•Œ

    def finish(self, summary: str) -> str:
        """å®ŒæˆçŸ«æ­£ä»»åŠ¡"""
        return json.dumps({
            "changes_count": len(self.changes),
            "summary": summary,
            "is_finish": True
        }, ensure_ascii=False)

    def analyze_frame(self, timestamp: float) -> str:
        """
        åˆ†æè§†é¢‘æŒ‡å®šæ—¶é—´æˆ³çš„ç”»é¢å†…å®¹

        Args:
            timestamp: è§†é¢‘æ—¶é—´æˆ³ï¼ˆç§’ï¼‰

        Returns:
            ç”»é¢ä¸­è¯†åˆ«åˆ°çš„å†…å®¹ï¼Œæˆ–é”™è¯¯ä¿¡æ¯ï¼ˆJSONæ ¼å¼ï¼‰
        """
        from core.utils.ask_gpt import ask_gpt_vision
        import subprocess
        from pathlib import Path

        # éªŒè¯ video_path
        if not self.video_path:
            return json.dumps({
                "error": "è§†é¢‘æ–‡ä»¶è·¯å¾„æœªè®¾ç½®",
                "suggestion": "è¯·ç¡®ä¿è§†é¢‘æ–‡ä»¶å·²æ­£ç¡®åŠ è½½"
            }, ensure_ascii=False)

        # éªŒè¯ timestamp
        if timestamp < 0:
            return json.dumps({
                "error": f"æ— æ•ˆçš„æ—¶é—´æˆ³: {timestamp}",
                "suggestion": "æ—¶é—´æˆ³å¿…é¡» >= 0"
            }, ensure_ascii=False)

        video_path = self.video_path

        # åˆ›å»ºè¾“å‡ºç›®å½•
        output_dir = Path("output/log/pic")
        output_dir.mkdir(parents=True, exist_ok=True)

        # æ–‡ä»¶å: å¸§æ—¶é—´æˆ³.png
        output_path = output_dir / f"frame_{timestamp:.1f}s.png"

        # ç”¨ ffmpeg æå–å•å¸§
        try:
            subprocess.run([
                "ffmpeg", "-ss", str(timestamp),
                "-i", str(video_path),
                "-vframes", "1",
                "-q:v", "2",
                "-y", str(output_path)
            ], check=True, capture_output=True, timeout=60)
        except FileNotFoundError:
            return json.dumps({
                "error": "ffmpeg æœªå®‰è£…æˆ–ä¸åœ¨ PATH ä¸­",
                "suggestion": "è¯·å®‰è£… ffmpeg å¹¶ç¡®ä¿å…¶åœ¨ç³»ç»Ÿ PATH ä¸­"
            }, ensure_ascii=False)
        except subprocess.CalledProcessError as e:
            return json.dumps({
                "error": f"ffmpeg æ‰§è¡Œå¤±è´¥",
                "suggestion": "è¯·æ£€æŸ¥è§†é¢‘æ–‡ä»¶æ˜¯å¦å­˜åœ¨ä¸”æ ¼å¼æœ‰æ•ˆ",
                "stderr": e.stderr.decode('utf-8', errors='ignore') if e.stderr else str(e)
            }, ensure_ascii=False)
        except subprocess.TimeoutExpired:
            return json.dumps({
                "error": "ffmpeg æ‰§è¡Œè¶…æ—¶",
                "suggestion": "è§†é¢‘æ–‡ä»¶å¯èƒ½æŸåæˆ–æ—¶é—´æˆ³è¶…å‡ºè§†é¢‘é•¿åº¦"
            }, ensure_ascii=False)

        # è°ƒç”¨ Vision API
        try:
            result = ask_gpt_vision(str(output_path), "è¯·åˆ†æè¿™å¼ å›¾ç‰‡")
            # è®°å½•æˆåŠŸçš„è§†è§‰è¾…åŠ©è°ƒç”¨
            vision_call = {
                "timestamp": timestamp,
                "result": result[:100] + "..." if len(result) > 100 else result
            }
            self.vision_calls.append(vision_call)
            # è®¾ç½®æ ‡è®°ï¼Œç”¨äºå…³è”åç»­çš„æœ¯è¯­æ›¿æ¢
            self.last_vision_call = vision_call
            return result
        except Exception as e:
            return json.dumps({
                "error": f"Vision API è°ƒç”¨å¤±è´¥: {str(e)}",
                "suggestion": "è¯·æ£€æŸ¥ API å¯†é’¥é…ç½®æˆ–ç½‘ç»œè¿æ¥"
            }, ensure_ascii=False)


# ==================== ä¸»å…¥å£å‡½æ•° ====================

def _rebuild_chunks_from_corrections(sentences: List[Sentence]) -> None:
    """åŸºäº sentence.corrections é‡å»º chunksï¼ˆé”å®šæ—¶é—´è¾¹ç•Œï¼‰"""
    asr_language = load_key("asr.language")
    joiner = get_joiner(asr_language)

    for sentence in sentences:
        if not sentence.corrections:
            continue

        orig_chunks = sentence.chunks
        if not orig_chunks:
            continue

        # æ„å»º chunk ä½ç½®æ˜ å°„ï¼šå­—ç¬¦ä½ç½® -> Chunk
        chunk_map = []
        curr_idx = 0
        for chunk in orig_chunks:
            start_pos = sentence.original_text.find(chunk.text, curr_idx)
            if start_pos == -1:
                start_pos = curr_idx
            end_pos = start_pos + len(chunk.text)
            chunk_map.append({
                "chunk": chunk,
                "start_idx": start_pos,
                "end_idx": end_pos
            })
            curr_idx = end_pos

        # æŒ‰ä½ç½®æ’åº correctionsï¼ˆä»åå¾€å‰å¤„ç†ï¼Œé¿å…ä½ç½®åç§»ï¼‰
        sorted_corrections = sorted(
            sentence.corrections,
            key=lambda c: c.start_idx,
            reverse=True
        )

        final_chunks = []
        skip_until = 0

        for item in chunk_map:
            chunk = item["chunk"]
            chunk_start = item["start_idx"]
            chunk_end = item["end_idx"]

            if chunk_start < skip_until:
                continue

            # æ£€æŸ¥æ˜¯å¦æœ‰ correction æ¶‰åŠè¿™ä¸ª chunk
            correction = None
            for corr in sorted_corrections:
                if not (chunk_end <= corr.start_idx or chunk_start >= corr.end_idx):
                    correction = corr
                    break

            if correction:
                # æ‰¾åˆ°æ‰€æœ‰è¢«è¿™ä¸ª correction å½±å“çš„ chunks
                affected_items = [
                    item for item in chunk_map
                    if not (item["end_idx"] <= correction.start_idx or
                           item["start_idx"] >= correction.end_idx)
                ]

                if affected_items:
                    # é”å®šæ—¶é—´è¾¹ç•Œ
                    fixed_start = affected_items[0]["chunk"].start
                    fixed_end = affected_items[-1]["chunk"].end
                    spk_id = affected_items[0]["chunk"].speaker_id
                else:
                    fixed_start = final_chunks[-1].end if final_chunks else sentence.start
                    fixed_end = fixed_start
                    spk_id = orig_chunks[0].speaker_id

                # ç”¨ new_text åˆ‡åˆ†å¹¶åˆ†é…æ—¶é—´
                new_chunks = _split_text_into_chunks(
                    correction.new_text, fixed_start, fixed_end,
                    asr_language, joiner, spk_id
                )
                final_chunks.extend(new_chunks)

                skip_until = correction.end_idx
            else:
                final_chunks.append(chunk)

        sentence.chunks = final_chunks
        if final_chunks:
            sentence.start = final_chunks[0].start
            sentence.end = final_chunks[-1].end


def _split_text_into_chunks(
    text: str, start_time: float, end_time: float,
    asr_language: str, joiner: str,
    speaker_id: Optional[str]
) -> List[Chunk]:
    """å°†æ–‡æœ¬åˆ‡åˆ†ä¸º chunks å¹¶åœ¨æ—¶é—´èŒƒå›´å†…å¹³å‡åˆ†é…"""
    # æ ¹æ®è¯­è¨€é€‰æ‹©åˆ‡åˆ†æ–¹å¼
    if asr_language in ['zh', 'ja', 'ko']:
        parts = list(text)
    else:
        parts = text.split()
        if joiner:
            text_with_joiner = joiner.join(parts)
            parts = text_with_joiner.split(joiner) if joiner else parts

    if not parts:
        return []

    total_duration = end_time - start_time
    chunk_duration = total_duration / len(parts)

    chunks = []
    current_time = start_time

    for i, part in enumerate(parts):
        chunk_start = current_time
        chunk_end = current_time + chunk_duration

        # æœ€åä¸€ä¸ª chunk ä½¿ç”¨ end_timeï¼ˆé¿å…æµ®ç‚¹è¯¯å·®ï¼‰
        if i == len(parts) - 1:
            chunk_end = end_time

        chunks.append(Chunk(
            text=part,
            start=chunk_start,
            end=chunk_end,
            speaker_id=speaker_id,
            index=i
        ))

        current_time = chunk_end

    return chunks


@timer("ASR æœ¯è¯­çŸ«æ­£")
@cache()
def correct_terms_in_sentences(sentences: List[Sentence]) -> List[Sentence]:
    """
    ä¸»å‡½æ•°ï¼šå¯¹å¥å­åˆ—è¡¨è¿›è¡Œæœ¯è¯­çŸ«æ­£

    Args:
        sentences: NLP åˆ†å¥åçš„ Sentence å¯¹è±¡åˆ—è¡¨

    Returns:
        List[Sentence]: çŸ«æ­£åçš„ Sentence å¯¹è±¡åˆ—è¡¨
    """
    # 0. åˆå§‹åŒ–çƒ­è¯åˆ†ç»„ï¼ˆå¦‚æœéœ€è¦ï¼‰
    from core.utils.config_utils import init_hotword_groups
    init_hotword_groups()

    # 1. æ£€æŸ¥å¼€å…³
    enabled = load_key("asr_term_correction.enabled")
    if not enabled:
        rprint("[yellow]â­ï¸ æœ¯è¯­çŸ«æ­£å·²ç¦ç”¨ï¼Œè·³è¿‡[/yellow]")
        return sentences

    # 2. åŠ è½½æ¿€æ´»åˆ†ç»„çš„çƒ­è¯é…ç½®
    active_group_id = load_key("asr_term_correction.active_group_id")
    groups = load_key("asr_term_correction.groups") or []

    # æ‰¾åˆ°æ¿€æ´»åˆ†ç»„
    active_group = next((g for g in groups if g["id"] == active_group_id), None)

    if not active_group:
        rprint("[yellow]âš ï¸ æœªæ‰¾åˆ°æ¿€æ´»çš„çƒ­è¯åˆ†ç»„ï¼Œè·³è¿‡çŸ«æ­£[/yellow]")
        return sentences

    terms_config = active_group.get("keyterms", [])
    if not terms_config:
        rprint(f"[yellow]â­ï¸ åˆ†ç»„ '{active_group['name']}' ä¸­æœªé…ç½®çƒ­è¯ï¼Œè·³è¿‡çŸ«æ­£[/yellow]")
        return sentences

    # 3. è§£ææœ¯è¯­åˆ—è¡¨
    terms_with_meanings = _parse_terms(terms_config)
    rprint(f"[blue]ğŸ” å¼€å§‹æœ¯è¯­çŸ«æ­£ï¼Œå…± {len(terms_with_meanings)} ä¸ªæœ¯è¯­[/blue]")
    for t in terms_with_meanings:
        rprint(f"  - {t['name']}" + (f": {t['meaning']}" if t['meaning'] else ""))

    # 4. ä¿å­˜åŸå§‹æ–‡æœ¬ï¼ˆç”¨äº chunk é‡å»ºï¼‰
    for sent in sentences:
        sent.original_text = sent.text

    # 5. è·å–è§†é¢‘æ–‡ä»¶è·¯å¾„
    from core._1_ytdlp import find_video_files
    video_path = find_video_files()

    # 6. åˆ›å»ºå·¥å…·æ‰§è¡Œå™¨
    tool_executor = SentenceToolExecutor(sentences, video_path=video_path)

    # 7. æ„å»ºç³»ç»Ÿæç¤ºè¯ï¼ˆä¼ å…¥ asr_languageï¼‰
    asr_language = load_key("asr.language")
    system_prompt = build_system_prompt(terms_with_meanings, asr_language)

    # 8. è°ƒç”¨ LLM Agent
    term_names = ", ".join(t['name'] for t in terms_with_meanings)
    user_task = f"è¯·æ£€æŸ¥è¿™ {len(sentences)} ä¸ªå¥å­ï¼Œæ‰¾å‡ºä»¥ä¸‹æœ¯è¯­çš„è¯­éŸ³è¯†åˆ«é”™è¯¯å¹¶çŸ«æ­£ï¼š{term_names}"

    result = ask_gpt_with_tools(
        system_prompt=system_prompt,
        prompt=user_task,
        tools=SENTENCES_TOOLS_DEFINITION,
        tool_executor=tool_executor,
        max_rounds=20,
        log_title="hotword_correction"
    )

    # 9. è¾“å‡ºç»Ÿè®¡
    if result and result.get("is_finish"):
        changes_count = result.get("changes_count", 0)
        if changes_count > 0:
            rprint(f"[green]âœ… çŸ«æ­£å®Œæˆ: {changes_count} å¤„ä¿®æ”¹[/green]")

            # æ˜¾ç¤ºä¿®æ”¹æ˜ç»†
            from collections import Counter
            stats = Counter(f"{c['old_text']} â†’ {c['new_text']}" for c in tool_executor.changes)
            for change, count in stats.most_common():
                rprint(f"  {change}: {count} å¤„")

            # ä¿å­˜çŸ«æ­£è®°å½•åˆ°æ–‡ä»¶ï¼ˆä¸æ§åˆ¶å°è¾“å‡ºä¸€è‡´ï¼‰
            _save_correction_log(tool_executor.changes, changes_count)
        else:
            rprint("[green]âœ… æœªå‘ç°éœ€è¦çŸ«æ­£çš„é”™è¯¯[/green]")
    else:
        rprint("[yellow]âš ï¸ LLM æœªæ­£å¸¸å®Œæˆï¼Œè¿”å›åŸå§‹å¥å­[/yellow]")

    # 10. åŒæ­¥ chunksï¼ˆåŸºäº corrections é‡å»ºï¼‰
    _rebuild_chunks_from_corrections(sentences)

    return sentences


def _save_correction_log(changes, changes_count):
    """ä¿å­˜çŸ«æ­£æ—¥å¿—åˆ° output/log/hotword_correct.txt"""
    from collections import Counter

    log_path = "output/log/hotword_correct.txt"

    with open(log_path, 'w', encoding='utf-8') as f:
        if changes_count > 0:
            f.write(f"âœ… çŸ«æ­£å®Œæˆ: {changes_count} å¤„ä¿®æ”¹\n")

            # æ„å»ºç»Ÿè®¡ä¿¡æ¯
            stats = Counter(f"{c['old_text']} â†’ {c['new_text']}" for c in changes)

            # åˆ†ç¦»æœ‰/æ— è§†è§‰è¾…åŠ©çš„çŸ«æ­£
            vision_assisted_changes = [c for c in changes if c.get('vision_assisted')]
            normal_changes = [c for c in changes if not c.get('vision_assisted')]

            # æ˜¾ç¤ºæ‰€æœ‰çŸ«æ­£ï¼ˆå¸¦è§†è§‰è¾…åŠ©æ ‡è®°ï¼‰
            f.write("\n")
            for change, count in stats.most_common():
                # æ£€æŸ¥è¿™ä¸ªçŸ«æ­£æ˜¯å¦ä½¿ç”¨äº†è§†è§‰è¾…åŠ©
                vision_item = next((c for c in vision_assisted_changes
                                   if c['old_text'] == change.split(' â†’ ')[0]
                                   and c['new_text'] == change.split(' â†’ ')[1]), None)
                if vision_item:
                    f.write(f"  {change}: {count} å¤„ ğŸ“·\n")
                else:
                    f.write(f"  {change}: {count} å¤„\n")

            # è§†è§‰è¾…åŠ©è¯¦æƒ…
            if vision_assisted_changes:
                f.write(f"\nğŸ“· è§†è§‰è¾…åŠ©è¯¦æƒ…:\n")
                vision_unique = {}
                for c in vision_assisted_changes:
                    key = f"{c['old_text']} â†’ {c['new_text']}"
                    if key not in vision_unique:
                        vision_unique[key] = c['vision_result']

                for change, result in vision_unique.items():
                    f.write(f"  [{change}]\n")
                    f.write(f"    â†’ è¯†åˆ«: {result}\n")
                f.write(f"\n  å¸§å›¾ç‰‡å·²ä¿å­˜è‡³: output/log/pic/\n")

        else:
            f.write("âœ… æœªå‘ç°éœ€è¦çŸ«æ­£çš„é”™è¯¯\n")

    rprint(f"[dim]ğŸ“ çŸ«æ­£æ—¥å¿—å·²ä¿å­˜: {log_path}[/dim]")
