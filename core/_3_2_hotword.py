"""
ASR çƒ­è¯çŸ«æ­£æ¨¡å— (Stage 2)

åœ¨ NLP åˆ†å¥åè¿›è¡Œï¼Œä½¿ç”¨ LLM Agent æ™ºèƒ½è¯†åˆ«å¹¶çŸ«æ­£ ASR é”™è¯¯

è¾“å…¥: List[Sentence] (æ¥è‡ª _3_1_split_nlp.py)
è¾“å‡º: List[Sentence] (çŸ«æ­£åï¼Œä¼ é€’ç»™ _3_3_split_meaning.py)
"""

import json
import os
import re
import unicodedata
from typing import List, Tuple, Dict, Any, Optional

from core.utils import rprint, load_key, timer
from core.utils.ask_gpt import ask_gpt_with_tools
from core.utils.models import Sentence, Chunk
from core.utils.sentence_tools import get_joiner


# ==================== å·¥å…·å®šä¹‰ ====================

SENTENCES_TOOLS_DEFINITION = [
    {
        "type": "function",
        "function": {
            "name": "read_sentences",
            "description": "è¯»å–å¥å­å†…å®¹ï¼Œè¿”å›å…¨éƒ¨æˆ–æŒ‡å®šç´¢å¼•èŒƒå›´",
            "parameters": {
                "type": "object",
                "properties": {
                    "start_idx": {"type": "integer", "description": "èµ·å§‹ç´¢å¼•ï¼ˆå¯é€‰ï¼Œä»0å¼€å§‹ï¼‰"},
                    "end_idx": {"type": "integer", "description": "ç»“æŸç´¢å¼•ï¼ˆå¯é€‰ï¼‰"}
                }
            }
        }
    },
    {
        "type": "function",
        "function": {
            "name": "get_context",
            "description": "è·å–æŸå¥å­çš„ä¸Šä¸‹æ–‡ï¼ˆå‰åå„Nå¥ï¼‰",
            "parameters": {
                "type": "object",
                "properties": {
                    "sentence_idx": {"type": "integer"},
                    "context_count": {"type": "integer", "default": 2}
                },
                "required": ["sentence_idx"]
            }
        }
    },
    {
        "type": "function",
        "function": {
            "name": "batch_replace",
            "description": "æ‰¹é‡æ‰§è¡Œå¤šå¤„æ›¿æ¢ã€‚åœ¨æ‰€æœ‰å¥å­ä¸­æŸ¥æ‰¾å¹¶æ›¿æ¢å¤šç§é”™è¯¯å½¢å¼",
            "parameters": {
                "type": "object",
                "properties": {
                    "replacements": {
                        "type": "array",
                        "items": {
                            "type": "object",
                            "properties": {
                                "old_text": {"type": "string", "description": "è¦æŸ¥æ‰¾çš„é”™è¯¯æ–‡æœ¬"},
                                "new_text": {"type": "string", "description": "æ›¿æ¢åçš„æ­£ç¡®æœ¯è¯­"}
                            },
                            "required": ["old_text", "new_text"]
                        }
                    }
                },
                "required": ["replacements"]
            }
        }
    },
    {
        "type": "function",
        "function": {
            "name": "finish",
            "description": "å®ŒæˆçŸ«æ­£ä»»åŠ¡",
            "parameters": {
                "type": "object",
                "properties": {
                    "summary": {"type": "string", "description": "ä¿®æ”¹æ€»ç»“"}
                },
                "required": ["summary"]
            }
        }
    }
]


# ==================== System Prompt ====================

def build_system_prompt(terms_with_meanings: List[dict]) -> str:
    """æ„å»º System Promptï¼ˆå®Œå…¨å¤ç”¨ agent_correct.pyï¼‰"""
    terms_info = "\n".join([
        f"  - {t['name']}" + (f": {t['meaning']}" if t['meaning'] else "")
        for t in terms_with_meanings
    ])

    return f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ ASRï¼ˆè‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼‰æœ¯è¯­çŸ«æ­£ä¸“å®¶ã€‚

## ä»»åŠ¡
çŸ«æ­£æ–‡æœ¬ä¸­è¢«è¯­éŸ³è¯†åˆ«é”™è¯¯çš„ä¸“ä¸šæœ¯è¯­ã€‚

## æœ¯è¯­åˆ—è¡¨åŠå«ä¹‰
{terms_info}

## âš ï¸ é‡è¦é™åˆ¶ï¼ˆå¿…é¡»éµå®ˆï¼‰
**åªèƒ½çŸ«æ­£ä¸Šè¿°æœ¯è¯­åˆ—è¡¨ä¸­çš„æœ¯è¯­**ï¼Œä¸è¦çŸ«æ­£å…¶ä»–å†…å®¹ã€‚
- å¦‚æœæ–‡æœ¬ä¸­æ²¡æœ‰ä¸Šè¿°æœ¯è¯­çš„é”™è¯¯å½¢å¼ï¼Œç›´æ¥è°ƒç”¨ finish
- ä¸è¦è‡ªåˆ›æœ¯è¯­æˆ–çŸ«æ­£ä¸åœ¨åˆ—è¡¨ä¸­çš„å†…å®¹
- åªå¤„ç†æŒ‡å®šçš„ {len(terms_with_meanings)} ä¸ªæœ¯è¯­
- **å¤æ•°å½¢å¼ä¹Ÿå…è®¸**ï¼šæ ¹æ®ä¸Šä¸‹æ–‡åˆ¤æ–­ï¼ˆå¦‚ APIs, URLsï¼‰

## å¯ç”¨å·¥å…·
1. read_sentences - è¯»å–å¥å­å†…å®¹
2. get_context - æŸ¥çœ‹æŸå¥çš„ä¸Šä¸‹æ–‡ï¼ˆå‰åå‡ å¥ï¼‰
3. batch_replace - æ‰¹é‡æ›¿æ¢ï¼ˆåœ¨æ‰€æœ‰å¥å­ä¸­æ‰§è¡Œå¤šä¸ªæ›¿æ¢è§„åˆ™ï¼‰
4. finish - å®ŒæˆçŸ«æ­£

## é«˜æ•ˆå·¥ä½œæµç¨‹ï¼ˆé‡è¦ï¼ï¼‰

1. **å…ˆå…¨å±€æ‰«æ**ï¼šç”¨ read_sentences æŸ¥çœ‹å¥å­å†…å®¹
2. **è®°å½•æ‰€æœ‰ç–‘ä¼¼é”™è¯¯**ï¼šåœ¨å¿ƒä¸­/è‰ç¨¿ä¸­è®°å½•æ‰€æœ‰å‘ç°çš„é”™è¯¯ä½ç½®å’Œå½¢å¼
3. **æ‰¹é‡å¤„ç†**ï¼šé˜…è¯»å®Œæˆåï¼Œä½¿ç”¨ batch_replace ä¸€æ¬¡æ€§å®Œæˆæ‰€æœ‰æ›¿æ¢
4. **æœ€åè°ƒç”¨ finish**

### æ‰¹é‡æ“ä½œç¤ºä¾‹

**æ¨èæ–¹å¼ï¼šä½¿ç”¨ batch_replace**
å¦‚æœå‘ç°å¤šç§é”™è¯¯æ¨¡å¼ï¼Œç”¨ä¸€æ¬¡ batch_replace å®Œæˆæ‰€æœ‰æ›¿æ¢ï¼š
```
batch_replace(replacements=[
  {{"old_text": "L L M", "new_text": "LLM"}},
  {{"old_text": "j son", "new_text": "JSON"}},
  {{"old_text": "A P I", "new_text": "API"}}
])
```

### ä½¿ç”¨ batch_replace çš„å…³é”®åŸåˆ™ï¼ˆé‡è¦ï¼ï¼‰

**ä¸€æ¬¡æäº¤æ‰€æœ‰è§„åˆ™ï¼ŒåŒ…æ‹¬å¤æ•°å½¢å¼**ï¼š
- å¦‚æœæœ¯è¯­åœ¨æ–‡æœ¬ä¸­æœ‰å¤æ•°å‡ºç°ï¼Œå¿…é¡»ä¸ºå•æ•°å’Œå¤æ•°å„å†™ä¸€æ¡è§„åˆ™
- ä¸è¦å…ˆæ›¿æ¢å†å›å¤´ä¿®å¤å¤æ•°ï¼Œä¸€æ¬¡åˆ°ä½

ç¤ºä¾‹ï¼šå‡è®¾æœ¯è¯­æ˜¯ "ABC"ï¼Œæ–‡æœ¬ä¸­å‘ç°é”™è¯¯å½¢å¼ "A B C" å’Œ "A B Cs"
```
batch_replace(replacements=[
  {{"old_text": "A B C", "new_text": "ABC"}},
  {{"old_text": "A B Cs", "new_text": "ABCs"}}
])
```

é”™è¯¯åšæ³•ï¼š
- åªæäº¤ {{"old_text": "A B Cs", "new_text": "ABC"}}ï¼ˆä¸¢å¤±å¤æ•°ï¼‰
- å…ˆæäº¤å•æ•°è§„åˆ™ï¼Œåç»­å†è¡¥å……å¤æ•°è§„åˆ™ï¼ˆæµªè´¹è½®æ¬¡ï¼‰

## é‡è¦åŸåˆ™

### å¤æ•°å½¢å¼
- æ ¹æ®ä¸Šä¸‹æ–‡åˆ¤æ–­æ˜¯å¦éœ€è¦å¤æ•°å½¢å¼
- å¦‚æœè¯­å¢ƒæ˜¯å¤æ•°ï¼ˆthese/those/all/multiple/several ç­‰æ ‡è®°ï¼‰ï¼Œæ›¿æ¢æ—¶ä½¿ç”¨å¤æ•°å½¢å¼

ç¤ºä¾‹ï¼š
- "these L L M models" â†’ æ›¿æ¢ä¸º "these LLM models"ï¼ˆå¤æ•°ï¼‰
- "a L L M model" â†’ æ›¿æ¢ä¸º "a LLM model"ï¼ˆå•æ•°ï¼‰

### è°¨æ…ä¿®æ­£
- åªä¿®æ”¹æ˜ç¡®æ˜¯ ASR è¯¯è¯†åˆ«çš„æƒ…å†µ
- å¦‚æœæŸä¸ªè¯åœ¨å½“å‰ä¸Šä¸‹æ–‡ä¸­æ˜¯åˆç†çš„ï¼Œä¸è¦ä¿®æ”¹

ç¤ºä¾‹ï¼š
- "I can see the point" â†’ "see" æ˜¯æ­£ç¡®çš„ï¼Œä¸è¦æ”¹æˆ "C" æˆ– "sea"
- "We use API calls" â†’ "API" æ˜¯æ­£ç¡®çš„ï¼Œä¸éœ€è¦ä¿®æ”¹

## å¸¸è§ ASR é”™è¯¯æ¨¡å¼å‚è€ƒ
- ç©ºæ ¼æ’å…¥ï¼šå­—æ¯é—´è¢«æ’å…¥ç©ºæ ¼ï¼ˆL L M â†’ LLMï¼‰
- å¤§å°å†™é”™è¯¯ï¼šé¦–å­—æ¯æœªå¤§å†™æˆ–å…¨å°å†™ï¼ˆjson â†’ JSONï¼‰
- åŒéŸ³è¿‘éŸ³ï¼šå‘éŸ³ç›¸ä¼¼çš„é”™è¯¯æ›¿æ¢

è¯·å¼€å§‹å·¥ä½œï¼Œè®°ä½ï¼šæ‰¹é‡å¤„ç†ï¼Œä¸è¦é€ä¸ªå¤„ç†ã€‚
"""


def _parse_terms(terms_config: List[str]) -> List[dict]:
    """è§£ææœ¯è¯­åˆ—è¡¨ï¼Œæ”¯æŒ 'æœ¯è¯­ : å«ä¹‰' æ ¼å¼"""
    parsed = []
    for term in terms_config:
        if ' : ' in term:
            parts = term.split(' : ', 1)
        elif ': ' in term:
            parts = term.split(': ', 1)
        elif 'ï¼š' in term:
            parts = term.split('ï¼š', 1)
        else:
            parsed.append({'name': term.strip(), 'meaning': None})
            continue

        parsed.append({
            'name': parts[0].strip(),
            'meaning': parts[1].strip() if len(parts) > 1 else None
        })
    return parsed


class SentenceToolExecutor:
    """Sentence å¯¹è±¡çš„å·¥å…·æ‰§è¡Œå™¨"""

    def __init__(self, sentences: List[Sentence]):
        self.sentences = sentences
        self.changes: List[Dict] = []

    def read_sentences(self, start_idx: int = None, end_idx: int = None) -> str:
        """è¯»å–å¥å­å†…å®¹ï¼Œè¿”å›å…¨éƒ¨æˆ–æŒ‡å®šç´¢å¼•èŒƒå›´"""
        total = len(self.sentences)

        if start_idx is None:
            start_idx = 0
        if end_idx is None:
            end_idx = total

        # é™åˆ¶æœ€å¤šæ˜¾ç¤º 100 å¥
        MAX_DISPLAY = 100
        truncated = False
        actual_end = end_idx

        if actual_end - start_idx > MAX_DISPLAY:
            actual_end = start_idx + MAX_DISPLAY
            truncated = True

        result = []
        for i in range(start_idx, min(actual_end, total)):
            result.append(f"ç¬¬{i}å¥: {self.sentences[i].text}")

        output = "\n".join(result)

        if truncated:
            remaining = total - actual_end
            output += f"\n\n[å…± {total} å¥ï¼Œå·²æ˜¾ç¤ºåˆ°ç¬¬ {actual_end} å¥ï¼Œå‰©ä½™ {remaining} å¥ã€‚"
            output += f"è¯·è°ƒç”¨ read_sentences(start_idx={actual_end}) ç»§ç»­æŸ¥çœ‹]"

        return output

    def get_context(self, sentence_idx: int, context_count: int = 2) -> str:
        """è·å–æŸå¥å­çš„ä¸Šä¸‹æ–‡"""
        start = max(0, sentence_idx - context_count)
        end = min(len(self.sentences), sentence_idx + context_count + 1)

        result = []
        for i in range(start, end):
            marker = ">>> " if i == sentence_idx else "    "
            result.append(f"{marker}ç¬¬{i}å¥: {self.sentences[i].text}")

        return "\n".join(result)

    def batch_replace(self, replacements: list) -> str:
        """æ‰¹é‡æ›¿æ¢æœ¯è¯­ï¼ˆåªä¿®æ”¹æ–‡æœ¬ï¼Œä¸åŠ¨ chunks å’Œæ—¶é—´æˆ³ï¼‰"""
        results = []
        total_changes = 0

        for replacement in replacements:
            old_text = replacement.get("old_text", "")
            new_text = replacement.get("new_text", "")

            if not old_text:
                results.append({"error": "old_text ä¸èƒ½ä¸ºç©º"})
                continue

            # åœ¨æ‰€æœ‰å¥å­ä¸­æŸ¥æ‰¾å¹¶æ›¿æ¢
            for sent_idx, sentence in enumerate(self.sentences):
                changes_count = self._replace_in_sentence(
                    sentence, sent_idx, old_text, new_text
                )
                total_changes += changes_count

            results.append({
                "old_text": old_text,
                "new_text": new_text,
                "count": total_changes
            })

        return json.dumps({
            "success": True,
            "total_changes": total_changes,
            "details": results
        }, ensure_ascii=False)

    def _replace_in_sentence(
        self, sentence: Sentence, sent_idx: int,
        old_text: str, new_text: str
    ) -> int:
        """
        åœ¨å•ä¸ªå¥å­ä¸­æŸ¥æ‰¾å¹¶æ›¿æ¢ï¼ˆåªä¿®æ”¹æ–‡æœ¬ï¼‰

        ä½¿ç”¨å¤šè¯­è¨€å•è¯è¾¹ç•Œæ£€æŸ¥ï¼Œæ”¯æŒï¼šä¸­ã€æ—¥ã€éŸ©ã€è‹±ã€å¾·ã€ä¿„ç­‰æ‰€æœ‰è¯­è¨€
        """
        changes_count = 0

        # åœ¨åŸå§‹æ–‡æœ¬ä¸­æŸ¥æ‰¾æ‰€æœ‰åŒ¹é…ä½ç½®
        for match in re.finditer(re.escape(old_text), sentence.text):
            start, end = match.span()

            # æ£€æŸ¥æ˜¯å¦ä¸ºå•è¯è¾¹ç•Œ
            if self._is_word_boundary(sentence.text, start, end - start):
                # æ‰§è¡Œæ›¿æ¢
                sentence.text = (
                    sentence.text[:start] +
                    new_text +
                    sentence.text[end:]
                )
                changes_count += 1

                # è®°å½•ä¿®æ”¹
                self.changes.append({
                    "sentence_idx": sent_idx,
                    "old_text": old_text,
                    "new_text": new_text
                })

        return changes_count

    def _is_word_boundary(self, text: str, pos: int, length: int) -> bool:
        """
        æ£€æŸ¥æŒ‡å®šä½ç½®æ˜¯å¦ä¸ºå•è¯è¾¹ç•Œï¼ˆå¤šè¯­è¨€é€šç”¨ï¼‰

        ç­–ç•¥ï¼šæ£€æŸ¥åŒ¹é…ä½ç½®å‰åæ˜¯å¦ä¸ºå­—æ¯ï¼ˆL* ç±»åˆ«ï¼‰
             å¦‚æœå‰åéƒ½ä¸æ˜¯å­—æ¯ï¼Œåˆ™ä¸ºå•è¯è¾¹ç•Œ

        æ”¯æŒï¼šä¸­ã€æ—¥ã€éŸ©ã€è‹±ã€å¾·ã€ä¿„ã€æ³•ã€æ„ã€è¥¿ç­‰æ‰€æœ‰è¯­è¨€

        Args:
            text: å®Œæ•´æ–‡æœ¬
            pos: åŒ¹é…èµ·å§‹ä½ç½®
            length: åŒ¹é…é•¿åº¦

        Returns:
            bool: True è¡¨ç¤ºæ˜¯å•è¯è¾¹ç•Œï¼ŒFalse è¡¨ç¤ºä¸æ˜¯
        """
        # æ£€æŸ¥å‰ä¸€ä¸ªå­—ç¬¦
        if pos > 0:
            prev_char = text[pos - 1]
            if unicodedata.category(prev_char).startswith('L'):  # å‰é¢æ˜¯å­—æ¯
                return False

        # æ£€æŸ¥åä¸€ä¸ªå­—ç¬¦
        end_pos = pos + length
        if end_pos < len(text):
            next_char = text[end_pos]
            if unicodedata.category(next_char).startswith('L'):  # åé¢æ˜¯å­—æ¯
                return False

        return True  # å‰åéƒ½ä¸æ˜¯å­—æ¯ï¼Œæ˜¯å•è¯è¾¹ç•Œ

    def finish(self, summary: str) -> str:
        """å®ŒæˆçŸ«æ­£ä»»åŠ¡"""
        return json.dumps({
            "changes_count": len(self.changes),
            "summary": summary,
            "is_finish": True
        }, ensure_ascii=False)


# ==================== ä¸»å…¥å£å‡½æ•° ====================

def _sync_chunks_to_text(sentences: List[Sentence], original_texts: List[str]) -> None:
    """
    åŒæ­¥ chunks åˆ°çŸ«æ­£åçš„æ–‡æœ¬ï¼ˆä½¿ç”¨ difflib å¯¹æ¯”ï¼‰

    ç­–ç•¥ï¼š
    - ä½¿ç”¨ difflib å¯¹æ¯”çŸ«æ­£å‰åçš„æ–‡æœ¬
    - æœªä¿®æ”¹éƒ¨åˆ† â†’ ä¿æŒåŸ chunks
    - ä¿®æ”¹éƒ¨åˆ† â†’ é‡å»º chunksï¼ˆåˆ‡åˆ† + é‡æ–°åˆ†é…æ—¶é—´æˆ³ï¼‰

    Args:
        sentences: Sentence å¯¹è±¡åˆ—è¡¨ï¼ˆä¼šå°±åœ°ä¿®æ”¹ï¼‰
        original_texts: çŸ«æ­£å‰çš„åŸå§‹æ–‡æœ¬åˆ—è¡¨
    """
    from difflib import SequenceMatcher

    for sent_idx, sentence in enumerate(sentences):
        original_text = original_texts[sent_idx]

        # æ–‡æœ¬æœªä¿®æ”¹ï¼Œä¿æŒåŸ chunks
        if original_text == sentence.text:
            continue

        # ä½¿ç”¨ difflib å¯¹æ¯”
        matcher = SequenceMatcher(None, original_text, sentence.text)

        new_chunks = []
        current_orig_chunk_idx = 0
        orig_chunks = sentence.chunks
        asr_language = load_key("asr.language")
        joiner = get_joiner(asr_language)

        for tag, i1, i2, j1, j2 in matcher.get_opcodes():
            if tag == 'equal':
                # æœªä¿®æ”¹éƒ¨åˆ†ï¼šä¿æŒåŸ chunks
                # é€šè¿‡å­—ç¬¦ä½ç½®æ‰¾åˆ°å¯¹åº”çš„ chunks
                char_pos = 0
                for chunk in orig_chunks:
                    chunk_end = char_pos + len(chunk.text)
                    # æ£€æŸ¥ chunk æ˜¯å¦å®Œå…¨åœ¨ [i1, i2) èŒƒå›´å†…
                    if char_pos >= i1 and chunk_end <= i2:
                        new_chunks.append(chunk)
                    elif chunk_end > i2:
                        # è¶…å‡ºå½“å‰ equal èŒƒå›´ï¼Œåœæ­¢å¤„ç†
                        break
                    char_pos = chunk_end

            elif tag == 'replace':
                # ä¿®æ”¹éƒ¨åˆ†ï¼šé‡å»º chunks
                # è·å–ä¿®æ”¹éƒ¨åˆ†çš„åŸå§‹æ—¶é—´èŒƒå›´
                orig_start_time = None
                orig_end_time = None

                # æ‰¾åˆ°åŸå§‹æ–‡æœ¬ä¸­å¯¹åº”ä½ç½®çš„æ—¶é—´æˆ³
                char_pos = 0
                for chunk in orig_chunks:
                    chunk_text = chunk.text
                    chunk_end = char_pos + len(chunk_text)

                    if char_pos <= i1 and chunk_end >= i2:
                        # chunk å®Œå…¨åœ¨ä¿®æ”¹èŒƒå›´å†…
                        if orig_start_time is None:
                            orig_start_time = chunk.start
                        orig_end_time = chunk.end
                    elif char_pos <= i1 < chunk_end:
                        # chunk è·¨è¶Šä¿®æ”¹å¼€å§‹è¾¹ç•Œ
                        if orig_start_time is None:
                            orig_start_time = chunk.start
                        orig_end_time = chunk.end
                    elif i1 < char_pos <= i2:
                        # chunk åœ¨ä¿®æ”¹èŒƒå›´å†…
                        orig_end_time = chunk.end

                    char_pos += len(chunk_text)

                # å¦‚æœæ‰¾ä¸åˆ°æ—¶é—´æˆ³ï¼Œä½¿ç”¨å¥å­çš„è¾¹ç•Œ
                if orig_start_time is None:
                    orig_start_time = sentence.start
                if orig_end_time is None:
                    orig_end_time = sentence.end

                # åˆ‡åˆ†æ–°æ–‡æœ¬
                new_text = sentence.text[j1:j2]
                rebuilt_chunks = _split_text_into_chunks(
                    new_text, orig_start_time, orig_end_time,
                    asr_language, joiner,
                    speaker_id=orig_chunks[0].speaker_id if orig_chunks else None
                )
                new_chunks.extend(rebuilt_chunks)

            elif tag == 'delete':
                # åˆ é™¤éƒ¨åˆ†ï¼šè·³è¿‡
                pass

            elif tag == 'insert':
                # æ’å…¥éƒ¨åˆ†ï¼šä½¿ç”¨åŸä½ç½®çš„æ—¶é—´æˆ³
                # åœ¨ j1-j2 ä½ç½®æ’å…¥ï¼Œä½¿ç”¨é™„è¿‘çš„æ—¶é—´æˆ³
                new_chunks.extend(_split_text_into_chunks(
                    sentence.text[j1:j2],
                    sentence.start, sentence.end,
                    asr_language, joiner,
                    speaker_id=orig_chunks[0].speaker_id if orig_chunks else None
                ))

        # æ›´æ–° sentence çš„ chunks
        sentence.chunks = new_chunks

        # ç¡®ä¿ Sentence.start/end ä¸ chunks ä¸€è‡´
        if new_chunks:
            sentence.start = new_chunks[0].start
            sentence.end = new_chunks[-1].end


def _split_text_into_chunks(
    text: str, start_time: float, end_time: float,
    asr_language: str, joiner: str,
    speaker_id: Optional[str]
) -> List[Chunk]:
    """
    å°†æ–‡æœ¬åˆ‡åˆ†ä¸º chunks å¹¶åˆ†é…æ—¶é—´æˆ³

    ç­–ç•¥ï¼š
    - ç©ºæ ¼åˆ†éš”è¯­è¨€ï¼šæŒ‰ç©ºæ ¼åˆ‡åˆ†
    - CJK è¯­è¨€ï¼šæŒ‰å­—ç¬¦åˆ‡åˆ†
    - æ—¶é—´æˆ³ï¼šå¹³å‡åˆ†é…ï¼Œè¾¹ç•Œä¸å˜
    """
    # æ ¹æ®è¯­è¨€é€‰æ‹©åˆ‡åˆ†æ–¹å¼
    if asr_language in ['zh', 'ja', 'ko']:
        # CJK è¯­è¨€ï¼šæŒ‰å­—ç¬¦åˆ‡åˆ†
        parts = list(text)
    else:
        # ç©ºæ ¼åˆ†éš”è¯­è¨€ï¼šæŒ‰ç©ºæ ¼åˆ‡åˆ†
        parts = text.split()
        if joiner:
            # å¦‚æœæœ‰ç©ºæ ¼è¿æ¥ç¬¦ï¼Œé‡æ–°æ·»åŠ ä»¥ä¾¿æ­£ç¡®åˆ†é…æ—¶é—´
            text_with_joiner = joiner.join(parts)
            parts = text_with_joiner.split(joiner) if joiner else parts

    if not parts:
        return []

    # åˆ†é…æ—¶é—´æˆ³
    total_duration = end_time - start_time
    chunk_duration = total_duration / len(parts)

    chunks = []
    current_time = start_time

    for i, part in enumerate(parts):
        chunk_start = current_time
        chunk_end = current_time + chunk_duration

        # æœ€åä¸€ä¸ª chunk ä½¿ç”¨ end_timeï¼ˆé¿å…æµ®ç‚¹è¯¯å·®ï¼‰
        if i == len(parts) - 1:
            chunk_end = end_time

        chunks.append(Chunk(
            text=part,
            start=chunk_start,
            end=chunk_end,
            speaker_id=speaker_id,
            index=i
        ))

        current_time = chunk_end

    return chunks


def _save_checkpoint(sentences: List[Sentence], path: str) -> None:
    """ä¿å­˜çŸ«æ­£åçš„å¥å­åˆ°æ–­ç‚¹æ–‡ä»¶"""
    os.makedirs(os.path.dirname(path), exist_ok=True)
    with open(path, 'w', encoding='utf-8') as f:
        for sentence in sentences:
            f.write(sentence.text + '\n')
    rprint(f"[dim]ğŸ’¾ æ–­ç‚¹å·²ä¿å­˜: {path}[/dim]")


def _load_checkpoint(sentences: List[Sentence], path: str) -> List[Sentence]:
    """ä»æ–­ç‚¹æ–‡ä»¶åŠ è½½çŸ«æ­£åçš„å¥å­"""
    with open(path, 'r', encoding='utf-8') as f:
        corrected_texts = [line.rstrip('\n') for line in f]

    # éªŒè¯æ•°é‡ä¸€è‡´
    if len(corrected_texts) != len(sentences):
        rprint(f"[yellow]âš ï¸ æ–­ç‚¹æ–‡ä»¶å¥å­æ•°é‡ä¸åŒ¹é… ({len(corrected_texts)} vs {len(sentences)})ï¼Œé‡æ–°æ‰§è¡ŒçŸ«æ­£[/yellow]")
        os.remove(path)
        return sentences

    # ä¿å­˜åŸå§‹æ–‡æœ¬ï¼ˆç”¨äº chunk åŒæ­¥ï¼‰
    original_texts = [s.text for s in sentences]

    # æ›´æ–°å¥å­çš„æ–‡æœ¬
    for sent, corrected_text in zip(sentences, corrected_texts):
        sent.text = corrected_text

    # åŒæ­¥ chunksï¼ˆçŸ«æ­£å‰ -> çŸ«æ­£åï¼‰
    _sync_chunks_to_text(sentences, original_texts)

    return sentences


@timer("ASR æœ¯è¯­çŸ«æ­£")
def correct_terms_in_sentences(sentences: List[Sentence]) -> List[Sentence]:
    """
    ä¸»å‡½æ•°ï¼šå¯¹å¥å­åˆ—è¡¨è¿›è¡Œæœ¯è¯­çŸ«æ­£

    Args:
        sentences: NLP åˆ†å¥åçš„ Sentence å¯¹è±¡åˆ—è¡¨

    Returns:
        List[Sentence]: çŸ«æ­£åçš„ Sentence å¯¹è±¡åˆ—è¡¨
    """
    # 1. æ£€æŸ¥å¼€å…³
    enabled = load_key("asr_term_correction.enabled")
    if not enabled:
        rprint("[yellow]â­ï¸ æœ¯è¯­çŸ«æ­£å·²ç¦ç”¨ï¼Œè·³è¿‡[/yellow]")
        return sentences

    # 2. åŠ è½½æœ¯è¯­é…ç½®
    terms_config = load_key("asr_term_correction.terms")
    if not terms_config:
        rprint("[yellow]â­ï¸ æœªé…ç½®æœ¯è¯­ï¼Œè·³è¿‡çŸ«æ­£[/yellow]")
        return sentences

    # 3. æ£€æŸ¥æ–­ç‚¹æ–‡ä»¶
    checkpoint_path = "output/log/hotword_correct.txt"
    if os.path.exists(checkpoint_path):
        rprint(f"[blue]ğŸ“‚ å‘ç°æ–­ç‚¹æ–‡ä»¶ï¼Œç›´æ¥åŠ è½½çŸ«æ­£ç»“æœ[/blue]")
        return _load_checkpoint(sentences, checkpoint_path)

    # 4. è§£ææœ¯è¯­åˆ—è¡¨
    terms_with_meanings = _parse_terms(terms_config)
    rprint(f"[blue]ğŸ” å¼€å§‹æœ¯è¯­çŸ«æ­£ï¼Œå…± {len(terms_with_meanings)} ä¸ªæœ¯è¯­[/blue]")
    for t in terms_with_meanings:
        rprint(f"  - {t['name']}" + (f": {t['meaning']}" if t['meaning'] else ""))

    # 5. åˆ›å»ºå·¥å…·æ‰§è¡Œå™¨
    tool_executor = SentenceToolExecutor(sentences)

    # 6. ä¿å­˜åŸå§‹æ–‡æœ¬ï¼ˆç”¨äºåç»­ chunk åŒæ­¥ï¼‰
    original_texts = [s.text for s in sentences]

    # 7. æ„å»ºç³»ç»Ÿæç¤ºè¯
    system_prompt = build_system_prompt(terms_with_meanings)

    # 8. è°ƒç”¨ LLM Agent
    user_task = f"è¯·çŸ«æ­£è¿™ {len(sentences)} ä¸ªå¥å­ä¸­çš„æœ¯è¯­é”™è¯¯"

    result = ask_gpt_with_tools(
        system_prompt=system_prompt,
        prompt=user_task,
        tools=SENTENCES_TOOLS_DEFINITION,
        tool_executor=tool_executor,
        max_rounds=20,
        log_title="hotword_correction"
    )

    # 9. è¾“å‡ºç»Ÿè®¡
    if result and result.get("is_finish"):
        changes_count = result.get("changes_count", 0)
        if changes_count > 0:
            rprint(f"[green]âœ… çŸ«æ­£å®Œæˆ: {changes_count} å¤„ä¿®æ”¹[/green]")

            # æ˜¾ç¤ºä¿®æ”¹æ˜ç»†
            from collections import Counter
            stats = Counter(f"{c['old_text']} â†’ {c['new_text']}" for c in tool_executor.changes)
            for change, count in stats.most_common():
                rprint(f"  {change}: {count} å¤„")
        else:
            rprint("[green]âœ… æœªå‘ç°éœ€è¦çŸ«æ­£çš„é”™è¯¯[/green]")
    else:
        rprint("[yellow]âš ï¸ LLM æœªæ­£å¸¸å®Œæˆï¼Œè¿”å›åŸå§‹å¥å­[/yellow]")

    # 10. åŒæ­¥ chunksï¼ˆä½¿ç”¨çŸ«æ­£å‰çš„æ–‡æœ¬è¿›è¡Œå¯¹æ¯”ï¼‰
    _sync_chunks_to_text(sentences, original_texts)

    # 11. ä¿å­˜æ–­ç‚¹æ–‡ä»¶
    _save_checkpoint(sentences, checkpoint_path)

    return sentences
