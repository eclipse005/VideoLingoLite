import pandas as pd
from typing import List
import math
import concurrent.futures

from core.utils import *
from core.utils.models import *
from rich.console import Console
from rich.progress import Progress, SpinnerColumn, TextColumn, BarColumn

console = Console()


def align_translation_with_source(src_text: str, tr_text: str, split_src: str) -> List[str]:
    """
    å¯¹é½è¯‘æ–‡ä¸åŸæ–‡æ‹†åˆ†ï¼ˆä½¿ç”¨ LLMï¼‰

    Args:
        src_text: åŸæ–‡
        tr_text: è¯‘æ–‡
        split_src: å¸¦æœ‰ [br] æ ‡è®°çš„æ‹†åˆ†ååŸæ–‡

    Returns:
        å¯¹é½æ‹†åˆ†åçš„è¯‘æ–‡åˆ—è¡¨
    """
    from core.prompts import get_align_prompt

    align_prompt = get_align_prompt(src_text, tr_text, split_src)

    def valid_align(response_data):
        if not isinstance(response_data, dict):
            return {"status": "error", "message": "Response must be a dictionary"}
        if 'align' not in response_data:
            return {"status": "error", "message": "Missing required field: 'align'"}
        if not isinstance(response_data['align'], list):
            return {"status": "error", "message": "Field 'align' must be a list"}
        if len(response_data['align']) < 2:
            return {"status": "error", "message": "Field 'align' must contain at least 2 items"}
        for i, item in enumerate(response_data['align']):
            if not isinstance(item, dict):
                return {"status": "error", "message": f"align[{i}] must be a dictionary"}
            if f'target_part_{i+1}' not in item:
                return {"status": "error", "message": f"Missing required field: 'target_part_{i+1}' in align[{i}]"}
        return {"status": "success", "message": "Align validation completed"}

    parsed = ask_gpt(align_prompt, resp_type='json', valid_def=valid_align, log_title='align_subs')
    align_data = parsed['align']

    # æå–å¯¹é½åçš„è¯‘æ–‡
    tr_parts = [item[f'target_part_{i+1}'].strip() for i, item in enumerate(align_data)]
    return tr_parts


def process_single_sentence_split(sent: Sentence, num_parts: int, index: int, asr_language: str) -> List[Sentence]:
    """
    å¤„ç†å•ä¸ªå¥å­çš„æ‹†åˆ†å¯¹é½ï¼ˆç”¨äºå¹¶å‘è°ƒç”¨ï¼‰

    Args:
        sent: è¦æ‹†åˆ†çš„ Sentence å¯¹è±¡
        num_parts: éœ€è¦æ‹†åˆ†æˆå‡ ä»½
        index: å¥å­ç´¢å¼•
        asr_language: ASR è¯­è¨€

    Returns:
        æ‹†åˆ†å¯¹é½åçš„ Sentence å¯¹è±¡åˆ—è¡¨
    """
    from core._3_2_split_meaning import find_br_positions_in_original
    from core.utils.sentence_tools import clean_word

    # ä½¿ç”¨ LLM æ‹†åˆ†åŸæ–‡
    split_src = split_sentence(sent.text, num_parts=num_parts, index=index).strip()

    if '[br]' not in split_src:
        # LLM è®¤ä¸ºä¸éœ€è¦æ‹†åˆ†å¯¹é½
        return [sent]

    # è°ƒç”¨ LLM å¯¹é½è¯‘æ–‡
    tr_parts = align_translation_with_source(sent.text, sent.translation, split_src)

    # éœ€è¦æ‹†åˆ†å¯¹é½ï¼šä½¿ç”¨ difflib åŒ¹é…æ‰¾åˆ° [br] ä½ç½®ï¼Œæ‹†åˆ† chunks
    br_positions = find_br_positions_in_original(split_src, sent.text)

    if not br_positions:
        # æ²¡æœ‰æ‰¾åˆ°æ‹†åˆ†ç‚¹ï¼Œä¿æŒåŸæ ·
        return [sent]

    # æ„å»ºå­—ç¬¦åˆ° Chunk çš„æ˜ å°„
    char_to_chunk = []
    for chunk_idx, chunk in enumerate(sent.chunks):
        cleaned_chunk_text = clean_word(chunk.text)
        char_to_chunk.extend([chunk_idx] * len(cleaned_chunk_text))

    # ç¡®å®š Chunk æ‹†åˆ†ç‚¹
    split_points = [0]
    for br_pos in br_positions:
        if br_pos < len(char_to_chunk):
            chunk_idx = char_to_chunk[br_pos]
            if chunk_idx not in split_points:
                split_points.append(chunk_idx)
    split_points.append(len(sent.chunks))
    split_points = sorted(set(split_points))

    # è·å–è¯­è¨€è¿æ¥ç¬¦
    joiner = get_joiner(asr_language)

    # æ‹†åˆ† Chunksï¼Œåˆ›å»ºæ–°çš„ Sentence å¯¹è±¡
    new_sentences = []
    for j in range(min(len(split_points) - 1, len(tr_parts))):
        start_idx = split_points[j]
        end_idx = split_points[j + 1]

        if start_idx >= end_idx:
            continue

        sub_chunks = sent.chunks[start_idx:end_idx]
        sub_text = joiner.join(c.text for c in sub_chunks)

        # ä½¿ç”¨å¯¹é½åçš„è¯‘æ–‡
        sub_translation = tr_parts[j] if j < len(tr_parts) else ""

        new_sentence = Sentence(
            chunks=sub_chunks,
            text=sub_text,
            translation=sub_translation,
            start=sub_chunks[0].start,
            end=sub_chunks[-1].end,
            index=sent.index + j,
            is_split=True
        )
        new_sentences.append(new_sentence)

    return new_sentences


@timer("æ‹†åˆ†å¯¹é½")
def split_for_sub_main(sentences: List[Sentence]) -> List[Sentence]:
    """
    å­—å¹•æ‹†åˆ†å¯¹é½ä¸»å‡½æ•°ï¼Œå¤„ç† Sentence å¯¹è±¡

    Args:
        sentences: Sentence å¯¹è±¡åˆ—è¡¨

    Returns:
        List[Sentence]: åˆ‡åˆ†å¯¹é½åçš„ Sentence å¯¹è±¡åˆ—è¡¨
    """
    console.print(f"[cyan]ğŸ” å¼€å§‹æ‹†åˆ†å¯¹é½ï¼Œå…± {len(sentences)} ä¸ªå¥å­[/cyan]")

    # Get source and target language ISO codes
    asr_language = load_key("asr.language")
    target_lang_desc = load_key("target_language")
    target_language = TARGET_LANG_MAP.get(target_lang_desc, 'en')

    # Get soft limits for source and target languages
    origin_soft_limit = get_language_length_limit(asr_language, 'origin')
    translate_soft_limit = get_language_length_limit(target_language, 'translate')

    # å¤šè½®æ‹†åˆ†å¯¹é½
    for attempt in range(3):
        console.print(f"[dim]æ£€æŸ¥é•¿åº¦é™åˆ¶ (ç¬¬ {attempt + 1} è½®)...[/dim]")

        # æ‰¾å‡ºéœ€è¦æ‹†åˆ†å¯¹é½çš„å¥å­
        to_split = []
        for i, sent in enumerate(sentences):
            src_exceeds = check_length_exceeds(sent.text, origin_soft_limit, asr_language)
            tr_exceeds = check_length_exceeds(sent.translation, translate_soft_limit, target_language)
            if src_exceeds or tr_exceeds:
                to_split.append(i)

            # æ¯100ä¸ªå¥å­æ˜¾ç¤ºä¸€æ¬¡è¿›åº¦
            if (i + 1) % 100 == 0:
                console.print(f"[dim]å·²æ£€æŸ¥ {i + 1}/{len(sentences)} ä¸ªå¥å­...[/dim]")

        console.print(f"[dim]é•¿åº¦æ£€æŸ¥å®Œæˆï¼Œå‘ç° {len(to_split)} ä¸ªéœ€è¦æ‹†åˆ†çš„å¥å­[/dim]")

        if not to_split:
            if attempt > 0:
                console.print('[green]âœ… æ‰€æœ‰å¥å­å·²ç¬¦åˆé•¿åº¦é™åˆ¶[/green]')
            break

        if to_split:
            console.print(f'[yellow]âš ï¸ å‘ç° {len(to_split)} ä¸ªéœ€è¦æ‹†åˆ†çš„å¥å­[/yellow]')

        # å¤„ç†éœ€è¦æ‹†åˆ†å¯¹é½çš„å¥å­ï¼Œæ„å»ºæ–°åˆ—è¡¨
        new_sentences = [None] * len(sentences)
        total_to_split = len(to_split)

        # å…ˆå¡«å……ä¸éœ€è¦æ‹†åˆ†å¯¹é½çš„å¥å­
        for i, sent in enumerate(sentences):
            if i not in to_split:
                new_sentences[i] = [sent]

        with Progress(
            SpinnerColumn(),
            TextColumn("[progress.description]{task.description}"),
            BarColumn(),
            TextColumn("[progress.percentage]{task.percentage:>3.0f}%"),
            TextColumn("({task.completed}/{task.total})"),
            transient=False,
            console=console
        ) as progress:
            task = progress.add_task(f"[cyan]ç¬¬ {attempt + 1} è½®: æ‹†åˆ†å¯¹é½ {total_to_split} ä¸ªå¥å­...", total=total_to_split)

            with concurrent.futures.ThreadPoolExecutor(max_workers=load_key("max_workers")) as executor:
                # æäº¤æ‰€æœ‰éœ€è¦æ‹†åˆ†çš„ä»»åŠ¡
                futures = {}
                for i in to_split:
                    sent = sentences[i]
                    # è®¡ç®—éœ€è¦æ‹†åˆ†æˆå‡ ä»½
                    text_length = get_effective_length(sent.text, asr_language)
                    num_parts = max(2, math.ceil(text_length / origin_soft_limit))

                    # æäº¤ LLM æ‹†åˆ†å¯¹é½ä»»åŠ¡
                    future = executor.submit(
                        process_single_sentence_split,
                        sent,
                        num_parts,
                        i,
                        asr_language
                    )
                    futures[future] = i

                # æŒ‰å®Œæˆé¡ºåºæ”¶é›†æ‹†åˆ†å¯¹é½ç»“æœ
                for future in concurrent.futures.as_completed(futures.keys()):
                    index = futures[future]
                    split_result_list = future.result()
                    new_sentences[index] = split_result_list
                    progress.update(task, advance=1)

        # å±•å¹³æ‹†åˆ†å¯¹é½åçš„å¥å­åˆ—è¡¨
        sentences = [s for sublist in new_sentences for s in sublist]

    # ä¿å­˜ç»“æœåˆ° CSVï¼ˆå‘åå…¼å®¹ï¼‰
    split_src = [sent.text for sent in sentences]
    split_trans = [sent.translation for sent in sentences]
    pd.DataFrame({'Source': split_src, 'Translation': split_trans}).to_csv(_5_SPLIT_SUB, index=False, encoding='utf-8-sig')
    pd.DataFrame({'Source': split_src, 'Translation': split_trans}).to_csv(_5_REMERGED, index=False, encoding='utf-8-sig')

    console.print("[bold green]âœ… å­—å¹•æ‹†åˆ†å¯¹é½å®Œæˆï¼[/bold green]")
    return sentences


if __name__ == '__main__':
    # æµ‹è¯•éœ€è¦ä»å¤–éƒ¨æä¾› sentences
    print("This module requires Sentence objects as input.")
