import pandas as pd
from typing import List
import concurrent.futures
import math

from core.utils import *
from core.utils.models import *
from rich.panel import Panel
from rich.console import Console

console = Console()


def split_for_sub_main(sentences: List[Sentence]) -> List[Sentence]:
    """
    å­—å¹•åˆ‡åˆ†ä¸»å‡½æ•°ï¼Œå¤„ç† Sentence å¯¹è±¡

    Args:
        sentences: Sentence å¯¹è±¡åˆ—è¡¨

    Returns:
        List[Sentence]: åˆ‡åˆ†åçš„ Sentence å¯¹è±¡åˆ—è¡¨
    """
    console.print("[bold green]ğŸš€ Start splitting subtitles...[/]")

    # ğŸ“Š æ˜¾ç¤ºæ¥æ”¶åˆ°çš„ Sentence å¯¹è±¡ä¿¡æ¯
    console.print(f'[cyan]ğŸ“Š Received {len(sentences)} Sentence objects from Stage 3[/cyan]')
    has_translation = sum(1 for s in sentences if s.translation)
    console.print(f'[dim]Sentences with translation: {has_translation}/{len(sentences)}[/dim]')

    # Get source and target language ISO codes
    asr_language = load_key("asr.language")
    target_lang_desc = load_key("target_language")
    target_language = TARGET_LANG_MAP.get(target_lang_desc, 'en')

    # Get soft limits for source and target languages
    origin_soft_limit = get_language_length_limit(asr_language, 'origin')
    translate_soft_limit = get_language_length_limit(target_language, 'translate')

    # å¤šè½®åˆ‡å‰²
    for attempt in range(3):
        console.print(Panel(f"ğŸ”„ Split attempt {attempt + 1}", expand=False))

        # æ‰¾å‡ºéœ€è¦åˆ‡åˆ†çš„å¥å­
        to_split = []
        for i, sent in enumerate(sentences):
            src_exceeds = check_length_exceeds(sent.text, origin_soft_limit, asr_language)
            tr_exceeds = check_length_exceeds(sent.translation, translate_soft_limit, target_language)
            if src_exceeds or tr_exceeds:
                to_split.append(i)

        if not to_split:
            console.print("[green]âœ… All subtitles are within length limits![/green]")
            break

        # å¤„ç†éœ€è¦åˆ‡åˆ†çš„å¥å­ï¼Œæ„å»ºæ–°åˆ—è¡¨
        new_sentences = []
        for i, sent in enumerate(sentences):
            if i not in to_split:
                # ä¸éœ€è¦æ‹†åˆ†ï¼Œç›´æ¥æ·»åŠ 
                new_sentences.append(sent)
                continue

            # éœ€è¦æ‹†åˆ†
            # è®¡ç®—éœ€è¦æ‹†åˆ†æˆå‡ ä»½
            text_length = get_effective_length(sent.text, asr_language)
            num_parts = max(2, math.ceil(text_length / origin_soft_limit))

            # ä½¿ç”¨ LLM æ‹†åˆ†åŸæ–‡
            split_src = split_sentence(sent.text, num_parts=num_parts).strip()

            if '[br]' in split_src:
                # éœ€è¦æ‹†åˆ†ï¼šä½¿ç”¨ difflib åŒ¹é…æ‰¾åˆ° [br] ä½ç½®ï¼Œæ‹†åˆ† chunks
                from core._3_2_split_meaning import find_br_positions_in_original
                from core.utils.sentence_tools import clean_word

                br_positions = find_br_positions_in_original(split_src, sent.text)

                if br_positions:
                    # æ„å»ºå­—ç¬¦åˆ° Chunk çš„æ˜ å°„ï¼ˆä½¿ç”¨æ¸…æ´—åçš„æ–‡æœ¬ï¼‰
                    # æ³¨æ„ï¼šè¿™é‡Œä¸éœ€è¦æ·»åŠ ç©ºæ ¼ï¼Œå› ä¸º find_br_positions_in_original è¿”å›çš„ä½ç½®
                    # æ˜¯åŸºäºæ¸…æ´—åçš„æ–‡æœ¬ï¼ˆclean_word å»é™¤äº†ç©ºæ ¼ï¼‰ï¼Œæ‰€ä»¥ char_to_chunk ä¹Ÿåº”è¯¥ä¸åŒ…å«ç©ºæ ¼
                    char_to_chunk = []
                    for chunk_idx, chunk in enumerate(sent.chunks):
                        cleaned_chunk_text = clean_word(chunk.text)
                        char_to_chunk.extend([chunk_idx] * len(cleaned_chunk_text))

                    # ç¡®å®š Chunk æ‹†åˆ†ç‚¹
                    split_points = [0]
                    for br_pos in br_positions:
                        if br_pos < len(char_to_chunk):
                            chunk_idx = char_to_chunk[br_pos]
                            if chunk_idx not in split_points:
                                split_points.append(chunk_idx)
                    split_points.append(len(sent.chunks))
                    split_points = sorted(set(split_points))

                    # è·å–è¯­è¨€è¿æ¥ç¬¦
                    joiner = get_joiner(asr_language)

                    # æ‹†åˆ† Chunksï¼Œåˆ›å»ºæ–°çš„ Sentence å¯¹è±¡
                    for j in range(len(split_points) - 1):
                        start_idx = split_points[j]
                        end_idx = split_points[j + 1]

                        if start_idx >= end_idx:
                            continue

                        sub_chunks = sent.chunks[start_idx:end_idx]
                        sub_text = joiner.join(c.text for c in sub_chunks)  # ä½¿ç”¨ joiner åˆ†éš”

                        new_sentence = Sentence(
                            chunks=sub_chunks,
                            text=sub_text,
                            translation="",  # è¯‘æ–‡éœ€è¦åç»­å¯¹é½
                            start=sub_chunks[0].start,
                            end=sub_chunks[-1].end,
                            index=sent.index + j,
                            is_split=True
                        )
                        new_sentences.append(new_sentence)
                else:
                    # æ²¡æœ‰æ‰¾åˆ°æ‹†åˆ†ç‚¹ï¼Œä¿æŒåŸæ ·
                    new_sentences.append(sent)
            else:
                # LLM è®¤ä¸ºä¸éœ€è¦æ‹†åˆ†
                new_sentences.append(sent)

        # æ›´æ–°å¥å­åˆ—è¡¨
        sentences = new_sentences

    # ä¿å­˜ç»“æœåˆ° CSVï¼ˆå‘åå…¼å®¹ï¼‰
    split_src = [sent.text for sent in sentences]
    split_trans = [sent.translation for sent in sentences]
    pd.DataFrame({'Source': split_src, 'Translation': split_trans}).to_csv(_5_SPLIT_SUB, index=False, encoding='utf-8-sig')
    pd.DataFrame({'Source': split_src, 'Translation': split_trans}).to_csv(_5_REMERGED, index=False, encoding='utf-8-sig')

    console.print("[bold green]âœ… Subtitle splitting completed![/bold green]")
    console.print(f'[cyan]ğŸ“Š Returning {len(sentences)} Sentence objects to Stage 5[/cyan]')
    return sentences


if __name__ == '__main__':
    # æµ‹è¯•éœ€è¦ä»å¤–éƒ¨æä¾› sentences
    print("This module requires Sentence objects as input.")
