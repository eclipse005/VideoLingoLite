"""
ASR Segments to Sentences Module (Stage 1)

æŒ‰æ ‡ç‚¹ç¬¦å·å°† ASR segments åˆ†å‰²ä¸ºå¥å­
ASR åç«¯ï¼ˆqwen/customï¼‰è¿”å›å•è¯çº§æ—¶é—´æˆ³ï¼Œè¿™é‡ŒæŒ‰æ ‡ç‚¹åˆ†å¥

Output: split_by_punctuation.txt (Stage 1 result)
"""

import json
import os
from typing import List

from core.utils import rprint, timer, load_key
from core.utils.models import _3_1_SPLIT_BY_PUNCTUATION, Chunk, Sentence
from core.utils.sentence_splitting import group_words_into_sentences_dicts, is_sentence_terminator
from core.utils.config_utils import get_joiner


def load_asr_json() -> dict:
    """åŠ è½½ ASR ç»“æœ JSON"""
    asr_json_path = "output/log/asr.json"
    if not os.path.exists(asr_json_path):
        raise FileNotFoundError(f"ASR ç»“æœä¸å­˜åœ¨: {asr_json_path}")

    with open(asr_json_path, 'r', encoding='utf-8') as f:
        return json.load(f)


def segments_to_sentences(asr_data: dict) -> List[Sentence]:
    """
    æŒ‰æ ‡ç‚¹ç¬¦å·å°† ASR segments åˆ†å‰²ä¸º Sentence å¯¹è±¡

    ASR åç«¯è¿”å›å•è¯çº§æ—¶é—´æˆ³ï¼Œè¿™é‡ŒæŒ‰æ ‡ç‚¹åˆ†å¥

    Args:
        asr_data: ASR JSON æ•°æ®ï¼ŒåŒ…å« segments åˆ—è¡¨ï¼ˆæ¯ä¸ª segment æœ‰ text å’Œ wordsï¼‰

    Returns:
        List[Sentence]
    """
    segments = asr_data.get('segments', [])
    if not segments:
        rprint("[yellow]âš ï¸ æœªæ‰¾åˆ° ASR segments[/yellow]")
        return []

    # è·å–è¯­è¨€ç±»å‹å’Œ joiner
    asr_language = load_key("asr.language")
    joiner = get_joiner(asr_language)

    all_sentences = []
    sentence_idx = 0

    for seg in segments:
        seg_text = seg.get('text', '')
        seg_words = seg.get('words', [])

        if not seg_words:
            continue

        # æ£€æŸ¥æ˜¯å¦åŒ…å«å¥å­ç»“æŸç¬¦å·
        has_terminator = any(is_sentence_terminator(c) for c in seg_text)

        if has_terminator:
            # æŒ‰æ ‡ç‚¹ç¬¦å·åˆ†å¥
            word_groups = group_words_into_sentences_dicts(seg_words)

            for word_group in word_groups:
                if not word_group:
                    continue

                # åˆ›å»º chunks
                chunks = []
                for word_idx, word_info in enumerate(word_group):
                    chunk = Chunk(
                        text=word_info['word'],
                        start=word_info['start'],
                        end=word_info['end'],
                        speaker_id=None,
                        index=word_idx
                    )
                    chunks.append(chunk)

                # åˆ›å»º Sentence å¯¹è±¡
                text = joiner.join([w['word'] for w in word_group])
                start = word_group[0]['start']
                end = word_group[-1]['end']

                sentence = Sentence(
                    chunks=chunks,
                    text=text,
                    start=start,
                    end=end,
                    index=sentence_idx
                )
                all_sentences.append(sentence)
                sentence_idx += 1
        else:
            # æ²¡æœ‰æ ‡ç‚¹ç¬¦å·ï¼Œåˆ›å»ºå•ä¸ª Sentence
            chunks = []
            for word_idx, word_info in enumerate(seg_words):
                chunk = Chunk(
                    text=word_info['word'],
                    start=word_info['start'],
                    end=word_info['end'],
                    speaker_id=None,
                    index=word_idx
                )
                chunks.append(chunk)

            sentence = Sentence(
                chunks=chunks,
                text=seg_text,
                start=seg['start'],
                end=seg['end'],
                index=sentence_idx
            )
            all_sentences.append(sentence)
            sentence_idx += 1

    return all_sentences


@timer("ASR Segments è½¬å¥å­")
def split_by_punctuation() -> List[Sentence]:
    """
    ASR Segments è½¬å¥å­ä¸»å‡½æ•°ï¼ˆStage 1ï¼‰

    æŒ‰æ ‡ç‚¹ç¬¦å·å°† ASR segments åˆ†å‰²ä¸ºå¥å­

    Returns:
        List[Sentence]: Sentence å¯¹è±¡åˆ—è¡¨
    """
    # 1. åŠ è½½ ASR ç»“æœ
    asr_data = load_asr_json()

    # 2. æŒ‰æ ‡ç‚¹ç¬¦å·åˆ†å¥
    segments = asr_data.get('segments', [])
    total_words = sum(len(seg.get('words', [])) for seg in segments)
    rprint(f"[blue]ğŸ” Stage 1: ASR â†’ {len(segments)} segment(s), {total_words} words â†’ æŒ‰æ ‡ç‚¹åˆ†å¥[/blue]")

    sentences = segments_to_sentences(asr_data)

    if not sentences:
        rprint("[yellow]âš ï¸ æ²¡æœ‰å¥å­åˆ›å»ºï¼Œè·³è¿‡åç»­å¤„ç†[/yellow]")
        return []

    rprint(f"[blue]   â†’ {len(sentences)} sentences[/blue]")

    # 3. ä¿å­˜åˆ°æ–‡ä»¶
    os.makedirs(os.path.dirname(_3_1_SPLIT_BY_PUNCTUATION), exist_ok=True)
    with open(_3_1_SPLIT_BY_PUNCTUATION, 'w', encoding='utf-8') as f:
        for s in sentences:
            f.write(s.text + '\n')

    rprint(f'[green]âœ… Stage 1 å®Œæˆ: {len(sentences)} ä¸ªå¥å­[/green]')
    return sentences


if __name__ == '__main__':
    split_by_punctuation()
