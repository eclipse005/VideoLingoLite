"""
ASR Segments to Sentences Module (Stage 1)

ç›´æ¥ä» Parakeet ASR çš„ segments åˆ›å»º Sentence å¯¹è±¡ï¼Œæ— éœ€ spaCy åˆ†å¥
å¯é€‰æ‹©åº”ç”¨åœé¡¿åˆ†å¥ï¼ˆåŸºäºæ—¶é—´æˆ³ï¼‰

Output: split_by_nlp.txt (Stage 1 result)
"""

import json
import os
import unicodedata
import pandas as pd
from typing import List

from core.utils import rprint, load_key, timer, safe_read_csv
from core.utils.models import _3_1_SPLIT_BY_NLP, _2_CLEANED_CHUNKS, Chunk, Sentence


def is_sentence_terminator(char: str) -> bool:
    """
    åˆ¤æ–­å­—ç¬¦æ˜¯å¦ä¸ºå¥å­ç»“æŸç¬¦å·ï¼ˆä½¿ç”¨ Unicode ç±»åˆ«ï¼‰ã€‚

    æ¶µç›–å¤šè¯­è¨€ï¼š
    - ä¸­æ–‡/æ—¥æ–‡ï¼šã€‚ï¼ï¼Ÿ
    - è‹±æ–‡ï¼š.!?
    - å…¶ä»–è¯­è¨€çš„å¥å­ç»“æŸç¬¦å·
    """
    if not char:
        return False

    # å¸¸è§å¥å­ç»“æŸç¬¦å·ï¼ˆä½œä¸ºè¡¥å……ï¼‰
    terminators = {'.', '!', '?', 'ã€‚', 'ï¼', 'ï¼Ÿ', 'â€¼', 'â‡', 'âˆ', 'â‰'}
    if char in terminators:
        return True

    # ä½¿ç”¨ Unicode ç±»åˆ«åˆ¤æ–­
    # Po: Other punctuationï¼ˆåŒ…å«å¤§å¤šæ•°æ ‡ç‚¹ç¬¦å·ï¼‰
    category = unicodedata.category(char)
    if category == 'Po':
        # è¿›ä¸€æ­¥è¿‡æ»¤ï¼Œåªä¿ç•™å¥å­ç»“æŸç±»çš„æ ‡ç‚¹
        # æ’é™¤é€—å·ã€é¡¿å·ã€å¼•å·ç­‰éå¥å­ç»“æŸç¬¦å·
        non_terminators = {',', 'ï¼Œ', 'ã€', ';', 'ï¼›', ':', 'ï¼š', '"', "'", 'ã€Œ', 'ã€', 'ã€', 'ã€', 'ï¼ˆ', 'ï¼‰', '(', ')', '[', ']', '{', '}', 'ãƒ»', 'Â·', 'â€¢'}
        if char not in non_terminators:
            return True

    return False


def group_chunks_into_sentences(chunks: List[Chunk]) -> List[List[Chunk]]:
    """
    æ ¹æ®å¥å­ç»“æŸç¬¦å·å°† Chunks åˆ†ç»„ä¸ºå¥å­ã€‚

    Args:
        chunks: List[Chunk]

    Returns:
        List of sentence groups, each group is a list of Chunks
    """
    if not chunks:
        return []

    sentences = []
    current_sentence = []

    for chunk in chunks:
        current_sentence.append(chunk)

        # æ£€æŸ¥è¯¥è¯æ˜¯å¦åŒ…å«å¥å­ç»“æŸç¬¦å·
        if chunk.text:
            # æ£€æŸ¥è¯çš„ä»»æ„å­—ç¬¦æ˜¯å¦ä¸ºå¥å­ç»“æŸç¬¦
            for char in chunk.text:
                if is_sentence_terminator(char):
                    sentences.append(current_sentence)
                    current_sentence = []
                    break

    # å¤„ç†å‰©ä½™çš„ chunk
    if current_sentence:
        sentences.append(current_sentence)

    return sentences


def load_asr_json() -> dict:
    """åŠ è½½ ASR ç»“æœ JSON"""
    asr_json_path = "output/log/asr.json"
    if not os.path.exists(asr_json_path):
        raise FileNotFoundError(f"ASR ç»“æœä¸å­˜åœ¨: {asr_json_path}")

    with open(asr_json_path, 'r', encoding='utf-8') as f:
        return json.load(f)


def has_segment_text(asr_data: dict) -> bool:
    """æ£€æŸ¥ asr.json æ˜¯å¦åŒ…å« segment.text å­—æ®µ"""
    segments = asr_data.get('segments', [])
    if not segments:
        return False
    return 'text' in segments[0] and 'start' in segments[0] and 'end' in segments[0]


def load_chunks_from_csv() -> List[Chunk]:
    """ä» cleaned_chunks.csv åŠ è½½ Chunk å¯¹è±¡"""
    df = safe_read_csv(_2_CLEANED_CHUNKS)
    chunks = []

    for row in df.itertuples(index=True):
        speaker_id = row.speaker_id if pd.notna(row.speaker_id) and row.speaker_id else None
        # æ¸…ç† text å­—æ®µçš„å¼•å·
        text = row.text
        if isinstance(text, str):
            text = text.strip('"')
        chunk = Chunk(
            text=text,
            start=float(row.start),
            end=float(row.end),
            speaker_id=speaker_id,
            index=row.Index
        )
        chunks.append(chunk)

    rprint(f"[green]âœ… Loaded {len(chunks)} chunks from {_2_CLEANED_CHUNKS}[/green]")
    return chunks


def create_sentences_from_chunks(chunks: List[Chunk]) -> List[Sentence]:
    """ä» Chunk åˆ—è¡¨ï¼ˆæŒ‰æ ‡ç‚¹æ–­å¥ï¼‰åˆ›å»º Sentence å¯¹è±¡"""
    chunk_groups = group_chunks_into_sentences(chunks)
    sentences = []

    for idx, chunk_group in enumerate(chunk_groups):
        if not chunk_group:
            continue

        # æ‹¼æ¥å¥å­æ–‡æœ¬
        text = ''.join([c.text for c in chunk_group])
        start = chunk_group[0].start
        end = chunk_group[-1].end

        sentence = Sentence(
            chunks=chunk_group,
            text=text,
            start=start,
            end=end,
            index=idx
        )
        sentences.append(sentence)

    return sentences


def create_chunks_from_words(words: List[dict], start_offset: int = 0) -> List[Chunk]:
    """
    ä» words åˆ—è¡¨åˆ›å»º Chunk å¯¹è±¡

    Args:
        words: [{'start': float, 'end': float, 'word': str}, ...]
        start_offset: èµ·å§‹æ—¶é—´åç§»ï¼ˆç”¨äºè°ƒè¯•ï¼‰

    Returns:
        List[Chunk]
    """
    chunks = []
    for idx, word_info in enumerate(words):
        chunk = Chunk(
            text=word_info['word'],
            start=word_info['start'],
            end=word_info['end'],
            speaker_id=None,  # ASR æ²¡æœ‰è¯´è¯äººä¿¡æ¯
            index=idx
        )
        chunks.append(chunk)
    return chunks


def segments_to_sentences(asr_data: dict) -> List[Sentence]:
    """
    ä» ASR segments åˆ›å»º Sentence å¯¹è±¡

    Args:
        asr_data: ASR JSON æ•°æ®ï¼ŒåŒ…å« segments åˆ—è¡¨

    Returns:
        List[Sentence]
    """
    segments = asr_data.get('segments', [])
    if not segments:
        rprint("[yellow]âš ï¸ æœªæ‰¾åˆ° ASR segments[/yellow]")
        return []

    sentences = []
    for idx, seg in enumerate(segments):
        # ä» words åˆ›å»º chunks
        words = seg.get('words', [])
        chunks = create_chunks_from_words(words)

        if not chunks:
            continue

        # åˆ›å»º Sentence å¯¹è±¡
        sentence = Sentence(
            chunks=chunks,
            text=seg['text'],
            start=seg['start'],
            end=seg['end'],
            index=idx
        )
        sentences.append(sentence)

    return sentences


@timer("ASR Segments è½¬å¥å­")
def split_by_spacy() -> List[Sentence]:
    """
    ASR Segments è½¬å¥å­ä¸»å‡½æ•°ï¼ˆStage 1ï¼‰

    ä¼˜å…ˆä½¿ç”¨ Parakeet ASR çš„ segments ç›´æ¥åˆ›å»º Sentence å¯¹è±¡
    å¦‚æœæ²¡æœ‰ segment.textï¼ˆCustom ASRï¼‰ï¼Œåˆ™ä» cleaned_chunks.csv è¯»å–å¹¶æŒ‰æ ‡ç‚¹æ–­å¥
    å¯é€‰ï¼šåº”ç”¨åœé¡¿åˆ†å¥

    Returns:
        List[Sentence]: Sentence å¯¹è±¡åˆ—è¡¨
    """
    # 1. åŠ è½½ ASR ç»“æœ
    asr_data = load_asr_json()

    # 2. æ£€æŸ¥æ˜¯å¦æœ‰ segment.textï¼Œå†³å®šä½¿ç”¨å“ªç§æ–¹å¼
    if has_segment_text(asr_data):
        # æ–¹å¼ Aï¼šParakeet æ ¼å¼ï¼Œæœ‰ segment.text/start/end
        segment_count = len(asr_data.get('segments', []))
        rprint(f"[blue]ğŸ” Stage 1: ASR Segments â†’ {segment_count} ä¸ªå¥å­[/blue]")
        sentences = segments_to_sentences(asr_data)
    else:
        # æ–¹å¼ Bï¼šCustom ASR æ ¼å¼ï¼Œæ²¡æœ‰ segment.textï¼Œä» CSV è¯»å–å¹¶æŒ‰æ ‡ç‚¹æ–­å¥
        rprint(f"[blue]ğŸ” Stage 1: No segment.text found, using cleaned_chunks.csv + punctuation split[/blue]")
        chunks = load_chunks_from_csv()
        sentences = create_sentences_from_chunks(chunks)
        rprint(f"[blue]   â†’ Punctuation split: {len(chunks)} chunks â†’ {len(sentences)} sentences[/blue]")

    if not sentences:
        rprint("[yellow]âš ï¸ æ²¡æœ‰å¥å­åˆ›å»ºï¼Œè·³è¿‡åç»­å¤„ç†[/yellow]")
        return []

    # 3. å¯é€‰ï¼šåº”ç”¨åœé¡¿åˆ†å¥
    pause_threshold = load_key("pause_split_threshold")
    if pause_threshold and pause_threshold > 0:
        from core.spacy_utils import split_by_pause
        original_count = len(sentences)
        sentences = split_by_pause(sentences, pause_threshold)
        if len(sentences) != original_count:
            rprint(f"[cyan]  â†ª åœé¡¿åˆ†å¥: {original_count} â†’ {len(sentences)} ä¸ª[/cyan]")

    # 4. ä¿å­˜åˆ°æ–‡ä»¶
    from pathlib import Path
    Path(_3_1_SPLIT_BY_NLP).parent.mkdir(parents=True, exist_ok=True)
    with open(_3_1_SPLIT_BY_NLP, 'w', encoding='utf-8') as f:
        for s in sentences:
            f.write(s.text + '\n')

    rprint(f'[green]âœ… Stage 1 å®Œæˆ[/green]')
    return sentences


if __name__ == '__main__':
    split_by_spacy()
