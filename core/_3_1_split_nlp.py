"""
NLP-based Sentence Segmentation Module (Stage 1)

This module uses spaCy to perform rule-based sentence splitting:
1. Split by punctuation marks (spaCy sentence boundaries)
2. Split by commas (with linguistic analysis)
3. Split by connectors (that, which, because, but, and, etc.)
4. Split long sentences by root (dynamic programming)

Output: split_by_nlp.txt (Stage 1 result)
"""

from typing import List
from spacy.language import Language

from core.spacy_utils import *
from core.utils.models import _3_1_SPLIT_BY_NLP, Chunk, Sentence
from core.utils import rprint, load_key, get_joiner
from core._2_asr import load_chunks


# ------------
# Character Position Mapping Functions
# ------------

def build_char_to_chunk_mapping(chunks: List[Chunk], joiner: str = "") -> List[int]:
    """
    æ„å»ºå­—ç¬¦åˆ° Chunk ç´¢å¼•çš„æ˜ å°„

    Args:
        chunks: Chunk å¯¹è±¡åˆ—è¡¨
        joiner: Chunk ä¹‹é—´çš„è¿æ¥ç¬¦ï¼ˆç©ºæ ¼åˆ†éš”è¯­è¨€ä¸º " "ï¼Œå…¶ä»–ä¸º ""ï¼‰

    Returns:
        æ¯ä¸ªå­—ç¬¦å¯¹åº”çš„ Chunk ç´¢å¼•åˆ—è¡¨
    """
    char_to_chunk = []
    for chunk_idx, chunk in enumerate(chunks):
        # æ·»åŠ  chunk çš„æ¯ä¸ªå­—ç¬¦
        char_to_chunk.extend([chunk_idx] * len(chunk.text))
        # å¦‚æœæœ‰ç©ºæ ¼åˆ†éš”ç¬¦ä¸”ä¸æ˜¯æœ€åä¸€ä¸ª chunkï¼Œæ·»åŠ ç©ºæ ¼çš„æ˜ å°„
        if joiner and chunk_idx < len(chunks) - 1:
            char_to_chunk.extend([chunk_idx] * len(joiner))
    return char_to_chunk


def nlp_split_to_sentences(chunks: List[Chunk], nlp: Language) -> List[Sentence]:
    """
    ä½¿ç”¨ spaCy è¿›è¡Œ NLP åˆ†å¥ï¼Œå°† Chunk å¯¹è±¡ç»„åˆæˆ Sentence å¯¹è±¡

    Args:
        chunks: Chunk å¯¹è±¡åˆ—è¡¨
        nlp: spaCy NLP æ¨¡å‹

    Returns:
        Sentence å¯¹è±¡åˆ—è¡¨
    """
    # Validate input chunks
    if not chunks:
        return []

    # è·å– ASR è¯­è¨€å¹¶ç¡®å®šè¿æ¥ç¬¦ï¼ˆç©ºæ ¼åˆ†éš”è¯­è¨€ä½¿ç”¨ " "ï¼Œå…¶ä»–ä½¿ç”¨ ""ï¼‰
    asr_language = load_key("asr.language")
    joiner = get_joiner(asr_language)

    # 1. æ‹¼æ¥æ‰€æœ‰ Chunk çš„æ–‡æœ¬ï¼ˆä½¿ç”¨ joiner åˆ†éš”ï¼‰
    full_text = joiner.join(chunk.text for chunk in chunks)
    if not full_text:
        return []

    # 2. æ„å»ºå­—ç¬¦åˆ° Chunk çš„æ˜ å°„ï¼ˆè€ƒè™‘ç©ºæ ¼ï¼‰
    char_to_chunk = build_char_to_chunk_mapping(chunks, joiner)

    # 3. ä½¿ç”¨ spaCy åˆ†å¥
    doc = nlp(full_text)
    sentences = []

    for sent_idx, sent in enumerate(doc.sents):
        start_char = sent.start_char
        end_char = sent.end_char

        # è¾¹ç•Œæ£€æŸ¥ - Ensure start_char is within valid range [0, len(full_text)-1]
        if start_char >= len(full_text):
            continue  # Skip invalid sentence
        start_char = max(0, start_char)
        # Ensure end_char is at least start_char + 1 and at most len(full_text)
        end_char = max(start_char + 1, min(end_char, len(full_text)))

        # æ‰¾åˆ°å¯¹åº”çš„ Chunk èŒƒå›´
        start_chunk_idx = char_to_chunk[start_char]
        end_chunk_idx = char_to_chunk[end_char - 1]

        # æå–å¯¹åº”çš„ Chunk å¯¹è±¡
        sentence_chunks = chunks[start_chunk_idx:end_chunk_idx + 1]

        # åˆ›å»º Sentence å¯¹è±¡
        sentence = Sentence(
            chunks=sentence_chunks,
            text=sent.text,
            start=sentence_chunks[0].start if sentence_chunks else 0.0,
            end=sentence_chunks[-1].end if sentence_chunks else 0.0,
            index=sent_idx
        )
        sentences.append(sentence)

    return sentences


def split_by_spacy() -> List[Sentence]:
    """
    NLP åˆ†å¥ä¸»å‡½æ•°ï¼ˆStage 1ï¼‰

    ä½¿ç”¨å¯¹è±¡åŒ–æµç¨‹ï¼šä» Chunks ç”Ÿæˆ Sentence å¯¹è±¡

    Returns:
        List[Sentence]: åˆ†å¥åçš„ Sentence å¯¹è±¡åˆ—è¡¨
    """
    rprint("[blue]ğŸ” Starting NLP-based sentence segmentation (Stage 1)[/blue]")

    nlp = init_nlp()

    # ä½¿ç”¨å¯¹è±¡åŒ–æµç¨‹ç”Ÿæˆ Sentence å¯¹è±¡
    sentences = split_by_nlp(nlp)

    rprint(f"[green]âœ… NLP sentence segmentation completed: {_3_1_SPLIT_BY_NLP}[/green]")
    rprint(f"[cyan]ğŸ“Š Generated {len(sentences)} Sentence objects[/cyan]")
    return sentences


# ------------
# New NLP Split Function with Character Position Tracking
# ------------

def split_by_nlp(nlp: Language) -> List[Sentence]:
    """
    NLP åˆ†å¥ä¸»å‡½æ•°

    è¾“å…¥: cleaned_chunks.csv â†’ List[Chunk]
    è¾“å‡º: List[Sentence] â†’ ä¿å­˜åˆ° split_by_nlp.txt (æ–‡æœ¬) å’Œè¿”å›å¯¹è±¡
    """
    rprint("[blue]ğŸ” Starting NLP sentence splitting...[/blue]")

    # 1. åŠ è½½ Chunk å¯¹è±¡
    chunks = load_chunks()

    # 2. NLP åˆ†å¥ï¼Œç”Ÿæˆ Sentence å¯¹è±¡
    sentences = nlp_split_to_sentences(chunks, nlp)

    # 3. ä¿å­˜æ–‡æœ¬åˆ°æ–‡ä»¶ï¼ˆå‘åå…¼å®¹ï¼‰
    with open(_3_1_SPLIT_BY_NLP, 'w', encoding='utf-8') as f:
        for sent in sentences:
            f.write(sent.text + '\n')

    rprint(f'[green]âœ… NLP splitting complete! {len(sentences)} sentences generated[/green]')
    rprint(f'[green]ğŸ’¾ Saved to: {_3_1_SPLIT_BY_NLP}[/green]')

    return sentences


if __name__ == '__main__':
    split_by_spacy()
