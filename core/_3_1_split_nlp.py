"""
ASR Segments to Sentences Module (Stage 1)

ç›´æ¥ä» ASR segments åˆ›å»º Sentence å¯¹è±¡
ASR åç«¯ï¼ˆqwen/customï¼‰å·²æŒ‰æ ‡ç‚¹åˆ†å¥ï¼Œè¿™é‡Œåªéœ€åˆ›å»º Sentence å¯¹è±¡

Output: split_by_nlp.txt (Stage 1 result)
"""

import json
import os
import pandas as pd
from typing import List

from core.utils import rprint, load_key, timer, safe_read_csv
from core.utils.models import _3_1_SPLIT_BY_NLP, _2_CLEANED_CHUNKS, Chunk, Sentence


def load_asr_json() -> dict:
    """åŠ è½½ ASR ç»“æœ JSON"""
    asr_json_path = "output/log/asr.json"
    if not os.path.exists(asr_json_path):
        raise FileNotFoundError(f"ASR ç»“æœä¸å­˜åœ¨: {asr_json_path}")

    with open(asr_json_path, 'r', encoding='utf-8') as f:
        return json.load(f)


def has_segment_text(asr_data: dict) -> bool:
    """æ£€æŸ¥ asr.json æ˜¯å¦åŒ…å« segment.text å­—æ®µ"""
    segments = asr_data.get('segments', [])
    if not segments:
        return False
    return 'text' in segments[0] and 'start' in segments[0] and 'end' in segments[0]


def segments_to_sentences(asr_data: dict) -> List[Sentence]:
    """
    ä» ASR segments åˆ›å»º Sentence å¯¹è±¡

    ASR åç«¯ï¼ˆqwen/customï¼‰å·²æŒ‰æ ‡ç‚¹åˆ†å¥ï¼Œè¿™é‡Œåªéœ€åˆ›å»º Sentence å¯¹è±¡

    Args:
        asr_data: ASR JSON æ•°æ®ï¼ŒåŒ…å« segments åˆ—è¡¨

    Returns:
        List[Sentence]
    """
    segments = asr_data.get('segments', [])
    if not segments:
        rprint("[yellow]âš ï¸ æœªæ‰¾åˆ° ASR segments[/yellow]")
        return []

    sentences = []
    for idx, seg in enumerate(segments):
        # ä» words åˆ›å»º chunks
        words = seg.get('words', [])
        chunks = []
        for word_idx, word_info in enumerate(words):
            chunk = Chunk(
                text=word_info['word'],
                start=word_info['start'],
                end=word_info['end'],
                speaker_id=None,
                index=word_idx
            )
            chunks.append(chunk)

        if not chunks:
            continue

        # åˆ›å»º Sentence å¯¹è±¡
        sentence = Sentence(
            chunks=chunks,
            text=seg['text'],
            start=seg['start'],
            end=seg['end'],
            index=idx
        )
        sentences.append(sentence)

    return sentences


def load_chunks_from_csv() -> List[Chunk]:
    """ä» cleaned_chunks.csv åŠ è½½ Chunk å¯¹è±¡ï¼ˆç”¨äº custom ASR æ—  segment.text çš„æƒ…å†µï¼‰"""
    df = safe_read_csv(_2_CLEANED_CHUNKS)
    chunks = []

    for row in df.itertuples(index=True):
        speaker_id = row.speaker_id if pd.notna(row.speaker_id) and row.speaker_id else None
        text = row.text
        if isinstance(text, str):
            text = text.strip('"')
        chunk = Chunk(
            text=text,
            start=float(row.start),
            end=float(row.end),
            speaker_id=speaker_id,
            index=row.Index
        )
        chunks.append(chunk)

    rprint(f"[green]âœ… Loaded {len(chunks)} chunks from {_2_CLEANED_CHUNKS}[/green]")
    return chunks


def group_chunks_into_sentences(chunks: List[Chunk]) -> List[List[Chunk]]:
    """
    æ ¹æ®å¥å­ç»“æŸç¬¦å·å°† Chunks åˆ†ç»„ä¸ºå¥å­ï¼ˆç”¨äº custom ASR æ— æ ‡ç‚¹çš„æƒ…å†µï¼‰
    """
    import unicodedata

    def is_sentence_terminator(char: str) -> bool:
        if not char:
            return False
        terminators = {'.', '!', '?', 'ã€‚', 'ï¼', 'ï¼Ÿ'}
        if char in terminators:
            return True
        category = unicodedata.category(char)
        if category == 'Po':
            non_terminators = {',', 'ï¼Œ', 'ã€', ';', 'ï¼›', ':', 'ï¼š'}
            if char not in non_terminators:
                return True
        return False

    if not chunks:
        return []

    sentences = []
    current_sentence = []

    for chunk in chunks:
        current_sentence.append(chunk)
        if chunk.text:
            for char in chunk.text:
                if is_sentence_terminator(char):
                    sentences.append(current_sentence)
                    current_sentence = []
                    break

    if current_sentence:
        sentences.append(current_sentence)

    return sentences


def create_sentences_from_chunks(chunks: List[Chunk]) -> List[Sentence]:
    """ä» Chunk åˆ—è¡¨ï¼ˆæŒ‰æ ‡ç‚¹æ–­å¥ï¼‰åˆ›å»º Sentence å¯¹è±¡"""
    chunk_groups = group_chunks_into_sentences(chunks)
    sentences = []

    for idx, chunk_group in enumerate(chunk_groups):
        if not chunk_group:
            continue

        text = ''.join([c.text for c in chunk_group])
        start = chunk_group[0].start
        end = chunk_group[-1].end

        sentence = Sentence(
            chunks=chunk_group,
            text=text,
            start=start,
            end=end,
            index=idx
        )
        sentences.append(sentence)

    return sentences


@timer("ASR Segments è½¬å¥å­")
def split_by_spacy() -> List[Sentence]:
    """
    ASR Segments è½¬å¥å­ä¸»å‡½æ•°ï¼ˆStage 1ï¼‰

    ASR åç«¯ï¼ˆqwen/customï¼‰å·²æŒ‰æ ‡ç‚¹åˆ†å¥ï¼Œè¿™é‡Œåªéœ€åˆ›å»º Sentence å¯¹è±¡
    å¦‚æœæ²¡æœ‰ segment.textï¼ˆCustom ASR æ—§æ ¼å¼ï¼‰ï¼Œåˆ™ä» CSV è¯»å–å¹¶æŒ‰æ ‡ç‚¹æ–­å¥

    Returns:
        List[Sentence]: Sentence å¯¹è±¡åˆ—è¡¨
    """
    # 1. åŠ è½½ ASR ç»“æœ
    asr_data = load_asr_json()

    # 2. æ£€æŸ¥æ˜¯å¦æœ‰ segment.text
    if has_segment_text(asr_data):
        # æœ‰ segment.textï¼ŒASR åç«¯å·²åˆ†å¥ï¼Œç›´æ¥åˆ›å»º Sentence å¯¹è±¡
        segment_count = len(asr_data.get('segments', []))
        rprint(f"[blue]ğŸ” Stage 1: ASR Segments â†’ {segment_count} ä¸ªå¥å­[/blue]")
        sentences = segments_to_sentences(asr_data)
    else:
        # æ²¡æœ‰ segment.textï¼ˆCustom ASR æ—§æ ¼å¼ï¼‰ï¼Œä» CSV è¯»å–å¹¶æŒ‰æ ‡ç‚¹æ–­å¥
        rprint(f"[blue]ğŸ” Stage 1: No segment.text found, using cleaned_chunks.csv + punctuation split[/blue]")
        chunks = load_chunks_from_csv()
        sentences = create_sentences_from_chunks(chunks)
        rprint(f"[blue]   â†’ {len(chunks)} chunks â†’ {len(sentences)} sentences[/blue]")

    if not sentences:
        rprint("[yellow]âš ï¸ æ²¡æœ‰å¥å­åˆ›å»ºï¼Œè·³è¿‡åç»­å¤„ç†[/yellow]")
        return []

    # 3. ä¿å­˜åˆ°æ–‡ä»¶
    from pathlib import Path
    Path(_3_1_SPLIT_BY_NLP).parent.mkdir(parents=True, exist_ok=True)
    with open(_3_1_SPLIT_BY_NLP, 'w', encoding='utf-8') as f:
        for s in sentences:
            f.write(s.text + '\n')

    rprint(f'[green]âœ… Stage 1 å®Œæˆ: {len(sentences)} ä¸ªå¥å­[/green]')
    return sentences


if __name__ == '__main__':
    split_by_spacy()
