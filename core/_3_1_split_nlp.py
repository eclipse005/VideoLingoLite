"""
NLP-based Sentence Segmentation Module (Stage 1)

This module uses spaCy to perform rule-based sentence splitting:
1. Split by punctuation marks (spaCy sentence boundaries)
2. Split by commas (with linguistic analysis)
3. Split by connectors (that, which, because, but, and, etc.)
4. Split long sentences by root (dynamic programming)

Output: split_by_nlp.txt (Stage 1 result)
"""

from typing import List
from spacy.language import Language

from core.spacy_utils import *
from core.utils.models import _3_1_SPLIT_BY_NLP, _CACHE_SENTENCES_NLP, Chunk, Sentence
from core.utils import rprint, load_key, get_joiner, Timer, cache_objects
from core._2_asr import load_chunks


# ------------
# Character Position Mapping Functions
# ------------

def build_char_to_chunk_mapping(chunks: List[Chunk], joiner: str = "") -> List[int]:
    """
    æ„å»ºå­—ç¬¦åˆ° Chunk ç´¢å¼•çš„æ˜ å°„

    Args:
        chunks: Chunk å¯¹è±¡åˆ—è¡¨
        joiner: Chunk ä¹‹é—´çš„è¿æ¥ç¬¦ï¼ˆç©ºæ ¼åˆ†éš”è¯­è¨€ä¸º " "ï¼Œå…¶ä»–ä¸º ""ï¼‰

    Returns:
        æ¯ä¸ªå­—ç¬¦å¯¹åº”çš„ Chunk ç´¢å¼•åˆ—è¡¨
    """
    char_to_chunk = []
    for chunk_idx, chunk in enumerate(chunks):
        # æ·»åŠ  chunk çš„æ¯ä¸ªå­—ç¬¦
        char_to_chunk.extend([chunk_idx] * len(chunk.text))
        # å¦‚æœæœ‰ç©ºæ ¼åˆ†éš”ç¬¦ä¸”ä¸æ˜¯æœ€åä¸€ä¸ª chunkï¼Œæ·»åŠ ç©ºæ ¼çš„æ˜ å°„
        if joiner and chunk_idx < len(chunks) - 1:
            char_to_chunk.extend([chunk_idx] * len(joiner))
    return char_to_chunk


def nlp_split_to_sentences(chunks: List[Chunk], nlp: Language) -> List[Sentence]:
    """
    ä½¿ç”¨ spaCy è¿›è¡Œ NLP åˆ†å¥ï¼Œå°† Chunk å¯¹è±¡ç»„åˆæˆ Sentence å¯¹è±¡
    ä¿®å¤ï¼šä¿è¯ Chunk åŸå­æ€§ï¼Œé¿å…ç”±æ ‡ç‚¹ç¬¦å·å¯¼è‡´çš„ Chunk é”™è¯¯åˆ†å‰²
    """
    # Validate input chunks
    if not chunks:
        return []

    # è·å– ASR è¯­è¨€å¹¶ç¡®å®šè¿æ¥ç¬¦
    asr_language = load_key("asr.language")
    joiner = get_joiner(asr_language)

    # 1. æ‹¼æ¥æ‰€æœ‰ Chunk çš„æ–‡æœ¬
    full_text = joiner.join(chunk.text for chunk in chunks)
    if not full_text:
        return []

    # 2. æ„å»ºå­—ç¬¦åˆ° Chunk çš„æ˜ å°„
    char_to_chunk = build_char_to_chunk_mapping(chunks, joiner)

    # 3. ä½¿ç”¨ spaCy åˆ†å¥
    doc = nlp(full_text)
    sentences = []

    # é¢„å…ˆè®¡ç®—æ¯ä¸ª Chunk åœ¨ full_text ä¸­çš„å­—ç¬¦è¾¹ç•Œ
    chunk_boundaries = []
    current_pos = 0
    for i, chunk in enumerate(chunks):
        chunk_start = current_pos
        chunk_end = current_pos + len(chunk.text)
        chunk_boundaries.append((chunk_start, chunk_end, i))
        current_pos = chunk_end + len(joiner)

    # è·Ÿè¸ªä¸‹ä¸€ä¸ªå¯ç”¨çš„ Chunk ç´¢å¼• (Slice çš„ä¸‹ç•Œ)
    next_available_chunk_idx = 0

    for sent_idx, sent in enumerate(doc.sents):
        start_char = sent.start_char
        end_char = sent.end_char

        # è¾¹ç•Œæ£€æŸ¥
        if start_char >= len(full_text):
            continue
        start_char = max(0, start_char)
        end_char = max(start_char + 1, min(end_char, len(full_text)))

        # æ‰¾åˆ°å¯¹åº”çš„ Chunk èŒƒå›´
        start_chunk_idx = char_to_chunk[start_char]
        end_chunk_idx = char_to_chunk[end_char - 1]

        # å…³é”®ä¿®æ­£ 1ï¼šç¡®ä¿èµ·å§‹ Chunk ä¸ä¼šå›é€€åˆ°å·²ä½¿ç”¨çš„ Chunk ä¹‹å‰
        # å¦‚æœ spaCy è¯†åˆ«çš„è¿™ä¸ªå¥å­çš„å¼€å¤´è¿˜åœ¨ä¸Šä¸€ä¸ªå¥å­åŒ…å«çš„ Chunk é‡Œï¼ˆä¾‹å¦‚ "ï¼Œ"ï¼‰ï¼Œ
        # æˆ‘ä»¬å°±ä»ä¸‹ä¸€ä¸ªå¯ç”¨ Chunk å¼€å§‹ç®—ï¼Œè¿™é€šå¸¸ä¼šå¯¼è‡´ start > endï¼Œä»è€Œè§¦å‘ä¸‹é¢çš„è·³è¿‡é€»è¾‘
        start_chunk_idx = max(start_chunk_idx, next_available_chunk_idx)

        # å…³é”®ä¿®æ­£ 2ï¼šå¦‚æœå½“å‰å¥å­æ˜ å°„åˆ°çš„ Chunk å·²ç»è¢«ä¸Šä¸€ä¸ªå¥å­ç”¨å…‰äº†ï¼Œè·³è¿‡æ­¤â€œæ®‹ä½™â€å¥å­
        # ä¾‹å¦‚ï¼š"ç»ˆã€‚ï¼Œ" æ•´ä¸ª Chunk å·²è¢«å½’å…¥ä¸Šä¸€å¥ï¼ŒspaCy å†æŠŠ "ï¼Œ" è¯†åˆ«ä¸ºæ–°å¥æ—¶ï¼Œè¿™é‡Œä¼šæ‹¦æˆª
        if start_chunk_idx >= len(chunks):
            continue

        # ç¡®ä¿ end_chunk_idx æ˜¯ Slice çš„ä¸Šç•Œï¼ˆå³åŒ…å«è¯¥ Chunkï¼Œæ‰€ä»¥è¦ +1 å¯¹åº” Python åˆ‡ç‰‡è¯­æ³•ï¼‰
        # åˆå§‹æ—¶ end_chunk_idx æŒ‡å‘åŒ…å« end_char çš„é‚£ä¸ª chunk çš„ç´¢å¼•
        
        # æ£€æŸ¥åˆ†å¥ç‚¹æ˜¯å¦åœ¨ Chunk å†…éƒ¨
        if end_chunk_idx < len(chunk_boundaries):
            end_chunk_start, end_chunk_end, _ = chunk_boundaries[end_chunk_idx]
            
            # å¦‚æœ spaCy çš„ end_char åœ¨ Chunk å†…éƒ¨ï¼ˆä¸åœ¨è¾¹ç•Œï¼‰ï¼Œå¼ºåˆ¶åŒ…å«æ•´ä¸ª Chunk
            # è¿™ä¿è¯äº† "ç»ˆã€‚ï¼Œ" ä½œä¸ºä¸€ä¸ªæ•´ä½“è¢«å½’å…¥å½“å‰å¥å­
            if end_char > end_chunk_start and end_char < end_chunk_end:
                 # å½“å‰ chunk å¿…é¡»å®Œæ•´åŒ…å«ï¼Œæ‰€ä»¥åˆ‡ç‰‡ä¸Šç•Œæ˜¯ index + 1
                 slice_end = end_chunk_idx + 1
            else:
                 # æ­£å¥½åœ¨è¾¹ç•Œï¼Œæˆ–è€…è·¨è¶Šäº†ï¼Œä¹Ÿè‡³å°‘è¦åŒ…å«åˆ°å½“å‰è¿™ä¸ª chunk
                 slice_end = end_chunk_idx + 1
        else:
            slice_end = end_chunk_idx + 1

        # ä¿®æ­£åˆ‡ç‰‡ä¸Šç•Œï¼šå¿…é¡»è‡³å°‘ç­‰äº start
        slice_end = max(slice_end, start_chunk_idx)

        # æå– Chunk å¯¹è±¡
        sentence_chunks = chunks[start_chunk_idx:slice_end]

        # å…³é”®ä¿®æ­£ 3ï¼šå¿½ç•¥ç©ºçš„å¥å­ï¼ˆå½“ spaCy çš„å¥å­å®Œå…¨è½åœ¨ä¸€ä¸ªå·²ç»è¢«ä¸Šä¸€å¥åå¹¶çš„ Chunk é‡Œæ—¶ï¼‰
        if not sentence_chunks:
            continue

        # æ›´æ–°æŒ‡é’ˆï¼šä¸‹ä¸€æ¬¡ä»å½“å‰åˆ‡ç‰‡çš„æœ«å°¾å¼€å§‹
        next_available_chunk_idx = slice_end

        # å…³é”®ä¿®æ­£ 4ï¼šæ ¹æ® Chunk é‡å»ºæ–‡æœ¬ï¼Œè€Œä¸æ˜¯ä½¿ç”¨ spaCy çš„ sent.text
        # è¿™ç¡®ä¿äº† Chunk å†…çš„æ–‡æœ¬ï¼ˆå¦‚ "ç»ˆã€‚ï¼Œ"ï¼‰æ˜¯å®Œæ•´çš„ï¼Œä¸ä¼šè¢«åˆ‡æ–­
        reconstructed_text = joiner.join(c.text for c in sentence_chunks)

        sentence = Sentence(
            chunks=sentence_chunks,
            text=reconstructed_text, # ä½¿ç”¨é‡å»ºçš„å®Œæ•´æ–‡æœ¬
            start=sentence_chunks[0].start if sentence_chunks else 0.0,
            end=sentence_chunks[-1].end if sentence_chunks else 0.0,
            index=len(sentences) # ä½¿ç”¨åˆ—è¡¨é•¿åº¦ä½œä¸ºç´¢å¼•ï¼Œä¿è¯è¿ç»­
        )
        sentences.append(sentence)

    return sentences


def split_by_spacy() -> List[Sentence]:
    """
    NLP åˆ†å¥ä¸»å‡½æ•°ï¼ˆStage 1ï¼‰

    ä½¿ç”¨å¯¹è±¡åŒ–æµç¨‹ï¼šä» Chunks ç”Ÿæˆ Sentence å¯¹è±¡

    Returns:
        List[Sentence]: åˆ†å¥åçš„ Sentence å¯¹è±¡åˆ—è¡¨
    """
    rprint("[blue]ğŸ” å¼€å§‹ NLP åˆ†å¥ (Stage 1)[/blue]")

    with Timer("NLP åˆ†å¥"):
        nlp = init_nlp()

        # ä½¿ç”¨å¯¹è±¡åŒ–æµç¨‹ç”Ÿæˆ Sentence å¯¹è±¡
        sentences = split_by_nlp(nlp)

    rprint(f"[green]âœ… NLP åˆ†å¥å®Œæˆ: {_3_1_SPLIT_BY_NLP}[/green]")
    return sentences


# ------------
# New NLP Split Function with Character Position Tracking
# ------------

@cache_objects(_CACHE_SENTENCES_NLP, _3_1_SPLIT_BY_NLP)
def split_by_nlp(nlp: Language) -> List[Sentence]:
    """
    NLP åˆ†å¥ä¸»å‡½æ•°

    è¾“å…¥: cleaned_chunks.csv â†’ List[Chunk]
    è¾“å‡º: List[Sentence] â†’ ä¿å­˜åˆ° split_by_nlp.txt (æ–‡æœ¬) å’Œè¿”å›å¯¹è±¡
    """
    # 1. åŠ è½½ Chunk å¯¹è±¡
    chunks = load_chunks()

    # 2. NLP åˆ†å¥ï¼Œç”Ÿæˆ Sentence å¯¹è±¡
    sentences = nlp_split_to_sentences(chunks, nlp)

    rprint(f'[green]âœ… å¤„ç†å®Œæˆï¼å…± {len(sentences)} ä¸ªå¥å­[/green]')

    return sentences


if __name__ == '__main__':
    split_by_spacy()
