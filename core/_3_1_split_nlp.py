"""
NLP-based Sentence Segmentation Module (Stage 1)

This module uses spaCy to perform rule-based sentence splitting:
1. Split by punctuation marks (spaCy sentence boundaries)
2. Split by commas (with linguistic analysis)
3. Split by connectors (that, which, because, but, and, etc.)
4. Split long sentences by root (dynamic programming)

Output: split_by_nlp.txt (Stage 1 result)
"""

from typing import List
from spacy.language import Language

from core.spacy_utils import *
# SudachiPy token length limit (bytes) - must match split_by_mark.py
SUDACHI_MAX_LENGTH = 40000

from core.utils.models import _3_1_SPLIT_BY_NLP, _CACHE_SENTENCES_NLP, Chunk, Sentence
from core.utils import rprint, load_key, get_joiner, timer, cache_objects
from core._2_asr import load_chunks


# ------------
# Character Position Mapping Functions
# ------------

def build_char_to_chunk_mapping(chunks: List[Chunk], joiner: str = "") -> List[int]:
    """
    æ„å»ºå­—ç¬¦åˆ° Chunk ç´¢å¼•çš„æ˜ å°„

    Args:
        chunks: Chunk å¯¹è±¡åˆ—è¡¨
        joiner: Chunk ä¹‹é—´çš„è¿æ¥ç¬¦ï¼ˆç©ºæ ¼åˆ†éš”è¯­è¨€ä¸º " "ï¼Œå…¶ä»–ä¸º ""ï¼‰

    Returns:
        æ¯ä¸ªå­—ç¬¦å¯¹åº”çš„ Chunk ç´¢å¼•åˆ—è¡¨
    """
    char_to_chunk = []
    for chunk_idx, chunk in enumerate(chunks):
        # æ·»åŠ  chunk çš„æ¯ä¸ªå­—ç¬¦
        char_to_chunk.extend([chunk_idx] * len(chunk.text))
        # å¦‚æœæœ‰ç©ºæ ¼åˆ†éš”ç¬¦ä¸”ä¸æ˜¯æœ€åä¸€ä¸ª chunkï¼Œæ·»åŠ ç©ºæ ¼çš„æ˜ å°„
        if joiner and chunk_idx < len(chunks) - 1:
            char_to_chunk.extend([chunk_idx] * len(joiner))
    return char_to_chunk


def _process_batch(chunks: List[Chunk], nlp: Language, joiner: str) -> List[Sentence]:
    """
    å¤„ç†å•ä¸ªæ‰¹æ¬¡çš„ Chunkï¼Œè¿”å› Sentence å¯¹è±¡åˆ—è¡¨

    è¿™æ˜¯ nlp_split_to_sentences çš„æ ¸å¿ƒé€»è¾‘ï¼Œæå–ä¸ºç‹¬ç«‹å‡½æ•°ä»¥æ”¯æŒåˆ†æ‰¹å¤„ç†
    """
    # 1. æ‹¼æ¥å½“å‰æ‰¹æ¬¡çš„ Chunk æ–‡æœ¬
    full_text = joiner.join(chunk.text for chunk in chunks)
    if not full_text:
        return []

    # 2. æ„å»ºå­—ç¬¦åˆ° Chunk çš„æ˜ å°„ï¼ˆä»…é’ˆå¯¹å½“å‰æ‰¹æ¬¡ï¼‰
    char_to_chunk = build_char_to_chunk_mapping(chunks, joiner)

    # 3. ä½¿ç”¨ spaCy åˆ†å¥
    doc = nlp(full_text)
    sentences = []

    # é¢„å…ˆè®¡ç®—æ¯ä¸ª Chunk åœ¨ full_text ä¸­çš„å­—ç¬¦è¾¹ç•Œ
    chunk_boundaries = []
    current_pos = 0
    for i, chunk in enumerate(chunks):
        chunk_start = current_pos
        chunk_end = current_pos + len(chunk.text)
        chunk_boundaries.append((chunk_start, chunk_end, i))
        current_pos = chunk_end + len(joiner)

    # è·Ÿè¸ªä¸‹ä¸€ä¸ªå¯ç”¨çš„ Chunk ç´¢å¼• (Slice çš„ä¸‹ç•Œ)
    next_available_chunk_idx = 0

    for sent_idx, sent in enumerate(doc.sents):
        start_char = sent.start_char
        end_char = sent.end_char

        # è¾¹ç•Œæ£€æŸ¥
        if start_char >= len(full_text):
            continue
        start_char = max(0, start_char)
        end_char = max(start_char + 1, min(end_char, len(full_text)))

        # æ‰¾åˆ°å¯¹åº”çš„ Chunk èŒƒå›´
        start_chunk_idx = char_to_chunk[start_char]
        end_chunk_idx = char_to_chunk[end_char - 1]

        # å…³é”®ä¿®æ­£ 1ï¼šç¡®ä¿èµ·å§‹ Chunk ä¸ä¼šå›é€€åˆ°å·²ä½¿ç”¨çš„ Chunk ä¹‹å‰
        start_chunk_idx = max(start_chunk_idx, next_available_chunk_idx)

        # å…³é”®ä¿®æ­£ 2ï¼šå¦‚æœå½“å‰å¥å­æ˜ å°„åˆ°çš„ Chunk å·²ç»è¢«ä¸Šä¸€ä¸ªå¥å­ç”¨å…‰äº†ï¼Œè·³è¿‡
        if start_chunk_idx >= len(chunks):
            continue

        # ç¡®ä¿ end_chunk_idx æ˜¯ Slice çš„ä¸Šç•Œ
        if end_chunk_idx < len(chunk_boundaries):
            end_chunk_start, end_chunk_end, _ = chunk_boundaries[end_chunk_idx]

            if end_char > end_chunk_start and end_char < end_chunk_end:
                 slice_end = end_chunk_idx + 1
            else:
                 slice_end = end_chunk_idx + 1
        else:
            slice_end = end_chunk_idx + 1

        # ä¿®æ­£åˆ‡ç‰‡ä¸Šç•Œï¼šå¿…é¡»è‡³å°‘ç­‰äº start
        slice_end = max(slice_end, start_chunk_idx)

        # æå– Chunk å¯¹è±¡
        sentence_chunks = chunks[start_chunk_idx:slice_end]

        # å…³é”®ä¿®æ­£ 3ï¼šå¿½ç•¥ç©ºçš„å¥å­
        if not sentence_chunks:
            continue

        # æ›´æ–°æŒ‡é’ˆï¼šä¸‹ä¸€æ¬¡ä»å½“å‰åˆ‡ç‰‡çš„æœ«å°¾å¼€å§‹
        next_available_chunk_idx = slice_end

        # å…³é”®ä¿®æ­£ 4ï¼šæ ¹æ® Chunk é‡å»ºæ–‡æœ¬
        reconstructed_text = joiner.join(c.text for c in sentence_chunks)

        sentence = Sentence(
            chunks=sentence_chunks,
            text=reconstructed_text,
            start=sentence_chunks[0].start if sentence_chunks else 0.0,
            end=sentence_chunks[-1].end if sentence_chunks else 0.0,
            index=len(sentences)
        )
        sentences.append(sentence)

    return sentences


def _process_japanese_in_batches(chunks: List[Chunk], nlp: Language, joiner: str) -> List[Sentence]:
    """
    å¯¹æ—¥è¯­è¿›è¡Œåˆ†æ‰¹å¤„ç†ï¼Œé¿å… SudachiPy å­—èŠ‚é™åˆ¶

    ç­–ç•¥ï¼š
    1. æŒ‰ Chunk è¾¹ç•Œç´¯ç§¯å­—èŠ‚å¤§å°
    2. æ¥è¿‘ SUDACHI_MAX_LENGTH æ—¶ï¼Œå‘å‰æŸ¥æ‰¾ç»“æŸç¬¦å·ï¼ˆã€‚ï¼ï¼Ÿï¼‰åˆ‡åˆ†
    3. æ‰¾ä¸åˆ°ç»“æŸç¬¦å·åˆ™æŒ‰å­—èŠ‚å¼ºåˆ¶åˆ‡åˆ†
    4. æ¯æ‰¹ç‹¬ç«‹å¤„ç†ï¼Œæœ€ååˆå¹¶ç»“æœ

    æ³¨æ„ï¼šé™é»˜å¤„ç†ï¼Œä¸æ‰“å°è¯¦ç»†æ—¥å¿—
    """
    all_sentences = []

    # è®¡ç®—æ¯ä¸ª Chunk çš„å­—èŠ‚å¤§å°å’Œç´¯ç§¯å­—èŠ‚
    chunk_bytes = [len(chunk.text.encode('utf-8')) for chunk in chunks]
    cumulative_bytes = []
    total = 0
    for b in chunk_bytes:
        total += b
        cumulative_bytes.append(total)

    # åˆ†æ‰¹å¤„ç†
    batch_start = 0
    batch_count = 0

    while batch_start < len(chunks):
        # æ‰¾åˆ°å½“å‰æ‰¹æ¬¡çš„ç»“æŸä½ç½®
        batch_end = batch_start
        batch_bytes = 0

        while batch_end < len(chunks):
            next_chunk_bytes = chunk_bytes[batch_end]

            # å¦‚æœæ·»åŠ æ­¤ Chunk ä¼šè¶…è¿‡é™åˆ¶
            if batch_bytes + next_chunk_bytes > SUDACHI_MAX_LENGTH and batch_end > batch_start:
                # å‘å‰æŸ¥æ‰¾æœ€è¿‘çš„ç»“æŸç¬¦å·ï¼ˆã€‚ï¼ï¼Ÿï¼‰
                sentence_end_found = False
                for look_back in range(min(50, batch_end - batch_start)):  # æœ€å¤šå›æº¯50ä¸ªchunk
                    check_idx = batch_end - 1 - look_back
                    if check_idx < batch_start:
                        break
                    chunk_text = chunks[check_idx].text
                    # æ£€æŸ¥æ˜¯å¦åŒ…å«æ—¥è¯­ç»“æŸç¬¦å·
                    if any(punct in chunk_text for punct in ['ã€‚', 'ï¼', 'ï¼Ÿ', 'ï¼', '!', '?']):
                        batch_end = check_idx + 1  # åœ¨åŒ…å«ç»“æŸç¬¦å·çš„chunkä¹‹ååˆ‡åˆ†
                        sentence_end_found = True
                        break

                if sentence_end_found:
                    break  # æ‰¾åˆ°ç»“æŸç¬¦å·ï¼Œåœ¨æ­¤åˆ‡åˆ†
                # å¦åˆ™ç»§ç»­å°è¯•æ·»åŠ ä¸‹ä¸€ä¸ªchunkï¼ˆå‘ä¸‹é¢çš„é€»è¾‘ç»§ç»­ï¼‰

            batch_bytes += next_chunk_bytes
            batch_end += 1

        batch_chunks = chunks[batch_start:batch_end]
        batch_count += 1

        # å¤„ç†å½“å‰æ‰¹æ¬¡
        batch_sentences = _process_batch(batch_chunks, nlp, joiner)
        all_sentences.extend(batch_sentences)

        # ç§»åŠ¨åˆ°ä¸‹ä¸€æ‰¹æ¬¡
        batch_start = batch_end

    return all_sentences


def nlp_split_to_sentences(chunks: List[Chunk], nlp: Language) -> List[Sentence]:
    """
    ä½¿ç”¨ spaCy è¿›è¡Œ NLP åˆ†å¥ï¼Œå°† Chunk å¯¹è±¡ç»„åˆæˆ Sentence å¯¹è±¡
    ä¿®å¤ï¼šä¿è¯ Chunk åŸå­æ€§ï¼Œé¿å…ç”±æ ‡ç‚¹ç¬¦å·å¯¼è‡´çš„ Chunk é”™è¯¯åˆ†å‰²

    å¯¹äºæ—¥è¯­ï¼šåˆ†æ‰¹å¤„ç†ä»¥é¿å… SudachiPy å­—èŠ‚é™åˆ¶
    """
    # Validate input chunks
    if not chunks:
        return []

    # è·å– ASR è¯­è¨€å¹¶ç¡®å®šè¿æ¥ç¬¦
    asr_language = load_key("asr.language")
    joiner = get_joiner(asr_language)

    # æ—¥è¯­éœ€è¦åˆ†æ‰¹å¤„ç†
    if asr_language == 'ja':
        return _process_japanese_in_batches(chunks, nlp, joiner)

    # å…¶ä»–è¯­è¨€ä½¿ç”¨åŸæœ‰é€»è¾‘
    return _process_batch(chunks, nlp, joiner)


@timer("NLP åˆ†å¥")
def split_by_spacy() -> List[Sentence]:
    """
    NLP åˆ†å¥ä¸»å‡½æ•°ï¼ˆStage 1ï¼‰

    ä½¿ç”¨å¯¹è±¡åŒ–æµç¨‹ï¼šä» Chunks ç”Ÿæˆ Sentence å¯¹è±¡

    Returns:
        List[Sentence]: åˆ†å¥åçš„ Sentence å¯¹è±¡åˆ—è¡¨
    """
    rprint("[blue]ğŸ” å¼€å§‹ NLP åˆ†å¥ (Stage 1)[/blue]")

    nlp = init_nlp()

    # ä½¿ç”¨å¯¹è±¡åŒ–æµç¨‹ç”Ÿæˆ Sentence å¯¹è±¡
    sentences = split_by_nlp(nlp)

    rprint(f"[green]âœ… NLP åˆ†å¥å®Œæˆ: {_3_1_SPLIT_BY_NLP}[/green]")
    return sentences


# ------------
# New NLP Split Function with Character Position Tracking
# ------------

@cache_objects(_CACHE_SENTENCES_NLP, _3_1_SPLIT_BY_NLP)
def split_by_nlp(nlp: Language) -> List[Sentence]:
    """
    NLP åˆ†å¥ä¸»å‡½æ•°

    è¾“å…¥: cleaned_chunks.csv â†’ List[Chunk]
    è¾“å‡º: List[Sentence] â†’ ä¿å­˜åˆ° split_by_nlp.txt (æ–‡æœ¬) å’Œè¿”å›å¯¹è±¡
    """
    # 1. åŠ è½½ Chunk å¯¹è±¡
    chunks = load_chunks()

    # 2. NLP åˆ†å¥ï¼Œç”Ÿæˆ Sentence å¯¹è±¡
    sentences = nlp_split_to_sentences(chunks, nlp)

    rprint(f'[green]âœ… å¤„ç†å®Œæˆï¼å…± {len(sentences)} ä¸ªå¥å­[/green]')

    return sentences


if __name__ == '__main__':
    split_by_spacy()
