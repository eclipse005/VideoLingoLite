"""
ASR Segments to Sentences Module (Stage 1)

ç›´æ¥ä» Parakeet ASR çš„ segments åˆ›å»º Sentence å¯¹è±¡ï¼Œæ— éœ€ spaCy åˆ†å¥
å¯é€‰æ‹©åº”ç”¨åœé¡¿åˆ†å¥ï¼ˆåŸºäºæ—¶é—´æˆ³ï¼‰

Output: split_by_nlp.txt (Stage 1 result)
"""

import json
import os
from typing import List

from core.utils import rprint, load_key, timer
from core.utils.models import _3_1_SPLIT_BY_NLP, Chunk, Sentence


def load_asr_json() -> dict:
    """åŠ è½½ ASR ç»“æœ JSON"""
    asr_json_path = "output/log/asr.json"
    if not os.path.exists(asr_json_path):
        raise FileNotFoundError(f"ASR ç»“æœä¸å­˜åœ¨: {asr_json_path}")

    with open(asr_json_path, 'r', encoding='utf-8') as f:
        return json.load(f)


def create_chunks_from_words(words: List[dict], start_offset: int = 0) -> List[Chunk]:
    """
    ä» words åˆ—è¡¨åˆ›å»º Chunk å¯¹è±¡

    Args:
        words: [{'start': float, 'end': float, 'word': str}, ...]
        start_offset: èµ·å§‹æ—¶é—´åç§»ï¼ˆç”¨äºè°ƒè¯•ï¼‰

    Returns:
        List[Chunk]
    """
    chunks = []
    for idx, word_info in enumerate(words):
        chunk = Chunk(
            text=word_info['word'],
            start=word_info['start'],
            end=word_info['end'],
            speaker_id=None,  # ASR æ²¡æœ‰è¯´è¯äººä¿¡æ¯
            index=idx
        )
        chunks.append(chunk)
    return chunks


def segments_to_sentences(asr_data: dict) -> List[Sentence]:
    """
    ä» ASR segments åˆ›å»º Sentence å¯¹è±¡

    Args:
        asr_data: ASR JSON æ•°æ®ï¼ŒåŒ…å« segments åˆ—è¡¨

    Returns:
        List[Sentence]
    """
    segments = asr_data.get('segments', [])
    if not segments:
        rprint("[yellow]âš ï¸ æœªæ‰¾åˆ° ASR segments[/yellow]")
        return []

    sentences = []
    for idx, seg in enumerate(segments):
        # ä» words åˆ›å»º chunks
        words = seg.get('words', [])
        chunks = create_chunks_from_words(words)

        if not chunks:
            continue

        # åˆ›å»º Sentence å¯¹è±¡
        sentence = Sentence(
            chunks=chunks,
            text=seg['text'],
            start=seg['start'],
            end=seg['end'],
            index=idx
        )
        sentences.append(sentence)

    return sentences


@timer("ASR Segments è½¬å¥å­")
def split_by_spacy() -> List[Sentence]:
    """
    ASR Segments è½¬å¥å­ä¸»å‡½æ•°ï¼ˆStage 1ï¼‰

    ä½¿ç”¨ Parakeet ASR çš„ segments ç›´æ¥åˆ›å»º Sentence å¯¹è±¡
    å¯é€‰ï¼šåº”ç”¨åœé¡¿åˆ†å¥

    Returns:
        List[Sentence]: ä» segments åˆ›å»ºçš„ Sentence å¯¹è±¡åˆ—è¡¨
    """
    # 1. åŠ è½½ ASR ç»“æœ
    asr_data = load_asr_json()
    segment_count = len(asr_data.get('segments', []))
    rprint(f"[blue]ğŸ” Stage 1: ASR Segments â†’ {segment_count} ä¸ªå¥å­[/blue]")

    # 2. ä» segments åˆ›å»º Sentence å¯¹è±¡
    sentences = segments_to_sentences(asr_data)
    if not sentences:
        rprint("[yellow]âš ï¸ æ²¡æœ‰å¥å­åˆ›å»ºï¼Œè·³è¿‡åç»­å¤„ç†[/yellow]")
        return []

    # 3. å¯é€‰ï¼šåº”ç”¨åœé¡¿åˆ†å¥
    pause_threshold = load_key("pause_split_threshold")
    if pause_threshold and pause_threshold > 0:
        from core.spacy_utils import split_by_pause
        original_count = len(sentences)
        sentences = split_by_pause(sentences, pause_threshold)
        if len(sentences) != original_count:
            rprint(f"[cyan]  â†ª åœé¡¿åˆ†å¥: {original_count} â†’ {len(sentences)} ä¸ª[/cyan]")

    # 4. ä¿å­˜åˆ°æ–‡ä»¶
    from pathlib import Path
    Path(_3_1_SPLIT_BY_NLP).parent.mkdir(parents=True, exist_ok=True)
    with open(_3_1_SPLIT_BY_NLP, 'w', encoding='utf-8') as f:
        for s in sentences:
            f.write(s.text + '\n')

    rprint(f'[green]âœ… Stage 1 å®Œæˆ[/green]')
    return sentences


if __name__ == '__main__':
    split_by_spacy()
