# Chunk/Sentence å¯¹è±¡åŒ–é‡æ„å®æ–½è®¡åˆ’

> **For Claude:** REQUIRED SUB-SKILL: Use superpowers:executing-plans to implement this plan task-by-task.

**ç›®æ ‡:** å°†æ•°æ®å¤„ç†ä»"æ–‡æœ¬å¤„ç† + difflib æ—¶é—´å¯¹é½"é‡æ„ä¸º"å¯¹è±¡åŒ–å¤„ç†"ï¼Œæ—¶é—´æˆ³å§‹ç»ˆè·Ÿéš Chunk å¯¹è±¡

**æ¶æ„:** Chunk å¯¹è±¡ï¼ˆè¯/å­—çº§åˆ«ï¼‰æºå¸¦æ—¶é—´æˆ³ï¼ŒSentence å¯¹è±¡ï¼ˆå¥å­çº§åˆ«ï¼‰ç»„åˆ Chunkã€‚æ—¶é—´æˆ³é€šè¿‡å¯¹è±¡å¼•ç”¨ä¼ é€’ï¼Œæ¶ˆé™¤å¤§éƒ¨åˆ† difflib åŒ¹é…ã€‚

**æŠ€æœ¯æ ˆ:** Python 3.10, pandas, dataclasses, difflib (ä»… LLM æ–­å¥æ—¶ä½¿ç”¨), spaCy (NLP åˆ†å¥)

---

## Task 1: åˆ›å»ºæ ¸å¿ƒæ•°æ®ç»“æ„ (Chunk å’Œ Sentence ç±»)

**Files:**
- Modify: `core/models.py`

**Step 1: åœ¨ core/models.py é¡¶éƒ¨æ·»åŠ å¿…è¦çš„å¯¼å…¥**

```python
from dataclasses import dataclass, field
from typing import List, Optional
```

**Step 2: åœ¨ core/models.py ä¸­æ·»åŠ  Chunk ç±»**

åœ¨ `_2_CLEANED_CHUNKS` å¸¸é‡å®šä¹‰ä¹‹å‰æ·»åŠ ï¼š

```python
@dataclass
class Chunk:
    """è¯/å­—çº§åˆ«çš„è¯­éŸ³è¯†åˆ«å•å…ƒï¼Œæºå¸¦æ—¶é—´æˆ³"""
    text: str           # è¯/å­—å†…å®¹ï¼ˆå¦‚ "Hello", "world."ï¼‰
    start: float        # å¼€å§‹æ—¶é—´ï¼ˆç§’ï¼‰
    end: float          # ç»“æŸæ—¶é—´ï¼ˆç§’ï¼‰
    speaker_id: Optional[str] = None  # è¯´è¯äººID
    index: int = 0      # åœ¨ cleaned_chunks.csv ä¸­çš„è¡Œå·

    @property
    def duration(self) -> float:
        """è·å–è¯¥è¯çš„æ—¶é•¿"""
        return self.end - self.start
```

**Step 3: åœ¨ core/models.py ä¸­æ·»åŠ  Sentence ç±»**

åœ¨ Chunk ç±»ä¹‹åæ·»åŠ ï¼š

```python
@dataclass
class Sentence:
    """å¥å­çº§åˆ«çš„å¯¹è±¡ï¼Œç”±å¤šä¸ª Chunk ç»„æˆ"""
    chunks: List[Chunk]     # ç»„æˆè¿™å¥è¯çš„æ‰€æœ‰ Chunk
    text: str               # ä» chunks æ‹¼æ¥çš„å®Œæ•´æ–‡æœ¬
    start: float            # = chunks[0].start
    end: float              # = chunks[-1].end
    translation: str = ""   # ç¿»è¯‘æ–‡æœ¬
    index: int = 0          # å¥å­åºå·
    is_split: bool = False  # æ˜¯å¦è¢« LLM åˆ‡åˆ†è¿‡

    @property
    def duration(self) -> float:
        """è·å–è¿™å¥è¯çš„æ—¶é•¿"""
        return self.end - self.start

    def update_timestamps(self):
        """æ ¹æ® chunks æ›´æ–° start å’Œ end æ—¶é—´æˆ³"""
        if self.chunks:
            self.start = self.chunks[0].start
            self.end = self.chunks[-1].end
```

**Step 4: éªŒè¯è¯­æ³•**

è¿è¡Œ: `python -c "from core.models import Chunk, Sentence; print('Import successful')"`
Expected: `Import successful`

**Step 5: æäº¤**

```bash
git add core/models.py
git commit -m "feat: add Chunk and Sentence dataclasses"
```

---

## Task 2: åˆ›å»º Chunk åŠ è½½å‡½æ•°

**Files:**
- Modify: `core/_2_asr.py`

**Step 1: åœ¨ core/_2_asr.py ä¸­æ·»åŠ  load_chunks å‡½æ•°**

åœ¨æ–‡ä»¶æœ«å°¾ã€`if __name__ == "__main__":` ä¹‹å‰æ·»åŠ ï¼š

```python
def load_chunks() -> List[Chunk]:
    """
    ä» cleaned_chunks.csv åŠ è½½ Chunk å¯¹è±¡åˆ—è¡¨

    Returns:
        List[Chunk]: è¯/å­—çº§åˆ«çš„ Chunk å¯¹è±¡åˆ—è¡¨
    """
    df = safe_read_csv(_2_CLEANED_CHUNKS)
    chunks = []

    for idx, row in df.iterrows():
        chunk = Chunk(
            text=str(row['text']).strip('"'),
            start=float(row['start']),
            end=float(row['end']),
            speaker_id=str(row['speaker_id']) if pd.notna(row['speaker_id']) and row['speaker_id'] else None,
            index=idx
        )
        chunks.append(chunk)

    rprint(f"[green]âœ… Loaded {len(chunks)} chunks from {_2_CLEANED_CHUNKS}[/green]")
    return chunks
```

**Step 2: éªŒè¯ load_chunks å‡½æ•°**

åˆ›å»ºæµ‹è¯•æ–‡ä»¶ `test_load_chunks.py`:

```python
# ä¸´æ—¶æµ‹è¯•æ–‡ä»¶
import sys
import os
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from core._2_asr import load_chunks

# ç¡®ä¿æœ‰ cleaned_chunks.csv
if os.path.exists('output/log/cleaned_chunks.csv'):
    chunks = load_chunks()
    print(f"First chunk: {chunks[0].text} ({chunks[0].start} - {chunks[0].end})")
    print(f"Total chunks: {len(chunks)}")
else:
    print("cleaned_chunks.csv not found, skipping test")
```

è¿è¡Œ: `python test_load_chunks.py`
Expected: æ˜¾ç¤ºç¬¬ä¸€ä¸ª chunk çš„ä¿¡æ¯å’Œæ€»æ•°

**Step 3: æ¸…ç†æµ‹è¯•æ–‡ä»¶**

```bash
rm test_load_chunks.py
```

**Step 4: æäº¤**

```bash
git add core/_2_asr.py
git commit -m "feat: add load_chunks() function to load Chunk objects"
```

---

## Task 3: åˆ›å»º NLP åˆ†å¥å‡½æ•°ï¼ˆå­—ç¬¦ä½ç½®è¿½è¸ªï¼‰

**Files:**
- Modify: `core/_3_1_split_nlp.py`

**Step 1: æ·»åŠ å¿…è¦çš„å¯¼å…¥**

åœ¨æ–‡ä»¶é¡¶éƒ¨æ·»åŠ ï¼š

```python
from dataclasses import dataclass, field
from typing import List, Optional
from core.models import Chunk, Sentence
```

**Step 2: æ·»åŠ å­—ç¬¦ä½ç½®æ˜ å°„å‡½æ•°**

åœ¨ `split_by_nlp()` å‡½æ•°ä¹‹å‰æ·»åŠ ï¼š

```python
def build_char_to_chunk_mapping(chunks: List[Chunk]) -> List[int]:
    """
    æ„å»ºå­—ç¬¦åˆ° Chunk ç´¢å¼•çš„æ˜ å°„

    Args:
        chunks: Chunk å¯¹è±¡åˆ—è¡¨

    Returns:
        æ¯ä¸ªå­—ç¬¦å¯¹åº”çš„ Chunk ç´¢å¼•åˆ—è¡¨
    """
    char_to_chunk = []
    for chunk_idx, chunk in enumerate(chunks):
        char_to_chunk.extend([chunk_idx] * len(chunk.text))
    return char_to_chunk


def nlp_split_to_sentences(chunks: List[Chunk], nlp) -> List[Sentence]:
    """
    ä½¿ç”¨ spaCy è¿›è¡Œ NLP åˆ†å¥ï¼Œå°† Chunk å¯¹è±¡ç»„åˆæˆ Sentence å¯¹è±¡

    Args:
        chunks: Chunk å¯¹è±¡åˆ—è¡¨
        nlp: spaCy NLP æ¨¡å‹

    Returns:
        Sentence å¯¹è±¡åˆ—è¡¨
    """
    # 1. æ‹¼æ¥æ‰€æœ‰ Chunk çš„æ–‡æœ¬
    full_text = "".join(chunk.text for chunk in chunks)

    # 2. æ„å»ºå­—ç¬¦åˆ° Chunk çš„æ˜ å°„
    char_to_chunk = build_char_to_chunk_mapping(chunks)

    # 3. ä½¿ç”¨ spaCy åˆ†å¥
    doc = nlp(full_text)
    sentences = []

    for sent_idx, sent in enumerate(doc.sents):
        start_char = sent.start_char
        end_char = sent.end_char

        # è¾¹ç•Œæ£€æŸ¥
        start_char = max(0, min(start_char, len(full_text) - 1))
        end_char = max(start_char + 1, min(end_char, len(full_text)))

        # æ‰¾åˆ°å¯¹åº”çš„ Chunk èŒƒå›´
        start_chunk_idx = char_to_chunk[start_char]
        end_chunk_idx = char_to_chunk[end_char - 1]

        # æå–å¯¹åº”çš„ Chunk å¯¹è±¡
        sentence_chunks = chunks[start_chunk_idx:end_chunk_idx + 1]

        # åˆ›å»º Sentence å¯¹è±¡
        sentence = Sentence(
            chunks=sentence_chunks,
            text=sent.text,
            start=sentence_chunks[0].start if sentence_chunks else 0.0,
            end=sentence_chunks[-1].end if sentence_chunks else 0.0,
            index=sent_idx
        )
        sentences.append(sentence)

    return sentences
```

**Step 3: ä¿®æ”¹ split_by_nlp() å‡½æ•°ä»¥ä½¿ç”¨æ–°çš„å¯¹è±¡**

ä¿®æ”¹ç°æœ‰çš„ `split_by_nlp(nlp)` å‡½æ•°ï¼š

```python
@check_file_exists(_3_1_SPLIT_BY_NLP)
def split_by_nlp(nlp):
    """
    NLP åˆ†å¥ä¸»å‡½æ•°

    è¾“å…¥: cleaned_chunks.csv â†’ List[Chunk]
    è¾“å‡º: List[Sentence] â†’ ä¿å­˜åˆ° split_by_nlp.txt (æ–‡æœ¬) å’Œè¿”å›å¯¹è±¡
    """
    console.print("[blue]ğŸ” Starting NLP sentence splitting...[/blue]")

    # 1. åŠ è½½ Chunk å¯¹è±¡
    chunks = load_chunks()

    # 2. NLP åˆ†å¥ï¼Œç”Ÿæˆ Sentence å¯¹è±¡
    sentences = nlp_split_to_sentences(chunks, nlp)

    # 3. ä¿å­˜æ–‡æœ¬åˆ°æ–‡ä»¶ï¼ˆå‘åå…¼å®¹ï¼‰
    with open(_3_1_SPLIT_BY_NLP, 'w', encoding='utf-8') as f:
        for sent in sentences:
            f.write(sent.text + '\n')

    console.print(f'[green]âœ… NLP splitting complete! {len(sentences)} sentences generated[/green]')
    console.print(f'[green]ğŸ’¾ Saved to: {_3_1_SPLIT_BY_NLP}[/green]')

    return sentences
```

**Step 4: æäº¤**

```bash
git add core/_3_1_split_nlp.py
git commit -m "feat: add nlp_split_to_sentences with char position tracking"
```

---

## Task 4: åˆ›å»º LLM æ–­å¥å‡½æ•°ï¼ˆæ‹†åˆ† Sentence.chunksï¼‰

**Files:**
- Modify: `core/_3_2_split_meaning.py`

**Step 1: æ·»åŠ å¿…è¦çš„å¯¼å…¥**

åœ¨æ–‡ä»¶é¡¶éƒ¨æ·»åŠ ï¼š

```python
from core.models import Sentence
```

**Step 2: æ·»åŠ  [br] ä½ç½®è§£æå‡½æ•°**

åœ¨æ–‡ä»¶ä¸­æ·»åŠ ï¼š

```python
def parse_br_positions(llm_output: str) -> List[int]:
    """
    è§£æ LLM è¾“å‡ºä¸­ [br] æ ‡è®°çš„ä½ç½®

    Args:
        llm_output: LLM è¿”å›çš„å¸¦ [br] æ ‡è®°çš„æ–‡æœ¬

    Returns:
        [br] åœ¨ LLM è¾“å‡ºä¸­çš„å­—ç¬¦ä½ç½®åˆ—è¡¨
    """
    import re
    return [m.start() for m in re.finditer(r'\[br\]', llm_output)]


def find_br_positions_in_original(llm_output: str, original_text: str) -> List[int]:
    """
    ä½¿ç”¨ difflib æ‰¾åˆ° [br] åœ¨åŸå§‹å¥å­ä¸­çš„å­—ç¬¦ä½ç½®

    Args:
        llm_output: LLM è¿”å›çš„å¸¦ [br] æ ‡è®°çš„æ–‡æœ¬
        original_text: åŸå§‹å¥å­æ–‡æœ¬

    Returns:
        [br] åœ¨åŸå§‹æ–‡æœ¬ä¸­çš„å­—ç¬¦ä½ç½®åˆ—è¡¨
    """
    import difflib
    from core.utils.sentence_tools import clean_word

    # 1. æ‰¾åˆ° [br] åœ¨ LLM è¾“å‡ºä¸­çš„ä½ç½®
    br_positions = parse_br_positions(llm_output)

    if not br_positions:
        return []

    # 2. æ¸…æ´—æ–‡æœ¬ç”¨äºåŒ¹é…
    llm_clean = clean_word(llm_output.replace('[br]', ''))
    original_clean = clean_word(original_text)

    # 3. ä½¿ç”¨ difflib åŒ¹é…
    s = difflib.SequenceMatcher(None, llm_clean, original_clean, autojunk=False)
    matching_blocks = s.get_matching_blocks()

    # 4. å»ºç«‹ LLM è¾“å‡ºä½ç½®åˆ°åŸå§‹æ–‡æœ¬ä½ç½®çš„æ˜ å°„
    llm_to_original = {}
    current_original_pos = 0

    for llm_start, orig_start, length in matching_blocks:
        if length == 0:
            continue
        for i in range(length):
            llm_to_original[llm_start + i] = orig_start + i

    # 5. æ‰¾åˆ°æ¯ä¸ª [br] å¯¹åº”çš„åŸå§‹ä½ç½®
    original_br_positions = []
    for br_pos in br_positions:
        # [br] åœ¨æ¸…æ´—åæ–‡æœ¬ä¸­çš„ä½ç½®ï¼ˆéœ€è¦å»é™¤ä¹‹å‰çš„ [br]ï¼‰
        llm_before_br = llm_output[:br_pos]
        llm_clean_before_br = clean_word(llm_before_br.replace('[br]', ''))

        if llm_clean_before_br in llm_to_original:
            original_pos = llm_to_original[llm_clean_before_br]
            original_br_positions.append(original_pos)

    return original_br_positions


def split_sentence_by_br(sentence: Sentence, llm_output: str) -> List[Sentence]:
    """
    æ ¹æ® LLM è¿”å›çš„ [br] æ ‡è®°æ‹†åˆ† Sentence

    Args:
        sentence: åŸå§‹ Sentence å¯¹è±¡
        llm_output: LLM è¿”å›çš„å¸¦ [br] æ ‡è®°çš„æ–‡æœ¬

    Returns:
        æ‹†åˆ†åçš„ Sentence å¯¹è±¡åˆ—è¡¨
    """
    # 1. æ‰¾åˆ° [br] åœ¨åŸå§‹å¥å­ä¸­çš„ä½ç½®
    br_positions = find_br_positions_in_original(llm_output, sentence.text)

    if not br_positions:
        # æ²¡æœ‰éœ€è¦æ‹†åˆ†çš„åœ°æ–¹
        return [sentence]

    # 2. æ„å»ºå­—ç¬¦åˆ° Chunk çš„æ˜ å°„
    char_to_chunk = []
    for chunk_idx, chunk in enumerate(sentence.chunks):
        char_to_chunk.extend([chunk_idx] * len(chunk.text))

    # 3. æ ¹æ® [br] ä½ç½®ç¡®å®š Chunk æ‹†åˆ†ç‚¹
    split_points = [0]  # èµ·å§‹ç‚¹
    for br_pos in br_positions:
        if br_pos < len(char_to_chunk):
            chunk_idx = char_to_chunk[br_pos]
            if chunk_idx not in split_points:
                split_points.append(chunk_idx)
    split_points.append(len(sentence.chunks))  # ç»“æŸç‚¹
    split_points = sorted(set(split_points))

    # 4. æ‹†åˆ† Chunksï¼Œåˆ›å»ºæ–°çš„ Sentence å¯¹è±¡
    new_sentences = []
    for i in range(len(split_points) - 1):
        start_idx = split_points[i]
        end_idx = split_points[i + 1]

        if start_idx >= end_idx:
            continue

        sub_chunks = sentence.chunks[start_idx:end_idx]

        new_sentence = Sentence(
            chunks=sub_chunks,
            text="".join(c.text for c in sub_chunks),
            start=sub_chunks[0].start,
            end=sub_chunks[-1].end,
            index=sentence.index + i,
            is_split=True
        )
        new_sentences.append(new_sentence)

    return new_sentences
```

**Step 3: æäº¤**

```bash
git add core/_3_2_split_meaning.py
git commit -m "feat: add split_sentence_by_br with difflib matching"
```

---

## Task 5: ä¿®æ”¹ parallel_split_sentences ä½¿ç”¨ Sentence å¯¹è±¡

**Files:**
- Modify: `core/_3_2_split_meaning.py`

**Step 1: ä¿®æ”¹ parallel_split_sentences å‡½æ•°ç­¾å**

```python
def parallel_split_sentences(sentences: List[Sentence], max_length, max_workers, retry_attempt=0):
    """ä½¿ç”¨ LLM å¹¶è¡Œæ‹†åˆ†é•¿å¥"""
    # ... å®ç°ä¿æŒä¸å˜ï¼Œä½†æ”¹ä¸ºæ“ä½œ Sentence å¯¹è±¡
```

**Step 2: ä¿®æ”¹å‡½æ•°å†…éƒ¨ä»¥ä½¿ç”¨ Sentence å¯¹è±¡**

åœ¨å¤„ç†æ—¶ä½¿ç”¨ `sentence.text` è·å–æ–‡æœ¬ï¼Œæ‹†åˆ†åè°ƒç”¨ `split_sentence_by_br()`

**Step 3: æäº¤**

```bash
git add core/_3_2_split_meaning.py
git commit -m "refactor: parallel_split_sentences to use Sentence objects"
```

---

## Task 6: ç®€åŒ– _6_gen_sub.pyï¼ˆç›´æ¥ä½¿ç”¨ Sentence.start/endï¼‰

**Files:**
- Modify: `core/_6_gen_sub.py`

**Step 1: æ·»åŠ ä» Sentence åˆ—è¡¨ç”Ÿæˆå­—å¹•çš„å‡½æ•°**

åœ¨æ–‡ä»¶ä¸­æ·»åŠ ï¼š

```python
def generate_subtitles_from_sentences(sentences: List[Sentence], subtitle_output_configs: list, output_dir: str, for_display: bool = True):
    """
    ç›´æ¥ä» Sentence å¯¹è±¡åˆ—è¡¨ç”Ÿæˆå­—å¹•ï¼Œä¸éœ€è¦åŒ¹é…

    Args:
        sentences: Sentence å¯¹è±¡åˆ—è¡¨
        subtitle_output_configs: å­—å¹•è¾“å‡ºé…ç½®
        output_dir: è¾“å‡ºç›®å½•
        for_display: æ˜¯å¦ç”¨äºæ˜¾ç¤º
    """
    df_trans_time = []

    for sent in sentences:
        df_trans_time.append({
            'Source': sent.text,
            'Translation': sent.translation,
            'timestamp': (sent.start, sent.end),
            'duration': sent.duration
        })

    # è½¬æ¢ä¸º DataFrame
    import pandas as pd
    df_trans_time = pd.DataFrame(df_trans_time)

    # ç§»é™¤é—´éš™
    for i in range(len(df_trans_time) - 1):
        delta_time = df_trans_time.loc[i + 1, 'timestamp'][0] - df_trans_time.loc[i, 'timestamp'][1]
        if 0 < delta_time < 1:
            df_trans_time.at[i, 'timestamp'] = (
                df_trans_time.loc[i, 'timestamp'][0],
                df_trans_time.loc[i + 1, 'timestamp'][0]
            )

    # è½¬æ¢ä¸º SRT æ ¼å¼
    df_trans_time['timestamp'] = df_trans_time['timestamp'].apply(
        lambda x: convert_to_srt_format(x[0], x[1])
    )

    # ç¾åŒ–å­—å¹•
    if for_display:
        import re
        import autocorrect_py as autocorrect
        df_trans_time['Translation'] = df_trans_time['Translation'].apply(
            lambda x: autocorrect.format(re.sub(r'[ï¼Œã€‚]', ' ', str(x).strip()).strip())
        )

    # è¾“å‡ºå­—å¹•
    def generate_subtitle_string(df, columns):
        result = []
        for i, row in df.iterrows():
            def safe_get(col):
                val = row.get(col, '')
                return str(val).strip() if pd.notna(val) else ''

            line1 = safe_get(columns[0])
            line2 = safe_get(columns[1]) if len(columns) > 1 else ''
            result.append(f"{i+1}\n{row['timestamp']}\n{line1}\n{line2}\n\n")
        return ''.join(result).strip()

    if output_dir:
        os.makedirs(output_dir, exist_ok=True)
        for filename, columns in subtitle_output_configs:
            subtitle_str = generate_subtitle_string(df_trans_time, columns)
            with open(os.path.join(output_dir, filename), 'w', encoding='utf-8') as f:
                f.write(subtitle_str)

    return df_trans_time
```

**Step 2: ä¿®æ”¹ align_timestamp_main å‡½æ•°**

```python
def align_timestamp_main():
    """ä¸»å‡½æ•°ï¼šåŠ è½½ Sentence å¯¹è±¡å¹¶ç”Ÿæˆå­—å¹•"""
    df_text = safe_read_csv(_2_CLEANED_CHUNKS)
    df_text['text'] = df_text['text'].str.strip('"').str.strip()
    df_translate = safe_read_csv(_5_SPLIT_SUB)
    df_translate['Translation'] = df_translate['Translation'].apply(clean_translation)

    # ä» CSV é‡å»º Sentence å¯¹è±¡ï¼ˆä¸´æ—¶æ–¹æ¡ˆï¼Œåç»­æ”¹ä¸ºä»å…ƒæ•°æ®åŠ è½½ï¼‰
    chunks = load_chunks()
    sentences = []  # éœ€è¦ä» translation CSV é‡å»º

    # TODO: è¿™é‡Œéœ€è¦ä» translation CSV é‡å»º Sentence å¯¹è±¡
    # ä¸´æ—¶æ–¹æ¡ˆï¼šä½¿ç”¨ç°æœ‰çš„ difflib åŒ¹é…
    align_timestamp(df_text, df_translate, SUBTITLE_OUTPUT_CONFIGS, _OUTPUT_DIR)
    console.print(Panel("[bold green]ğŸ‰ğŸ“ Subtitles generation completed! Please check in the `output` folder ğŸ‘€[/bold green]"))

    # åˆå¹¶ç©ºå­—å¹•
    merge_empty_subtitle()
```

**Step 3: æäº¤**

```bash
git add core/_6_gen_sub.py
git commit -m "refactor: add generate_subtitles_from_sentences function"
```

---

## Task 7: æ›´æ–° Streamlit å…¥å£ä»¥æ”¯æŒæ–°çš„å¯¹è±¡æµç¨‹

**Files:**
- Modify: `st.py`

**Step 1: æ£€æŸ¥ç°æœ‰çš„å¤„ç†æµç¨‹**

ç¡®ä¿ `_3_1_split_nlp.py` å’Œ `_3_2_split_meaning.py` è¿”å› Sentence å¯¹è±¡

**Step 2: æäº¤**

```bash
git add st.py
git commit -m "refactor: update Streamlit flow for Sentence objects"
```

---

## Task 8: æµ‹è¯•ç«¯åˆ°ç«¯æµç¨‹

**Step 1: è¿è¡Œå®Œæ•´çš„å¤„ç†æµç¨‹**

```bash
# è¿è¡Œ Streamlit æˆ–ç›´æ¥è°ƒç”¨æ¨¡å—
python -m core._2_asr
python -m core._3_1_split_nlp
python -m core._3_2_split_meaning
python -m core._4_1_summarize
python -m core._4_2_translate
python -m core._5_split_sub
python -m core._6_gen_sub
```

**Step 2: éªŒè¯è¾“å‡º**

æ£€æŸ¥ `output/` ç›®å½•ä¸‹çš„å­—å¹•æ–‡ä»¶ï¼Œç¡®ä¿æ—¶é—´æˆ³æ­£ç¡®

**Step 3: æäº¤æµ‹è¯•é…ç½®**

```bash
git add .
git commit -m "test: verify end-to-end Sentence object flow"
```

---

## Task 9: æ¸…ç†å’Œä¼˜åŒ–

**Files:**
- Modify: å¤šä¸ªæ–‡ä»¶

**Step 1: åˆ é™¤ä¸å†éœ€è¦çš„ difflib åŒ¹é…ä»£ç **

é™¤äº† `_3_2_split_meaning.py` ä¸­çš„ LLM æ–­å¥åŒ¹é…ï¼Œåˆ é™¤å…¶ä»– difflib ç›¸å…³ä»£ç 

**Step 2: æ·»åŠ ç±»å‹æç¤º**

ç¡®ä¿æ‰€æœ‰å‡½æ•°éƒ½æœ‰æ­£ç¡®çš„ç±»å‹æç¤º

**Step 3: æäº¤**

```bash
git add .
git commit -m "refactor: remove unused difflib matching code"
```

---

## Task 10: æ›´æ–°æ–‡æ¡£

**Files:**
- Modify: `README.md`, `CLAUDE.md`

**Step 1: æ›´æ–°æ¶æ„è¯´æ˜**

æ·»åŠ  Chunk/Sentence å¯¹è±¡æ¨¡å‹çš„è¯´æ˜

**Step 2: æäº¤**

```bash
git add README.md CLAUDE.md
git commit -m "docs: update architecture for Chunk/Sentence objects"
```

---

## éªŒæ”¶æ ‡å‡†

1. âœ… Chunk å’Œ Sentence ç±»å®šä¹‰å®Œæ•´
2. âœ… load_chunks() èƒ½æ­£ç¡®åŠ è½½ Chunk å¯¹è±¡
3. âœ… NLP åˆ†å¥ä½¿ç”¨å­—ç¬¦ä½ç½®è¿½è¸ªï¼Œç”Ÿæˆ Sentence å¯¹è±¡
4. âœ… LLM æ–­å¥æ­£ç¡®æ‹†åˆ† Sentence.chunks
5. âœ… å­—å¹•ç”Ÿæˆç›´æ¥ä½¿ç”¨ Sentence.start/end
6. âœ… ç«¯åˆ°ç«¯æµç¨‹æ­£å¸¸è¿è¡Œ
7. âœ… æ—¶é—´æˆ³å‡†ç¡®ï¼ˆåŸºäºåŸå§‹ ASR æ•°æ®ï¼‰
8. âœ… ä»£ç å¯è¯»æ€§æé«˜ï¼Œdifflib ä½¿ç”¨æœ€å°åŒ–

---

## æ³¨æ„äº‹é¡¹

1. **å‘åå…¼å®¹**ï¼šä¿ç•™ç°æœ‰çš„æ–‡æœ¬æ–‡ä»¶è¾“å‡ºï¼ˆsplit_by_nlp.txt ç­‰ï¼‰
2. **æ–­ç‚¹ç»­ä¼ **ï¼šåç»­æ·»åŠ å…ƒæ•°æ®ä¿å­˜/åŠ è½½åŠŸèƒ½
3. **è¾¹ç•Œæƒ…å†µ**ï¼šChunk å†…éƒ¨æ–‡å­—è¢«æ‹†åˆ†çš„æƒ…å†µï¼ˆæ ‡è®°ä¸º TODOï¼‰
4. **æµ‹è¯•**ï¼šæ¯ä¸ªé˜¶æ®µéƒ½è¦å……åˆ†æµ‹è¯•ï¼Œç¡®ä¿æ—¶é—´æˆ³å‡†ç¡®
